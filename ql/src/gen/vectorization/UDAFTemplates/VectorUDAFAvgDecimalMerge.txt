/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;

import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.hive.common.type.HiveDecimal;
import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil;
import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.plan.AggregationDesc;
import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal;
import org.apache.hadoop.hive.ql.util.JavaDataModel;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;

import com.google.common.base.Preconditions;

/**
 * Generated from template VectorUDAFAvg.txt.
 */
@Description(name = "avg",
    value = "_FUNC_(AVG) - Returns the average value of expr (vectorized, type: decimal)")
public class <ClassName> extends VectorAggregateExpression {

    private static final long serialVersionUID = 1L;

    /** class for storing the current aggregate value. */
    static class Aggregation implements AggregationBuffer {

      private static final long serialVersionUID = 1L;

      transient private final HiveDecimalWritable mergeSum = new HiveDecimalWritable();
      transient private long mergeCount;
      transient private boolean isNull;

      public void merge(long count, HiveDecimalWritable sum) {
        if (isNull) {
          // Make a copy since we intend to mutate sum.
          mergeCount = count;
          mergeSum.set(sum);
          isNull = false;
        } else {
          // Note that if sum is out of range, mutateAdd will ignore the call.
          // At the end, sum.isSet() can be checked for null.
          mergeCount += count;
          mergeSum.mutateAdd(sum);
        }
      }

      public void mergeNoNullCheck(long count, HiveDecimalWritable sum) {
        mergeCount += count;
        mergeSum.mutateAdd(sum);
      }

      @Override
      public int getVariableSize() {
        throw new UnsupportedOperationException();
      }

      @Override
      public void reset() {
        isNull = true;
        mergeCount = 0;
        mergeSum.setFromLong(0L);
      }
    }

#IF PARTIAL2
    transient private Object[] partialResult;
    transient private LongWritable resultCount;
    transient private HiveDecimalWritable resultSum;
    transient private HiveDecimalWritable resultInput;
    transient private StructObjectInspector soi;
#ENDIF PARTIAL2
#IF FINAL
    transient private HiveDecimalWritable tempDecWritable;
    transient private HiveDecimalWritable fullResult;
    transient private ObjectInspector oi;
#ENDIF FINAL

    private transient int countOffset;
    private transient int sumOffset;
    private transient int inputOffset;

    /**
     * The scale of the SUM in the partial output
     */
    private int sumScale;

    /**
     * The precision of the SUM in the partial output
     */
    private int sumPrecision;

    /**
     * the scale of the input expression
     */
    private int inputScale;

    /**
     * the precision of the input expression
     */
    private int inputPrecision;

    public <ClassName>(VectorExpression inputExpression,
        GenericUDAFEvaluator.Mode mode) {
      super(inputExpression, mode);
#IF PARTIAL2
      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.PARTIAL2);
#ENDIF PARTIAL2
#IF FINAL
      Preconditions.checkState(this.mode == GenericUDAFEvaluator.Mode.FINAL);
#ENDIF FINAL
    }

    private void init() {
#IF PARTIAL2
      partialResult = new Object[3];
      resultCount = new LongWritable();
      resultSum = new HiveDecimalWritable();
      resultInput = new HiveDecimalWritable(0L);
      partialResult[0] = resultCount;
      partialResult[1] = resultSum;
      partialResult[2] = resultInput;
#ENDIF PARTIAL2
#IF FINAL
      tempDecWritable = new HiveDecimalWritable();
      fullResult = new HiveDecimalWritable();
#ENDIF FINAL
    }

#IF PARTIAL2
    private void initPartialResultInspector() {
#ENDIF PARTIAL2
#IF FINAL
    private void initFullResultInspector() {
#ENDIF FINAL

      // the output type of the vectorized partial aggregate must match the
      // expected type for the row-mode aggregation
      // For decimal, the type is "same number of integer digits and 4 more decimal digits"

      DecimalTypeInfo decTypeInfo =
          GenericUDAFAverageEvaluatorDecimal.deriveResultDecimalTypeInfo(
               inputPrecision, inputScale, mode);
      this.sumScale = decTypeInfo.scale();
      this.sumPrecision = decTypeInfo.precision();

#IF PARTIAL2
      List<ObjectInspector> foi = new ArrayList<ObjectInspector>();
      foi.add(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
      foi.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo));
      List<String> fname = new ArrayList<String>();
      fname.add("count");
      fname.add("sum");
      fname.add("input");
      soi = ObjectInspectorFactory.getStandardStructObjectInspector(fname, foi);
#ENDIF PARTIAL2
#IF FINAL
      oi = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(decTypeInfo);
#ENDIF FINAL
    }

    private Aggregation getCurrentAggregationBuffer(
        VectorAggregationBufferRow[] aggregationBufferSets,
        int bufferIndex,
        int row) {
      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(bufferIndex);
      return myagg;
    }

    @Override
    public void aggregateInputSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      VectorizedRowBatch batch) throws HiveException {

      int batchSize = batch.size;

      if (batchSize == 0) {
        return;
      }

      inputExpression.evaluate(batch);

      StructColumnVector inputStructColVector =
          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
      ColumnVector[] fields = inputStructColVector.fields;

      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
      HiveDecimalWritable[] sumVector = ((DecimalColumnVector) fields[sumOffset]).vector;

      if (inputStructColVector.noNulls) {
        if (inputStructColVector.isRepeating) {
          iterateNoNullsRepeatingWithAggregationSelection(
            aggregationBufferSets, bufferIndex,
            countVector[0], sumVector[0], batchSize);
        } else {
          if (batch.selectedInUse) {
            iterateNoNullsSelectionWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector, sumVector, batch.selected, batchSize);
          } else {
            iterateNoNullsWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector, sumVector, batchSize);
          }
        }
      } else {
        if (inputStructColVector.isRepeating) {
          if (batch.selectedInUse) {
            iterateHasNullsRepeatingSelectionWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector[0], sumVector[0], batchSize, batch.selected, inputStructColVector.isNull);
          } else {
            iterateHasNullsRepeatingWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector[0], sumVector[0], batchSize, inputStructColVector.isNull);
          }
        } else {
          if (batch.selectedInUse) {
            iterateHasNullsSelectionWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector, sumVector, batchSize, batch.selected, inputStructColVector.isNull);
          } else {
            iterateHasNullsWithAggregationSelection(
              aggregationBufferSets, bufferIndex,
              countVector, sumVector, batchSize, inputStructColVector.isNull);
          }
        }
      }
    }

    private void iterateNoNullsRepeatingWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long count,
      HiveDecimalWritable sum,
      int batchSize) {

      for (int i=0; i < batchSize; ++i) {
        Aggregation myagg = getCurrentAggregationBuffer(
          aggregationBufferSets,
          bufferIndex,
          i);
        myagg.merge(count, sum);
      }
    }

    private void iterateNoNullsSelectionWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long[] countVector,
      HiveDecimalWritable[] sumVector,
      int[] selection,
      int batchSize) {

      for (int i=0; i < batchSize; ++i) {
        Aggregation myagg = getCurrentAggregationBuffer(
          aggregationBufferSets,
          bufferIndex,
          i);
        final int batchIndex = selection[i];
        myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
      }
    }

    private void iterateNoNullsWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long[] countVector,
      HiveDecimalWritable[] sumVector,
      int batchSize) {
      for (int i=0; i < batchSize; ++i) {
        Aggregation myagg = getCurrentAggregationBuffer(
          aggregationBufferSets,
          bufferIndex,
          i);
        myagg.merge(countVector[i], sumVector[i]);
      }
    }

    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long count,
      HiveDecimalWritable sum,
      int batchSize,
      int[] selection,
      boolean[] isNull) {

      if (isNull[0]) {
        return;
      }

      for (int i=0; i < batchSize; ++i) {
        Aggregation myagg = getCurrentAggregationBuffer(
          aggregationBufferSets,
          bufferIndex,
          i);
        myagg.merge(count, sum);
      }

    }

    private void iterateHasNullsRepeatingWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long count,
      HiveDecimalWritable sum,
      int batchSize,
      boolean[] isNull) {

      if (isNull[0]) {
        return;
      }

      for (int i=0; i < batchSize; ++i) {
        Aggregation myagg = getCurrentAggregationBuffer(
          aggregationBufferSets,
          bufferIndex,
          i);
        myagg.merge(count, sum);
      }
    }

    private void iterateHasNullsSelectionWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long[] countVector,
      HiveDecimalWritable[] sumVector,
      int batchSize,
      int[] selection,
      boolean[] isNull) {

      for (int i = 0; i < batchSize; i++) {
        final int batchIndex = selection[i];
        if (!isNull[batchIndex]) {
          Aggregation myagg = getCurrentAggregationBuffer(
            aggregationBufferSets,
            bufferIndex,
            i);
          myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
        }
      }
   }

    private void iterateHasNullsWithAggregationSelection(
      VectorAggregationBufferRow[] aggregationBufferSets,
      int bufferIndex,
      long[] countVector,
      HiveDecimalWritable[] sumVector,
      int batchSize,
      boolean[] isNull) {

      for (int i=0; i < batchSize; ++i) {
        if (!isNull[i]) {
          Aggregation myagg = getCurrentAggregationBuffer(
            aggregationBufferSets,
            bufferIndex,
            i);
          myagg.merge(countVector[i], sumVector[i]);
        }
      }
   }

    @Override
    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
        throws HiveException {

      inputExpression.evaluate(batch);

      StructColumnVector inputStructColVector =
          (StructColumnVector) batch.cols[this.inputExpression.getOutputColumn()];
      ColumnVector[] fields = inputStructColVector.fields;

      long[] countVector = ((LongColumnVector) fields[countOffset]).vector;
      HiveDecimalWritable[] sumVector = ((DecimalColumnVector) fields[sumOffset]).vector;

      int batchSize = batch.size;

      if (batchSize == 0) {
        return;
      }

      Aggregation myagg = (Aggregation)agg;

      if (inputStructColVector.isRepeating) {
        if (inputStructColVector.noNulls) {
          if (myagg.isNull) {
            myagg.isNull = false;
            myagg.mergeSum.setFromLong(0L);
            myagg.mergeCount = 0;
          }
          myagg.mergeCount += countVector[0] * batchSize;
          HiveDecimal sum = sumVector[0].getHiveDecimal();
          HiveDecimal multiple = sum.multiply(HiveDecimal.create(batchSize));
          myagg.mergeSum.mutateAdd(multiple);
        }
        return;
      }

      if (!batch.selectedInUse && inputStructColVector.noNulls) {
        iterateNoSelectionNoNulls(myagg, countVector, sumVector, batchSize);
      } else if (!batch.selectedInUse) {
        iterateNoSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull);
      } else if (inputStructColVector.noNulls){
        iterateSelectionNoNulls(myagg, countVector, sumVector, batchSize, batch.selected);
      } else {
        iterateSelectionHasNulls(myagg, countVector, sumVector, batchSize, inputStructColVector.isNull, batch.selected);
      }
    }

    private void iterateSelectionHasNulls(
        Aggregation myagg,
        long[] countVector,
        HiveDecimalWritable[] sumVector,
        int batchSize,
        boolean[] isNull,
        int[] selected) {

      for (int i = 0; i < batchSize; i++) {
        final int batchIndex = selected[i];
        if (!isNull[batchIndex]) {
          myagg.merge(countVector[batchIndex], sumVector[batchIndex]);
        }
      }
    }

    private void iterateSelectionNoNulls(
        Aggregation myagg,
        long[] countVector,
        HiveDecimalWritable[] sumVector,
        int batchSize,
        int[] selected) {

      if (myagg.isNull) {
        myagg.isNull = false;
        myagg.mergeSum.setFromLong(0L);
        myagg.mergeCount = 0;
      }

      for (int i = 0; i< batchSize; i++) {
        final int batchIndex = selected[i];
        myagg.mergeNoNullCheck(countVector[batchIndex], sumVector[batchIndex]);
      }
    }

    private void iterateNoSelectionHasNulls(
        Aggregation myagg,
        long[] countVector,
        HiveDecimalWritable[] sumVector,
        int batchSize,
        boolean[] isNull) {

      for(int i = 0; i < batchSize; i++) {
        if (!isNull[i]) {
          myagg.merge(countVector[i], sumVector[i]);
        }
      }
    }

    private void iterateNoSelectionNoNulls(
        Aggregation myagg,
        long[] countVector,
        HiveDecimalWritable[] sumVector,
        int batchSize) {
      if (myagg.isNull) {
        myagg.isNull = false;
        myagg.mergeSum.setFromLong(0L);
        myagg.mergeCount = 0;
      }

      for (int i = 0; i < batchSize; i++) {
        myagg.mergeNoNullCheck(countVector[i], sumVector[i]);
      }
    }

    @Override
    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
      return new Aggregation();
    }

    @Override
    public void reset(AggregationBuffer agg) throws HiveException {
      Aggregation myAgg = (Aggregation) agg;
      myAgg.reset();
    }

    @Override
    public Object evaluateOutput(
        AggregationBuffer agg) throws HiveException {
      Aggregation myagg = (Aggregation) agg;
      // !isSet checks for overflow.
      if (myagg.isNull || !myagg.mergeSum.isSet()) {
        return null;
      }
      else {
        Preconditions.checkState(myagg.mergeCount > 0);
#IF PARTIAL2
        resultCount.set (myagg.mergeCount);
        resultSum.set(myagg.mergeSum);
        return partialResult;
#ENDIF PARTIAL2
#IF FINAL
        tempDecWritable.setFromLong (myagg.mergeCount);
        fullResult.set(myagg.mergeSum);
        fullResult.mutateDivide(tempDecWritable);
        fullResult.mutateEnforcePrecisionScale(sumPrecision, sumScale);
        return fullResult;
#ENDIF FINAL
      }
    }

  @Override
    public ObjectInspector getOutputObjectInspector() {
#IF PARTIAL2
    return soi;
#ENDIF PARTIAL2
#IF FINAL
    return oi;
#ENDIF FINAL
  }

  @Override
  public long getAggregationBufferFixedSize() {
    JavaDataModel model = JavaDataModel.get();
    return JavaDataModel.alignUp(
      model.object() +
      model.primitive2() * 2,
      model.memoryAlign());
  }

  @Override
  public void init(AggregationDesc desc) throws HiveException {
    init();

    ExprNodeDesc inputExpr = desc.getParameters().get(0);

     StructTypeInfo partialStructTypeInfo = (StructTypeInfo) inputExpr.getTypeInfo();

    ArrayList<String> fieldNames =  partialStructTypeInfo.getAllStructFieldNames();
    countOffset = fieldNames.indexOf("count");
    sumOffset = fieldNames.indexOf("sum");
    inputOffset = fieldNames.indexOf("input");

    DecimalTypeInfo tiInput = (DecimalTypeInfo) partialStructTypeInfo.getAllStructFieldTypeInfos().get(sumOffset);
    this.inputScale = tiInput.scale();
    this.inputPrecision = tiInput.precision();

#IF PARTIAL2
    initPartialResultInspector();
#ENDIF PARTIAL2
#IF FINAL
    initFullResultInspector();
#ENDIF FINAL
  }
}

