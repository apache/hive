PREHOOK: query: CREATE TABLE over1k_n2(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(20,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1k_n2
POSTHOOK: query: CREATE TABLE over1k_n2(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(20,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1k_n2
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_n2
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@over1k_n2
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_n2
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@over1k_n2
PREHOOK: query: CREATE TABLE t1_n48(`dec` decimal(22,2), value_dec decimal(22,2)) STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1_n48
POSTHOOK: query: CREATE TABLE t1_n48(`dec` decimal(22,2), value_dec decimal(22,2)) STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1_n48
PREHOOK: query: INSERT INTO TABLE t1_n48 select `dec`, cast(d as decimal(22,2)) from over1k_n2
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k_n2
PREHOOK: Output: default@t1_n48
POSTHOOK: query: INSERT INTO TABLE t1_n48 select `dec`, cast(d as decimal(22,2)) from over1k_n2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k_n2
POSTHOOK: Output: default@t1_n48
POSTHOOK: Lineage: t1_n48.dec EXPRESSION [(over1k_n2)over1k_n2.FieldSchema(name:dec, type:decimal(20,2), comment:null), ]
POSTHOOK: Lineage: t1_n48.value_dec EXPRESSION [(over1k_n2)over1k_n2.FieldSchema(name:d, type:double, comment:null), ]
PREHOOK: query: CREATE TABLE t2_n29(`dec` decimal(24,0), value_dec decimal(24,0)) STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t2_n29
POSTHOOK: query: CREATE TABLE t2_n29(`dec` decimal(24,0), value_dec decimal(24,0)) STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t2_n29
PREHOOK: query: INSERT INTO TABLE t2_n29 select `dec`, cast(d as decimal(24,0)) from over1k_n2
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k_n2
PREHOOK: Output: default@t2_n29
POSTHOOK: query: INSERT INTO TABLE t2_n29 select `dec`, cast(d as decimal(24,0)) from over1k_n2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k_n2
POSTHOOK: Output: default@t2_n29
POSTHOOK: Lineage: t2_n29.dec EXPRESSION [(over1k_n2)over1k_n2.FieldSchema(name:dec, type:decimal(20,2), comment:null), ]
POSTHOOK: Lineage: t2_n29.value_dec EXPRESSION [(over1k_n2)over1k_n2.FieldSchema(name:d, type:double, comment:null), ]
PREHOOK: query: explain vectorization detail
select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_n29
                  Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(24,0), 1:value_dec:decimal(24,0), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(24,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(24,0))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(26,2))
                          1 _col0 (type: decimal(26,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(24,0), value_dec:decimal(24,0)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n48
                  Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(22,2), 1:value_dec:decimal(22,2), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(22,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(22,2))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(26,2))
                          1 _col0 (type: decimal(26,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: col 0:decimal(26,2)
                            bigTableValueExpressions: col 0:decimal(26,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 258473 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 258473 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(22,2), value_dec:decimal(22,2)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
45.00	45
45.00	45
45.00	45
45.00	45
45.00	45
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
PREHOOK: query: select count(*) from (select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_n48.`dec`, t2_n29.`dec` from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
106
PREHOOK: query: explain vectorization detail
select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_n29
                  Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(24,0), 1:value_dec:decimal(24,0), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(24,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(24,0)), value_dec (type: decimal(24,0))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(26,2))
                          1 _col0 (type: decimal(26,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(24,0), value_dec:decimal(24,0)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n48
                  Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(22,2), 1:value_dec:decimal(22,2), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(22,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(22,2)), value_dec (type: decimal(22,2))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 234976 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(26,2))
                          1 _col0 (type: decimal(26,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: col 0:decimal(26,2)
                            bigTableValueExpressions: col 0:decimal(26,2), col 1:decimal(22,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1, _col2, _col3
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 258473 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 258473 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(22,2), value_dec:decimal(22,2)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(24,0)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
14.00	33.66	14	10
14.00	33.66	14	22
14.00	33.66	14	34
14.00	33.66	14	39
14.00	33.66	14	42
14.00	33.66	14	45
14.00	33.66	14	46
14.00	33.66	14	49
14.00	33.66	14	5
17.00	14.26	17	1
17.00	14.26	17	14
17.00	14.26	17	16
17.00	14.26	17	19
17.00	14.26	17	2
17.00	14.26	17	22
17.00	14.26	17	29
17.00	14.26	17	3
17.00	14.26	17	4
17.00	14.26	17	44
45.00	23.55	45	1
45.00	23.55	45	2
45.00	23.55	45	22
45.00	23.55	45	24
45.00	23.55	45	42
6.00	29.78	6	16
6.00	29.78	6	28
6.00	29.78	6	30
6.00	29.78	6	34
6.00	29.78	6	36
6.00	29.78	6	44
62.00	21.02	62	15
62.00	21.02	62	15
62.00	21.02	62	21
62.00	21.02	62	21
62.00	21.02	62	22
62.00	21.02	62	25
62.00	21.02	62	29
62.00	21.02	62	3
62.00	21.02	62	34
62.00	21.02	62	47
62.00	21.02	62	47
62.00	21.02	62	49
64.00	37.76	64	0
64.00	37.76	64	10
64.00	37.76	64	10
64.00	37.76	64	13
64.00	37.76	64	23
64.00	37.76	64	25
64.00	37.76	64	26
64.00	37.76	64	27
64.00	37.76	64	27
64.00	37.76	64	30
64.00	37.76	64	32
64.00	37.76	64	34
64.00	37.76	64	35
64.00	37.76	64	38
64.00	37.76	64	40
64.00	37.76	64	43
64.00	37.76	64	5
64.00	37.76	64	50
70.00	24.59	70	2
70.00	24.59	70	25
70.00	24.59	70	27
70.00	24.59	70	28
70.00	24.59	70	3
70.00	24.59	70	32
70.00	24.59	70	44
79.00	15.12	79	1
79.00	15.12	79	15
79.00	15.12	79	25
79.00	15.12	79	30
79.00	15.12	79	35
79.00	15.12	79	35
89.00	15.09	89	1
89.00	15.09	89	15
89.00	15.09	89	23
89.00	15.09	89	27
89.00	15.09	89	28
89.00	15.09	89	29
89.00	15.09	89	30
89.00	15.09	89	32
89.00	15.09	89	39
89.00	15.09	89	40
89.00	15.09	89	45
89.00	15.09	89	7
9.00	48.96	9	12
9.00	48.96	9	15
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	20
9.00	48.96	9	20
9.00	48.96	9	21
9.00	48.96	9	21
9.00	48.96	9	26
9.00	48.96	9	27
9.00	48.96	9	34
9.00	48.96	9	38
9.00	48.96	9	41
9.00	48.96	9	42
9.00	48.96	9	45
9.00	48.96	9	48
9.00	48.96	9	49
9.00	48.96	9	5
9.00	48.96	9	7
9.00	48.96	9	7
PREHOOK: query: select count(*) from (select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n48
PREHOOK: Input: default@t2_n29
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_n48.`dec`, t1_n48.value_dec, t2_n29.`dec`, t2_n29.value_dec from t1_n48 join t2_n29 on (t1_n48.`dec`=t2_n29.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n48
POSTHOOK: Input: default@t2_n29
#### A masked pattern was here ####
106
PREHOOK: query: CREATE TABLE over1k_small(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(14,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1k_small
POSTHOOK: query: CREATE TABLE over1k_small(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(14,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1k_small
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_small
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@over1k_small
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_small
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@over1k_small
PREHOOK: query: CREATE TABLE t1_small(`dec` decimal(14,2), value_dec decimal(14,2)) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1_small
POSTHOOK: query: CREATE TABLE t1_small(`dec` decimal(14,2), value_dec decimal(14,2)) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1_small
PREHOOK: query: INSERT INTO TABLE t1_small select `dec`, cast(d as decimal(14,2)) from over1k_small
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k_small
PREHOOK: Output: default@t1_small
POSTHOOK: query: INSERT INTO TABLE t1_small select `dec`, cast(d as decimal(14,2)) from over1k_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k_small
POSTHOOK: Output: default@t1_small
POSTHOOK: Lineage: t1_small.dec SIMPLE [(over1k_small)over1k_small.FieldSchema(name:dec, type:decimal(14,2), comment:null), ]
POSTHOOK: Lineage: t1_small.value_dec EXPRESSION [(over1k_small)over1k_small.FieldSchema(name:d, type:double, comment:null), ]
PREHOOK: query: CREATE TABLE t2_small(`dec` decimal(14,0), value_dec decimal(14,0)) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t2_small
POSTHOOK: query: CREATE TABLE t2_small(`dec` decimal(14,0), value_dec decimal(14,0)) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t2_small
PREHOOK: query: INSERT INTO TABLE t2_small select `dec`, cast(d as decimal(14,0)) from over1k_small
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k_small
PREHOOK: Output: default@t2_small
POSTHOOK: query: INSERT INTO TABLE t2_small select `dec`, cast(d as decimal(14,0)) from over1k_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k_small
POSTHOOK: Output: default@t2_small
POSTHOOK: Lineage: t2_small.dec EXPRESSION [(over1k_small)over1k_small.FieldSchema(name:dec, type:decimal(14,2), comment:null), ]
POSTHOOK: Lineage: t2_small.value_dec EXPRESSION [(over1k_small)over1k_small.FieldSchema(name:d, type:double, comment:null), ]
PREHOOK: query: explain vectorization detail
select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_small
                  Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,0)/DECIMAL_64, 1:value_dec:decimal(14,0)/DECIMAL_64, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 3:decimal(14,0))(children: ConvertDecimal64ToDecimal(col 0:decimal(14,0)/DECIMAL_64) -> 3:decimal(14,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,0))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(14,0)/DECIMAL_64, value_dec:decimal(14,0)/DECIMAL_64
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(14,0)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_small
                  Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,2)/DECIMAL_64, 1:value_dec:decimal(14,2)/DECIMAL_64, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 3:decimal(14,2))(children: ConvertDecimal64ToDecimal(col 0:decimal(14,2)/DECIMAL_64) -> 3:decimal(14,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,2))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: ConvertDecimal64ToDecimal(col 0:decimal(16,2)/DECIMAL_64) -> 4:decimal(16,2)
                            bigTableValueExpressions: ConvertDecimal64ToDecimal(col 0:decimal(16,2)/DECIMAL_64) -> 5:decimal(16,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(14,2)/DECIMAL_64, value_dec:decimal(14,2)/DECIMAL_64
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(14,2), decimal(16,2), decimal(16,2)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
45.00	45
45.00	45
45.00	45
45.00	45
45.00	45
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
PREHOOK: query: select count(*) from (select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
106
PREHOOK: query: explain vectorization detail
select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_small
                  Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,0)/DECIMAL_64, 1:value_dec:decimal(14,0)/DECIMAL_64, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 3:decimal(14,0))(children: ConvertDecimal64ToDecimal(col 0:decimal(14,0)/DECIMAL_64) -> 3:decimal(14,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,0)), value_dec (type: decimal(14,0))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(14,0)/DECIMAL_64, value_dec:decimal(14,0)/DECIMAL_64
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(14,0)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_small
                  Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,2)/DECIMAL_64, 1:value_dec:decimal(14,2)/DECIMAL_64, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 3:decimal(14,2))(children: ConvertDecimal64ToDecimal(col 0:decimal(14,2)/DECIMAL_64) -> 3:decimal(14,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,2)), value_dec (type: decimal(14,2))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: ConvertDecimal64ToDecimal(col 0:decimal(16,2)/DECIMAL_64) -> 4:decimal(16,2)
                            bigTableValueExpressions: ConvertDecimal64ToDecimal(col 0:decimal(16,2)/DECIMAL_64) -> 5:decimal(16,2), ConvertDecimal64ToDecimal(col 1:decimal(14,2)/DECIMAL_64) -> 3:decimal(14,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1, _col2, _col3
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(14,2)/DECIMAL_64, value_dec:decimal(14,2)/DECIMAL_64
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(14,2), decimal(16,2), decimal(16,2), decimal(14,0)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
14.00	33.66	14	10
14.00	33.66	14	22
14.00	33.66	14	34
14.00	33.66	14	39
14.00	33.66	14	42
14.00	33.66	14	45
14.00	33.66	14	46
14.00	33.66	14	49
14.00	33.66	14	5
17.00	14.26	17	1
17.00	14.26	17	14
17.00	14.26	17	16
17.00	14.26	17	19
17.00	14.26	17	2
17.00	14.26	17	22
17.00	14.26	17	29
17.00	14.26	17	3
17.00	14.26	17	4
17.00	14.26	17	44
45.00	23.55	45	1
45.00	23.55	45	2
45.00	23.55	45	22
45.00	23.55	45	24
45.00	23.55	45	42
6.00	29.78	6	16
6.00	29.78	6	28
6.00	29.78	6	30
6.00	29.78	6	34
6.00	29.78	6	36
6.00	29.78	6	44
62.00	21.02	62	15
62.00	21.02	62	15
62.00	21.02	62	21
62.00	21.02	62	21
62.00	21.02	62	22
62.00	21.02	62	25
62.00	21.02	62	29
62.00	21.02	62	3
62.00	21.02	62	34
62.00	21.02	62	47
62.00	21.02	62	47
62.00	21.02	62	49
64.00	37.76	64	0
64.00	37.76	64	10
64.00	37.76	64	10
64.00	37.76	64	13
64.00	37.76	64	23
64.00	37.76	64	25
64.00	37.76	64	26
64.00	37.76	64	27
64.00	37.76	64	27
64.00	37.76	64	30
64.00	37.76	64	32
64.00	37.76	64	34
64.00	37.76	64	35
64.00	37.76	64	38
64.00	37.76	64	40
64.00	37.76	64	43
64.00	37.76	64	5
64.00	37.76	64	50
70.00	24.59	70	2
70.00	24.59	70	25
70.00	24.59	70	27
70.00	24.59	70	28
70.00	24.59	70	3
70.00	24.59	70	32
70.00	24.59	70	44
79.00	15.12	79	1
79.00	15.12	79	15
79.00	15.12	79	25
79.00	15.12	79	30
79.00	15.12	79	35
79.00	15.12	79	35
89.00	15.09	89	1
89.00	15.09	89	15
89.00	15.09	89	23
89.00	15.09	89	27
89.00	15.09	89	28
89.00	15.09	89	29
89.00	15.09	89	30
89.00	15.09	89	32
89.00	15.09	89	39
89.00	15.09	89	40
89.00	15.09	89	45
89.00	15.09	89	7
9.00	48.96	9	12
9.00	48.96	9	15
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	20
9.00	48.96	9	20
9.00	48.96	9	21
9.00	48.96	9	21
9.00	48.96	9	26
9.00	48.96	9	27
9.00	48.96	9	34
9.00	48.96	9	38
9.00	48.96	9	41
9.00	48.96	9	42
9.00	48.96	9	45
9.00	48.96	9	48
9.00	48.96	9	49
9.00	48.96	9	5
9.00	48.96	9	7
9.00	48.96	9	7
PREHOOK: query: select count(*) from (select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
106
PREHOOK: query: explain vectorization detail
select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_small
                  Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,0), 1:value_dec:decimal(14,0), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(14,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,0))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [[] is disabled because it is not in hive.vectorized.input.format.supports.enabled []]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(14,0), value_dec:decimal(14,0)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_small
                  Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,2), 1:value_dec:decimal(14,2), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(14,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,2))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0]
                      Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: col 0:decimal(16,2)
                            bigTableValueExpressions: col 0:decimal(16,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [[] is disabled because it is not in hive.vectorized.input.format.supports.enabled []]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: dec:decimal(14,2), value_dec:decimal(14,2)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
14.00	14
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
17.00	17
45.00	45
45.00	45
45.00	45
45.00	45
45.00	45
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
6.00	6
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
62.00	62
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
64.00	64
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
70.00	70
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
79.00	79
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
89.00	89
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
9.00	9
PREHOOK: query: select count(*) from (select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_small.`dec`, t2_small.`dec` from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
106
PREHOOK: query: explain vectorization detail
select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: explain vectorization detail
select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: t2_small
                  Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,0), 1:value_dec:decimal(14,0), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(14,0))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,0)), value_dec (type: decimal(14,0))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 4966 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        Spark Hash Table Sink Vectorization:
                            className: VectorSparkHashTableSinkOperator
                            native: true
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [[] is disabled because it is not in hive.vectorized.input.format.supports.enabled []]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(14,0), value_dec:decimal(14,0)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_small
                  Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:dec:decimal(14,2), 1:value_dec:decimal(14,2), 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 0:decimal(14,2))
                    predicate: dec is not null (type: boolean)
                    Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: dec (type: decimal(14,2)), value_dec (type: decimal(14,2))
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1049 Data size: 11234 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: decimal(16,2))
                          1 _col0 (type: decimal(16,2))
                        Map Join Vectorization:
                            bigTableKeyExpressions: col 0:decimal(16,2)
                            bigTableValueExpressions: col 0:decimal(16,2), col 1:decimal(14,2)
                            className: VectorMapJoinOperator
                            native: false
                            nativeConditionsMet: hive.mapjoin.optimized.hashtable IS true, hive.vectorized.execution.mapjoin.native.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, One MapJoin Condition IS true, No nullsafe IS true, Small table vectorizes IS true
                            nativeConditionsNotMet: Optimized Table and Supports Key Types IS false
                            nativeNotSupportedKeyTypes: DECIMAL
                        outputColumnNames: _col0, _col1, _col2, _col3
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                        File Output Operator
                          compressed: false
                          File Sink Vectorization:
                              className: VectorFileSinkOperator
                              native: false
                          Statistics: Num rows: 1153 Data size: 12357 Basic stats: COMPLETE Column stats: NONE
                          table:
                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [[] is disabled because it is not in hive.vectorized.input.format.supports.enabled []]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: dec:decimal(14,2), value_dec:decimal(14,2)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(14,0)]
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
14.00	33.66	14	10
14.00	33.66	14	22
14.00	33.66	14	34
14.00	33.66	14	39
14.00	33.66	14	42
14.00	33.66	14	45
14.00	33.66	14	46
14.00	33.66	14	49
14.00	33.66	14	5
17.00	14.26	17	1
17.00	14.26	17	14
17.00	14.26	17	16
17.00	14.26	17	19
17.00	14.26	17	2
17.00	14.26	17	22
17.00	14.26	17	29
17.00	14.26	17	3
17.00	14.26	17	4
17.00	14.26	17	44
45.00	23.55	45	1
45.00	23.55	45	2
45.00	23.55	45	22
45.00	23.55	45	24
45.00	23.55	45	42
6.00	29.78	6	16
6.00	29.78	6	28
6.00	29.78	6	30
6.00	29.78	6	34
6.00	29.78	6	36
6.00	29.78	6	44
62.00	21.02	62	15
62.00	21.02	62	15
62.00	21.02	62	21
62.00	21.02	62	21
62.00	21.02	62	22
62.00	21.02	62	25
62.00	21.02	62	29
62.00	21.02	62	3
62.00	21.02	62	34
62.00	21.02	62	47
62.00	21.02	62	47
62.00	21.02	62	49
64.00	37.76	64	0
64.00	37.76	64	10
64.00	37.76	64	10
64.00	37.76	64	13
64.00	37.76	64	23
64.00	37.76	64	25
64.00	37.76	64	26
64.00	37.76	64	27
64.00	37.76	64	27
64.00	37.76	64	30
64.00	37.76	64	32
64.00	37.76	64	34
64.00	37.76	64	35
64.00	37.76	64	38
64.00	37.76	64	40
64.00	37.76	64	43
64.00	37.76	64	5
64.00	37.76	64	50
70.00	24.59	70	2
70.00	24.59	70	25
70.00	24.59	70	27
70.00	24.59	70	28
70.00	24.59	70	3
70.00	24.59	70	32
70.00	24.59	70	44
79.00	15.12	79	1
79.00	15.12	79	15
79.00	15.12	79	25
79.00	15.12	79	30
79.00	15.12	79	35
79.00	15.12	79	35
89.00	15.09	89	1
89.00	15.09	89	15
89.00	15.09	89	23
89.00	15.09	89	27
89.00	15.09	89	28
89.00	15.09	89	29
89.00	15.09	89	30
89.00	15.09	89	32
89.00	15.09	89	39
89.00	15.09	89	40
89.00	15.09	89	45
89.00	15.09	89	7
9.00	48.96	9	12
9.00	48.96	9	15
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	2
9.00	48.96	9	20
9.00	48.96	9	20
9.00	48.96	9	21
9.00	48.96	9	21
9.00	48.96	9	26
9.00	48.96	9	27
9.00	48.96	9	34
9.00	48.96	9	38
9.00	48.96	9	41
9.00	48.96	9	42
9.00	48.96	9	45
9.00	48.96	9	48
9.00	48.96	9	49
9.00	48.96	9	5
9.00	48.96	9	7
9.00	48.96	9	7
PREHOOK: query: select count(*) from (select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_small
PREHOOK: Input: default@t2_small
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from (select t1_small.`dec`, t1_small.value_dec, t2_small.`dec`, t2_small.value_dec from t1_small join t2_small on (t1_small.`dec`=t2_small.`dec`)) as t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_small
POSTHOOK: Input: default@t2_small
#### A masked pattern was here ####
106
