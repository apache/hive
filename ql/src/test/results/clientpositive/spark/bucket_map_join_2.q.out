PREHOOK: query: drop table table1
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table table1
POSTHOOK: type: DROPTABLE
PREHOOK: query: drop table table2
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table table2
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table table1(key string, value string) clustered by (key, value)
sorted by (key desc, value desc) into 1 BUCKETS stored as textfile
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table1
POSTHOOK: query: create table table1(key string, value string) clustered by (key, value)
sorted by (key desc, value desc) into 1 BUCKETS stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table1
PREHOOK: query: create table table2(key string, value string) clustered by (value, key)
sorted by (value desc, key desc) into 1 BUCKETS stored as textfile
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table2
POSTHOOK: query: create table table2(key string, value string) clustered by (value, key)
sorted by (value desc, key desc) into 1 BUCKETS stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table2
PREHOOK: query: load data local inpath '../../data/files/SortCol1Col2.txt' overwrite into table table1
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@table1
POSTHOOK: query: load data local inpath '../../data/files/SortCol1Col2.txt' overwrite into table table1
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@table1
PREHOOK: query: load data local inpath '../../data/files/SortCol2Col1.txt' overwrite into table table2
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@table2
POSTHOOK: query: load data local inpath '../../data/files/SortCol2Col1.txt' overwrite into table table2
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@table2
PREHOOK: query: -- The tables are bucketed in same columns in different order,
-- but sorted in different column orders
-- Neither bucketed map-join, nor sort-merge join should be performed

explain extended
select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value
PREHOOK: type: QUERY
POSTHOOK: query: -- The tables are bucketed in same columns in different order,
-- but sorted in different column orders
-- Neither bucketed map-join, nor sort-merge join should be performed

explain extended
select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_JOIN
         TOK_TABREF
            TOK_TABNAME
               table1
            a
         TOK_TABREF
            TOK_TABNAME
               table2
            b
         and
            =
               .
                  TOK_TABLE_OR_COL
                     a
                  key
               .
                  TOK_TABLE_OR_COL
                     b
                  key
            =
               .
                  TOK_TABLE_OR_COL
                     a
                  value
               .
                  TOK_TABLE_OR_COL
                     b
                  value
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_HINTLIST
            TOK_HINT
               TOK_MAPJOIN
               TOK_HINTARGLIST
                  b
         TOK_SELEXPR
            TOK_FUNCTIONSTAR
               count


STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 0 Data size: 21 Basic stats: PARTIAL Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (key is not null and value is not null) (type: boolean)
                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 key (type: string), value (type: string)
                        1 key (type: string), value (type: string)
                      Position of Big Table: 0
            Local Work:
              Map Reduce Local Work
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: table2
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE true
                    SORTBUCKETCOLSPREFIX TRUE
                    bucket_count 1
                    bucket_field_name value
                    columns key,value
                    columns.comments 
                    columns.types string:string
#### A masked pattern was here ####
                    name default.table2
                    numFiles 1
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct table2 { string key, string value}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 21
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE true
                      SORTBUCKETCOLSPREFIX TRUE
                      bucket_count 1
                      bucket_field_name value
                      columns key,value
                      columns.comments 
                      columns.types string:string
#### A masked pattern was here ####
                      name default.table2
                      numFiles 1
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct table2 { string key, string value}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 21
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.table2
                  name: default.table2
            Truncated Path -> Alias:
              /table2 [b]

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 0 Data size: 20 Basic stats: PARTIAL Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (key is not null and value is not null) (type: boolean)
                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 key (type: string), value (type: string)
                        1 key (type: string), value (type: string)
                      input vertices:
                        1 Map 3
                      Position of Big Table: 0
                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
                      Group By Operator
                        aggregations: count()
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          sort order: 
                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                          tag: -1
                          value expressions: _col0 (type: bigint)
                          auto parallelism: false
            Local Work:
              Map Reduce Local Work
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: table1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE true
                    SORTBUCKETCOLSPREFIX TRUE
                    bucket_count 1
                    bucket_field_name key
                    columns key,value
                    columns.comments 
                    columns.types string:string
#### A masked pattern was here ####
                    name default.table1
                    numFiles 1
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct table1 { string key, string value}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 20
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE true
                      SORTBUCKETCOLSPREFIX TRUE
                      bucket_count 1
                      bucket_field_name key
                      columns key,value
                      columns.comments 
                      columns.types string:string
#### A masked pattern was here ####
                      name default.table1
                      numFiles 1
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct table1 { string key, string value}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 20
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.table1
                  name: default.table1
            Truncated Path -> Alias:
              /table1 [a]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: bigint)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        properties:
                          columns _col0
                          columns.types bigint
                          escape.delim \
                          hive.serialization.extend.nesting.levels true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value
PREHOOK: type: QUERY
PREHOOK: Input: default@table1
PREHOOK: Input: default@table2
#### A masked pattern was here ####
POSTHOOK: query: select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table1
POSTHOOK: Input: default@table2
#### A masked pattern was here ####
4
