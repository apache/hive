PREHOOK: query: CREATE TABLE testTable1(key INT, value STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@testTable1
POSTHOOK: query: CREATE TABLE testTable1(key INT, value STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@testTable1
PREHOOK: query: CREATE TABLE testTable2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@testTable2
POSTHOOK: query: CREATE TABLE testTable2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@testTable2
PREHOOK: query: EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1 
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@testtable1
PREHOOK: Output: default@testtable2
POSTHOOK: query: EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1 
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@testtable1
POSTHOOK: Output: default@testtable2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-2
  Stage-4 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      Edges:
        Reducer 2 <- Map 4 (GROUP PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Map 5 (GROUP PARTITION-LEVEL SORT, 2)
#### A masked pattern was here ####
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key < 20) (type: boolean)
                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(DISTINCT substr(value, 5))
                      keys: key (type: string), substr(value, 5) (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key < 20) (type: boolean)
                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(DISTINCT substr(value, 5))
                      keys: key (type: string), value (type: string), substr(value, 5) (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), CAST( _col1 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.testtable1
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(DISTINCT KEY._col2:0._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string), CAST( _col2 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.testtable2

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.testtable1

  Stage: Stage-3
    Stats Work
      Basic Stats Work:

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.testtable2

  Stage: Stage-4
    Stats Work
      Basic Stats Work:

PREHOOK: query: FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@testtable1
PREHOOK: Output: default@testtable2
POSTHOOK: query: FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@testtable1
POSTHOOK: Output: default@testtable2
POSTHOOK: Lineage: testtable1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: testtable1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.val1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT key, value FROM testTable1 ORDER BY 1, 2
PREHOOK: type: QUERY
PREHOOK: Input: default@testtable1
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM testTable1 ORDER BY 1, 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@testtable1
#### A masked pattern was here ####
0	1
2	1
4	1
5	1
8	1
9	1
10	1
11	1
12	1
15	1
17	1
18	1
19	1
PREHOOK: query: SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3
PREHOOK: type: QUERY
PREHOOK: Input: default@testtable2
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@testtable2
#### A masked pattern was here ####
0	val_0	1
2	val_2	1
4	val_4	1
5	val_5	1
8	val_8	1
9	val_9	1
10	val_10	1
11	val_11	1
12	val_12	1
15	val_15	1
17	val_17	1
18	val_18	1
19	val_19	1
PREHOOK: query: EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@testtable1
PREHOOK: Output: default@testtable2
POSTHOOK: query: EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@testtable1
POSTHOOK: Output: default@testtable2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-2
  Stage-4 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      Edges:
        Reducer 2 <- Map 4 (GROUP PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Map 5 (GROUP PARTITION-LEVEL SORT, 2)
#### A masked pattern was here ####
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key < 20) (type: boolean)
                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(DISTINCT substr(value, 5))
                      keys: key (type: string), substr(value, 5) (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key < 20) (type: boolean)
                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(DISTINCT substr(value, 5))
                      keys: value (type: string), key (type: string), substr(value, 5) (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), CAST( _col1 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.testtable1
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(DISTINCT KEY._col2:0._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: UDFToInteger(_col1) (type: int), _col0 (type: string), CAST( _col2 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.testtable2

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.testtable1

  Stage: Stage-3
    Stats Work
      Basic Stats Work:

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.testtable2

  Stage: Stage-4
    Stats Work
      Basic Stats Work:

PREHOOK: query: FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@testtable1
PREHOOK: Output: default@testtable2
POSTHOOK: query: FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@testtable1
POSTHOOK: Output: default@testtable2
POSTHOOK: Lineage: testtable1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: testtable1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.val1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: testtable2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT key, value FROM testTable1 ORDER BY 1, 2
PREHOOK: type: QUERY
PREHOOK: Input: default@testtable1
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM testTable1 ORDER BY 1, 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@testtable1
#### A masked pattern was here ####
0	1
2	1
4	1
5	1
8	1
9	1
10	1
11	1
12	1
15	1
17	1
18	1
19	1
PREHOOK: query: SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3
PREHOOK: type: QUERY
PREHOOK: Input: default@testtable2
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@testtable2
#### A masked pattern was here ####
0	val_0	1
2	val_2	1
4	val_4	1
5	val_5	1
8	val_8	1
9	val_9	1
10	val_10	1
11	val_11	1
12	val_12	1
15	val_15	1
17	val_17	1
18	val_18	1
19	val_19	1
PREHOOK: query: EXPLAIN
SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN
SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 2)
        Reducer 3 <- Reducer 2 (SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (UDFToDouble(key) <= 20.0D) (type: boolean)
                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: bigint), _col0 (type: string)
                  sort order: -+
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 83 Data size: 881 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
0	3
5	3
12	2
15	2
18	2
10	1
11	1
17	1
19	1
2	1
20	1
4	1
8	1
9	1
PREHOOK: query: EXPLAIN
SELECT c1, c2, c3, c4
FROM (
 FROM 
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN 
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b 
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN
SELECT c1, c2, c3, c4
FROM (
 FROM 
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN 
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b 
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 2), Reducer 6 (PARTITION-LEVEL SORT, 2)
        Reducer 4 <- Reducer 3 (SORT, 1)
        Reducer 6 <- Map 5 (GROUP, 2)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ((UDFToDouble(key) < 20.0D) and (UDFToDouble(key) > 15.0D)) (type: boolean)
                    Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string), value (type: string), substr(value, 5) (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2
                        Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                          sort order: +++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src2
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ((UDFToDouble(key) < 20.0D) and (UDFToDouble(key) > 15.0D)) (type: boolean)
                    Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      keys: key (type: string), value (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 27 Data size: 286 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 27 Data size: 286 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    mode: complete
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 13 Data size: 137 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string)
                      sort order: +
                      Map-reduce partition columns: _col0 (type: string)
                      Statistics: Num rows: 13 Data size: 137 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col1 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col0 (type: string)
                  1 _col0 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 29 Data size: 314 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                  sort order: --++
                  Statistics: Num rows: 29 Data size: 314 Basic stats: COMPLETE Column stats: NONE
        Reducer 4 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey2 (type: string), KEY.reducesinkkey3 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 29 Data size: 314 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 29 Data size: 314 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 27 Data size: 286 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 27 Data size: 286 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: string)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT c1, c2, c3, c4
FROM (
 FROM 
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN 
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b 
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT c1, c2, c3, c4
FROM (
 FROM 
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN 
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b 
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
19	val_19	19	val_19
18	val_18	18	val_18
17	val_17	17	val_17
PREHOOK: query: EXPLAIN
SELECT key FROM src ORDER BY 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN
SELECT key FROM src ORDER BY 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: _col0
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string)
                      sort order: +
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key FROM src ORDER BY 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT key FROM src ORDER BY 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
0
0
0
10
100
100
103
103
104
104
105
11
111
113
113
114
116
118
118
119
119
119
12
12
120
120
125
125
126
128
128
128
129
129
131
133
134
134
136
137
137
138
138
138
138
143
145
146
146
149
149
15
15
150
152
152
153
155
156
157
158
160
162
163
164
164
165
165
166
167
167
167
168
169
169
169
169
17
170
172
172
174
174
175
175
176
176
177
178
179
179
18
18
180
181
183
186
187
187
187
189
19
190
191
191
192
193
193
193
194
195
195
196
197
197
199
199
199
2
20
200
200
201
202
203
203
205
205
207
207
208
208
208
209
209
213
213
214
216
216
217
217
218
219
219
221
221
222
223
223
224
224
226
228
229
229
230
230
230
230
230
233
233
235
237
237
238
238
239
239
24
24
241
242
242
244
247
248
249
252
255
255
256
256
257
258
26
26
260
262
263
265
265
266
27
272
272
273
273
273
274
275
277
277
277
277
278
278
28
280
280
281
281
282
282
283
284
285
286
287
288
288
289
291
292
296
298
298
298
30
302
305
306
307
307
308
309
309
310
311
311
311
315
316
316
316
317
317
318
318
318
321
321
322
322
323
325
325
327
327
327
33
331
331
332
333
333
335
336
338
339
34
341
342
342
344
344
345
348
348
348
348
348
35
35
35
351
353
353
356
360
362
364
365
366
367
367
368
369
369
369
37
37
373
374
375
377
378
379
382
382
384
384
384
386
389
392
393
394
395
395
396
396
396
397
397
399
399
4
400
401
401
401
401
401
402
403
403
403
404
404
406
406
406
406
407
409
409
409
41
411
413
413
414
414
417
417
417
418
419
42
42
421
424
424
427
429
429
43
430
430
430
431
431
431
432
435
436
437
438
438
438
439
439
44
443
444
446
448
449
452
453
454
454
454
455
457
458
458
459
459
460
462
462
463
463
466
466
466
467
468
468
468
468
469
469
469
469
469
47
470
472
475
477
478
478
479
480
480
480
481
482
483
484
485
487
489
489
489
489
490
491
492
492
493
494
495
496
497
498
498
498
5
5
5
51
51
53
54
57
58
58
64
65
66
67
67
69
70
70
70
72
72
74
76
76
77
78
8
80
82
83
83
84
84
85
86
87
9
90
90
90
92
95
95
96
97
97
98
98
PREHOOK: query: EXPLAIN
SELECT distinct key FROM src ORDER BY 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN
SELECT distinct key FROM src ORDER BY 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 2)
        Reducer 3 <- Reducer 2 (SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      keys: key (type: string)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT distinct key FROM src ORDER BY 1
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT distinct key FROM src ORDER BY 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
0
10
100
103
104
105
11
111
113
114
116
118
119
12
120
125
126
128
129
131
133
134
136
137
138
143
145
146
149
15
150
152
153
155
156
157
158
160
162
163
164
165
166
167
168
169
17
170
172
174
175
176
177
178
179
18
180
181
183
186
187
189
19
190
191
192
193
194
195
196
197
199
2
20
200
201
202
203
205
207
208
209
213
214
216
217
218
219
221
222
223
224
226
228
229
230
233
235
237
238
239
24
241
242
244
247
248
249
252
255
256
257
258
26
260
262
263
265
266
27
272
273
274
275
277
278
28
280
281
282
283
284
285
286
287
288
289
291
292
296
298
30
302
305
306
307
308
309
310
311
315
316
317
318
321
322
323
325
327
33
331
332
333
335
336
338
339
34
341
342
344
345
348
35
351
353
356
360
362
364
365
366
367
368
369
37
373
374
375
377
378
379
382
384
386
389
392
393
394
395
396
397
399
4
400
401
402
403
404
406
407
409
41
411
413
414
417
418
419
42
421
424
427
429
43
430
431
432
435
436
437
438
439
44
443
444
446
448
449
452
453
454
455
457
458
459
460
462
463
466
467
468
469
47
470
472
475
477
478
479
480
481
482
483
484
485
487
489
490
491
492
493
494
495
496
497
498
5
51
53
54
57
58
64
65
66
67
69
70
72
74
76
77
78
8
80
82
83
84
85
86
87
9
90
92
95
96
97
98
