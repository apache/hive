PREHOOK: query: CREATE TABLE part_table1 (col int) PARTITIONED BY (part1_col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_table1
POSTHOOK: query: CREATE TABLE part_table1 (col int) PARTITIONED BY (part1_col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_table1
PREHOOK: query: CREATE TABLE part_table2 (col int) PARTITIONED BY (part2_col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_table2
POSTHOOK: query: CREATE TABLE part_table2 (col int) PARTITIONED BY (part2_col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_table2
PREHOOK: query: CREATE TABLE part_table3 (col int) PARTITIONED BY (part3_col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_table3
POSTHOOK: query: CREATE TABLE part_table3 (col int) PARTITIONED BY (part3_col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_table3
PREHOOK: query: CREATE TABLE part_table4 (col int) PARTITIONED BY (part4_col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_table4
POSTHOOK: query: CREATE TABLE part_table4 (col int) PARTITIONED BY (part4_col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_table4
PREHOOK: query: CREATE TABLE part_table5 (col int) PARTITIONED BY (part5_col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_table5
POSTHOOK: query: CREATE TABLE part_table5 (col int) PARTITIONED BY (part5_col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_table5
PREHOOK: query: CREATE TABLE reg_table (col int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@reg_table
POSTHOOK: query: CREATE TABLE reg_table (col int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@reg_table
PREHOOK: query: ALTER TABLE part_table1 ADD PARTITION (part1_col = 1)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table1
POSTHOOK: query: ALTER TABLE part_table1 ADD PARTITION (part1_col = 1)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table1
POSTHOOK: Output: default@part_table1@part1_col=1
PREHOOK: query: ALTER TABLE part_table2 ADD PARTITION (part2_col = 1)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table2
POSTHOOK: query: ALTER TABLE part_table2 ADD PARTITION (part2_col = 1)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table2
POSTHOOK: Output: default@part_table2@part2_col=1
PREHOOK: query: ALTER TABLE part_table2 ADD PARTITION (part2_col = 2)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table2
POSTHOOK: query: ALTER TABLE part_table2 ADD PARTITION (part2_col = 2)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table2
POSTHOOK: Output: default@part_table2@part2_col=2
PREHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 1)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table3
POSTHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 1)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table3
POSTHOOK: Output: default@part_table3@part3_col=1
PREHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 2)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table3
POSTHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 2)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table3
POSTHOOK: Output: default@part_table3@part3_col=2
PREHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 3)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table3
POSTHOOK: query: ALTER TABLE part_table3 ADD PARTITION (part3_col = 3)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table3
POSTHOOK: Output: default@part_table3@part3_col=3
PREHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 1)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table4
POSTHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 1)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table4
POSTHOOK: Output: default@part_table4@part4_col=1
PREHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 2)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table4
POSTHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 2)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table4
POSTHOOK: Output: default@part_table4@part4_col=2
PREHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 3)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table4
POSTHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 3)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table4
POSTHOOK: Output: default@part_table4@part4_col=3
PREHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 4)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table4
POSTHOOK: query: ALTER TABLE part_table4 ADD PARTITION (part4_col = 4)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table4
POSTHOOK: Output: default@part_table4@part4_col=4
PREHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 1)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table5
POSTHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 1)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table5
POSTHOOK: Output: default@part_table5@part5_col=1
PREHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 2)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table5
POSTHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 2)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table5
POSTHOOK: Output: default@part_table5@part5_col=2
PREHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 3)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table5
POSTHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 3)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table5
POSTHOOK: Output: default@part_table5@part5_col=3
PREHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 4)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table5
POSTHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 4)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table5
POSTHOOK: Output: default@part_table5@part5_col=4
PREHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 5)
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Output: default@part_table5
POSTHOOK: query: ALTER TABLE part_table5 ADD PARTITION (part5_col = 5)
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Output: default@part_table5
POSTHOOK: Output: default@part_table5@part5_col=5
PREHOOK: query: INSERT INTO TABLE part_table1 PARTITION (part1_col = 1) VALUES (1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table1@part1_col=1
POSTHOOK: query: INSERT INTO TABLE part_table1 PARTITION (part1_col = 1) VALUES (1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table1@part1_col=1
POSTHOOK: Lineage: part_table1 PARTITION(part1_col=1).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table2 PARTITION (part2_col = 1) VALUES (1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table2@part2_col=1
POSTHOOK: query: INSERT INTO TABLE part_table2 PARTITION (part2_col = 1) VALUES (1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table2@part2_col=1
POSTHOOK: Lineage: part_table2 PARTITION(part2_col=1).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table2 PARTITION (part2_col = 2) VALUES (2)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table2@part2_col=2
POSTHOOK: query: INSERT INTO TABLE part_table2 PARTITION (part2_col = 2) VALUES (2)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table2@part2_col=2
POSTHOOK: Lineage: part_table2 PARTITION(part2_col=2).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 1) VALUES (1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table3@part3_col=1
POSTHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 1) VALUES (1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table3@part3_col=1
POSTHOOK: Lineage: part_table3 PARTITION(part3_col=1).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 2) VALUES (2)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table3@part3_col=2
POSTHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 2) VALUES (2)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table3@part3_col=2
POSTHOOK: Lineage: part_table3 PARTITION(part3_col=2).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 3) VALUES (3)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table3@part3_col=3
POSTHOOK: query: INSERT INTO TABLE part_table3 PARTITION (part3_col = 3) VALUES (3)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table3@part3_col=3
POSTHOOK: Lineage: part_table3 PARTITION(part3_col=3).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 1) VALUES (1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table4@part4_col=1
POSTHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 1) VALUES (1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table4@part4_col=1
POSTHOOK: Lineage: part_table4 PARTITION(part4_col=1).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 2) VALUES (2)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table4@part4_col=2
POSTHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 2) VALUES (2)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table4@part4_col=2
POSTHOOK: Lineage: part_table4 PARTITION(part4_col=2).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 3) VALUES (3)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table4@part4_col=3
POSTHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 3) VALUES (3)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table4@part4_col=3
POSTHOOK: Lineage: part_table4 PARTITION(part4_col=3).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 4) VALUES (4)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table4@part4_col=4
POSTHOOK: query: INSERT INTO TABLE part_table4 PARTITION (part4_col = 4) VALUES (4)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table4@part4_col=4
POSTHOOK: Lineage: part_table4 PARTITION(part4_col=4).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 1) VALUES (1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table5@part5_col=1
POSTHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 1) VALUES (1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table5@part5_col=1
POSTHOOK: Lineage: part_table5 PARTITION(part5_col=1).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 2) VALUES (2)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table5@part5_col=2
POSTHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 2) VALUES (2)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table5@part5_col=2
POSTHOOK: Lineage: part_table5 PARTITION(part5_col=2).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 3) VALUES (3)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table5@part5_col=3
POSTHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 3) VALUES (3)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table5@part5_col=3
POSTHOOK: Lineage: part_table5 PARTITION(part5_col=3).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 4) VALUES (4)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table5@part5_col=4
POSTHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 4) VALUES (4)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table5@part5_col=4
POSTHOOK: Lineage: part_table5 PARTITION(part5_col=4).col SCRIPT []
PREHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 5) VALUES (5)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@part_table5@part5_col=5
POSTHOOK: query: INSERT INTO TABLE part_table5 PARTITION (part5_col = 5) VALUES (5)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@part_table5@part5_col=5
POSTHOOK: Lineage: part_table5 PARTITION(part5_col=5).col SCRIPT []
PREHOOK: query: INSERT INTO table reg_table VALUES (1), (2), (3), (4), (5), (6)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@reg_table
POSTHOOK: query: INSERT INTO table reg_table VALUES (1), (2), (3), (4), (5), (6)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@reg_table
POSTHOOK: Lineage: reg_table.col SCRIPT []
PREHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-2 depends on stages: Stage-3
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-3
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: pt1
                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part1_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                    Select Operator
                      expressions: _col1 (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: int)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          Target Columns: [Map 2 -> [part2_col:int (part2_col)]]
                          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: pt2
                  Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part2_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col3 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: rt
                  filterExpr: col is not null (type: boolean)
                  Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: col is not null (type: boolean)
                    Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: col (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col3 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col3 (type: int), _col4 (type: int), _col1 (type: int), _col2 (type: int), _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4
                          Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                          File Output Operator
                            compressed: false
                            Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                            table:
                                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	1	1	1	1
PREHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-4 is a root stage
  Stage-3 depends on stages: Stage-4
  Stage-2 depends on stages: Stage-3
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-4
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: pt1
                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part1_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                    Select Operator
                      expressions: _col1 (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: int)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          Target Columns: [Map 2 -> [part3_col:int (part3_col)], Map 3 -> [part2_col:int (part2_col)]]
                          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-3
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: pt2
                  Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part2_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      input vertices:
                        1 Map 4
                      Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col3 (type: int)
                      Select Operator
                        expressions: _col3 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part3_col:int (part3_col)]]
                            Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: pt3
                  Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part3_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col3 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col5 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: rt
                  filterExpr: col is not null (type: boolean)
                  Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: col is not null (type: boolean)
                    Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: col (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col5 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col5 (type: int), _col6 (type: int), _col3 (type: int), _col4 (type: int), _col1 (type: int), _col2 (type: int), _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                          Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                          File Output Operator
                            compressed: false
                            Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                            table:
                                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	1	1	1	1	1	1
PREHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               part_table4 pt4,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
        AND    pt4.part4_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@part_table4
PREHOOK: Input: default@part_table4@part4_col=1
PREHOOK: Input: default@part_table4@part4_col=2
PREHOOK: Input: default@part_table4@part4_col=3
PREHOOK: Input: default@part_table4@part4_col=4
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               part_table4 pt4,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
        AND    pt4.part4_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@part_table4
POSTHOOK: Input: default@part_table4@part4_col=1
POSTHOOK: Input: default@part_table4@part4_col=2
POSTHOOK: Input: default@part_table4@part4_col=3
POSTHOOK: Input: default@part_table4@part4_col=4
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-5 is a root stage
  Stage-4 depends on stages: Stage-5
  Stage-3 depends on stages: Stage-4
  Stage-2 depends on stages: Stage-3
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-5
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: pt1
                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part1_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                    Select Operator
                      expressions: _col1 (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: int)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          Target Columns: [Map 2 -> [part4_col:int (part4_col)], Map 3 -> [part3_col:int (part3_col)], Map 4 -> [part2_col:int (part2_col)]]
                          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-4
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: pt2
                  Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part2_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      input vertices:
                        1 Map 5
                      Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col3 (type: int)
                      Select Operator
                        expressions: _col3 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part4_col:int (part4_col)], Map 3 -> [part3_col:int (part3_col)]]
                            Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-3
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: pt3
                  Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part3_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col3 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      input vertices:
                        1 Map 4
                      Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col5 (type: int)
                      Select Operator
                        expressions: _col5 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part4_col:int (part4_col)]]
                            Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: pt4
                  Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part4_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col5 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col7 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: rt
                  filterExpr: col is not null (type: boolean)
                  Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: col is not null (type: boolean)
                    Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: col (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col7 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col7 (type: int), _col8 (type: int), _col5 (type: int), _col6 (type: int), _col3 (type: int), _col4 (type: int), _col1 (type: int), _col2 (type: int), _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                          Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                          File Output Operator
                            compressed: false
                            Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                            table:
                                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       part_table4 pt4,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
AND    pt4.part4_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@part_table4
PREHOOK: Input: default@part_table4@part4_col=1
PREHOOK: Input: default@part_table4@part4_col=2
PREHOOK: Input: default@part_table4@part4_col=3
PREHOOK: Input: default@part_table4@part4_col=4
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       part_table4 pt4,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
AND    pt4.part4_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@part_table4
POSTHOOK: Input: default@part_table4@part4_col=1
POSTHOOK: Input: default@part_table4@part4_col=2
POSTHOOK: Input: default@part_table4@part4_col=3
POSTHOOK: Input: default@part_table4@part4_col=4
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	1	1	1	1	1	1	1	1
PREHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               part_table4 pt4,
               part_table5 pt5,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
        AND    pt4.part4_col = pt1.part1_col
        AND    pt5.part5_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@part_table4
PREHOOK: Input: default@part_table4@part4_col=1
PREHOOK: Input: default@part_table4@part4_col=2
PREHOOK: Input: default@part_table4@part4_col=3
PREHOOK: Input: default@part_table4@part4_col=4
PREHOOK: Input: default@part_table5
PREHOOK: Input: default@part_table5@part5_col=1
PREHOOK: Input: default@part_table5@part5_col=2
PREHOOK: Input: default@part_table5@part5_col=3
PREHOOK: Input: default@part_table5@part5_col=4
PREHOOK: Input: default@part_table5@part5_col=5
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain SELECT *
        FROM   part_table1 pt1,
               part_table2 pt2,
               part_table3 pt3,
               part_table4 pt4,
               part_table5 pt5,
               reg_table rt
        WHERE  rt.col = pt1.part1_col
        AND    pt2.part2_col = pt1.part1_col
        AND    pt3.part3_col = pt1.part1_col
        AND    pt4.part4_col = pt1.part1_col
        AND    pt5.part5_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@part_table4
POSTHOOK: Input: default@part_table4@part4_col=1
POSTHOOK: Input: default@part_table4@part4_col=2
POSTHOOK: Input: default@part_table4@part4_col=3
POSTHOOK: Input: default@part_table4@part4_col=4
POSTHOOK: Input: default@part_table5
POSTHOOK: Input: default@part_table5@part5_col=1
POSTHOOK: Input: default@part_table5@part5_col=2
POSTHOOK: Input: default@part_table5@part5_col=3
POSTHOOK: Input: default@part_table5@part5_col=4
POSTHOOK: Input: default@part_table5@part5_col=5
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-6 is a root stage
  Stage-5 depends on stages: Stage-6
  Stage-4 depends on stages: Stage-5
  Stage-3 depends on stages: Stage-4
  Stage-2 depends on stages: Stage-3
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-6
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: pt1
                  Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part1_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                    Select Operator
                      expressions: _col1 (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: int)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          Target Columns: [Map 2 -> [part5_col:int (part5_col)], Map 3 -> [part4_col:int (part4_col)], Map 4 -> [part3_col:int (part3_col)], Map 5 -> [part2_col:int (part2_col)]]
                          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-5
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: pt2
                  Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part2_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col1 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      input vertices:
                        1 Map 6
                      Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col3 (type: int)
                      Select Operator
                        expressions: _col3 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part5_col:int (part5_col)], Map 3 -> [part4_col:int (part4_col)], Map 4 -> [part3_col:int (part3_col)]]
                            Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-4
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: pt3
                  Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part3_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col3 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      input vertices:
                        1 Map 5
                      Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col5 (type: int)
                      Select Operator
                        expressions: _col5 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part5_col:int (part5_col)], Map 3 -> [part4_col:int (part4_col)]]
                            Statistics: Num rows: 3 Data size: 3 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-3
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: pt4
                  Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part4_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col5 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                      input vertices:
                        1 Map 4
                      Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col1 (type: int)
                          1 _col7 (type: int)
                      Select Operator
                        expressions: _col7 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: int)
                          minReductionHashAggr: 0.99
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target Columns: [Map 2 -> [part5_col:int (part5_col)]]
                            Statistics: Num rows: 4 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: pt5
                  Statistics: Num rows: 5 Data size: 5 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: col (type: int), part5_col (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 5 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col1 (type: int)
                        1 _col7 (type: int)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 5 Data size: 5 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col9 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: rt
                  filterExpr: col is not null (type: boolean)
                  Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: col is not null (type: boolean)
                    Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: col (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col9 (type: int)
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
                        input vertices:
                          1 Map 2
                        Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col9 (type: int), _col10 (type: int), _col7 (type: int), _col8 (type: int), _col5 (type: int), _col6 (type: int), _col3 (type: int), _col4 (type: int), _col1 (type: int), _col2 (type: int), _col0 (type: int)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
                          Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                          File Output Operator
                            compressed: false
                            Statistics: Num rows: 6 Data size: 6 Basic stats: COMPLETE Column stats: NONE
                            table:
                                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Local Work:
              Map Reduce Local Work

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       part_table4 pt4,
       part_table5 pt5,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
AND    pt4.part4_col = pt1.part1_col
AND    pt5.part5_col = pt1.part1_col
PREHOOK: type: QUERY
PREHOOK: Input: default@part_table1
PREHOOK: Input: default@part_table1@part1_col=1
PREHOOK: Input: default@part_table2
PREHOOK: Input: default@part_table2@part2_col=1
PREHOOK: Input: default@part_table2@part2_col=2
PREHOOK: Input: default@part_table3
PREHOOK: Input: default@part_table3@part3_col=1
PREHOOK: Input: default@part_table3@part3_col=2
PREHOOK: Input: default@part_table3@part3_col=3
PREHOOK: Input: default@part_table4
PREHOOK: Input: default@part_table4@part4_col=1
PREHOOK: Input: default@part_table4@part4_col=2
PREHOOK: Input: default@part_table4@part4_col=3
PREHOOK: Input: default@part_table4@part4_col=4
PREHOOK: Input: default@part_table5
PREHOOK: Input: default@part_table5@part5_col=1
PREHOOK: Input: default@part_table5@part5_col=2
PREHOOK: Input: default@part_table5@part5_col=3
PREHOOK: Input: default@part_table5@part5_col=4
PREHOOK: Input: default@part_table5@part5_col=5
PREHOOK: Input: default@reg_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT *
FROM   part_table1 pt1,
       part_table2 pt2,
       part_table3 pt3,
       part_table4 pt4,
       part_table5 pt5,
       reg_table rt
WHERE  rt.col = pt1.part1_col
AND    pt2.part2_col = pt1.part1_col
AND    pt3.part3_col = pt1.part1_col
AND    pt4.part4_col = pt1.part1_col
AND    pt5.part5_col = pt1.part1_col
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_table1
POSTHOOK: Input: default@part_table1@part1_col=1
POSTHOOK: Input: default@part_table2
POSTHOOK: Input: default@part_table2@part2_col=1
POSTHOOK: Input: default@part_table2@part2_col=2
POSTHOOK: Input: default@part_table3
POSTHOOK: Input: default@part_table3@part3_col=1
POSTHOOK: Input: default@part_table3@part3_col=2
POSTHOOK: Input: default@part_table3@part3_col=3
POSTHOOK: Input: default@part_table4
POSTHOOK: Input: default@part_table4@part4_col=1
POSTHOOK: Input: default@part_table4@part4_col=2
POSTHOOK: Input: default@part_table4@part4_col=3
POSTHOOK: Input: default@part_table4@part4_col=4
POSTHOOK: Input: default@part_table5
POSTHOOK: Input: default@part_table5@part5_col=1
POSTHOOK: Input: default@part_table5@part5_col=2
POSTHOOK: Input: default@part_table5@part5_col=3
POSTHOOK: Input: default@part_table5@part5_col=4
POSTHOOK: Input: default@part_table5@part5_col=5
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	1	1	1	1	1	1	1	1	1	1
PREHOOK: query: DROP TABLE part_table1
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@part_table1
PREHOOK: Output: default@part_table1
POSTHOOK: query: DROP TABLE part_table1
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@part_table1
POSTHOOK: Output: default@part_table1
PREHOOK: query: DROP TABLE part_table2
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@part_table2
PREHOOK: Output: default@part_table2
POSTHOOK: query: DROP TABLE part_table2
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@part_table2
POSTHOOK: Output: default@part_table2
PREHOOK: query: DROP TABLE part_table3
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@part_table3
PREHOOK: Output: default@part_table3
POSTHOOK: query: DROP TABLE part_table3
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@part_table3
POSTHOOK: Output: default@part_table3
PREHOOK: query: DROP TABLE part_table4
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@part_table4
PREHOOK: Output: default@part_table4
POSTHOOK: query: DROP TABLE part_table4
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@part_table4
POSTHOOK: Output: default@part_table4
PREHOOK: query: DROP TABLE part_table5
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@part_table5
PREHOOK: Output: default@part_table5
POSTHOOK: query: DROP TABLE part_table5
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@part_table5
POSTHOOK: Output: default@part_table5
PREHOOK: query: DROP TABLE reg_table
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@reg_table
PREHOOK: Output: default@reg_table
POSTHOOK: query: DROP TABLE reg_table
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@reg_table
POSTHOOK: Output: default@reg_table
