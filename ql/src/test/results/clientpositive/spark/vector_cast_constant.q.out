PREHOOK: query: DROP TABLE over1k_n0
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE over1k_n0
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE over1korc_n0
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE over1korc_n0
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE over1k_n0(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1k_n0
POSTHOOK: query: CREATE TABLE over1k_n0(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1k_n0
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_n0
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@over1k_n0
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k_n0
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@over1k_n0
PREHOOK: query: CREATE TABLE over1korc_n0(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1korc_n0
POSTHOOK: query: CREATE TABLE over1korc_n0(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1korc_n0
PREHOOK: query: INSERT INTO TABLE over1korc_n0 SELECT * FROM over1k_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k_n0
PREHOOK: Output: default@over1korc_n0
POSTHOOK: query: INSERT INTO TABLE over1korc_n0 SELECT * FROM over1k_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k_n0
POSTHOOK: Output: default@over1korc_n0
POSTHOOK: Lineage: over1korc_n0.b SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:b, type:bigint, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.bin SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:bin, type:binary, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.bo SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:bo, type:boolean, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.d SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:d, type:double, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.dec SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]
POSTHOOK: Lineage: over1korc_n0.f SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:f, type:float, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.i SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:i, type:int, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.s SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:s, type:string, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.si SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:si, type:smallint, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.t SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:t, type:tinyint, comment:null), ]
POSTHOOK: Lineage: over1korc_n0.ts SIMPLE [(over1k_n0)over1k_n0.FieldSchema(name:ts, type:timestamp, comment:null), ]
PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc_n0 GROUP BY i ORDER BY i LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc_n0 GROUP BY i ORDER BY i LIMIT 10
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 2)
        Reducer 3 <- Reducer 2 (SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: over1korc_n0
                  Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                  Select Operator
                    expressions: i (type: int)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [2]
                    Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(50), count(), sum(50.0D), count(50.0D), sum(50), count(50)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumLong(ConstantVectorExpression(val 50) -> 12:int) -> bigint, VectorUDAFCountStar(*) -> bigint, VectorUDAFSumDouble(ConstantVectorExpression(val 50.0) -> 13:double) -> double, VectorUDAFCount(ConstantVectorExpression(val 50.0) -> 14:double) -> bigint, VectorUDAFSumDecimal(ConstantVectorExpression(val 50) -> 15:decimal(10,0)) -> decimal(20,0), VectorUDAFCount(ConstantVectorExpression(val 50) -> 16:decimal(10,0)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 2:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2, 3, 4, 5]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                        TopN Hash Memory Usage: 0.1
                        value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: double), _col4 (type: bigint), _col5 (type: decimal(12,0)), _col6 (type: bigint)
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
        Reducer 2 
            Execution mode: vectorized
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), count(VALUE._col1), sum(VALUE._col2), count(VALUE._col3), sum(VALUE._col4), count(VALUE._col5)
                Group By Vectorization:
                    aggregators: VectorUDAFSumLong(col 1:bigint) -> bigint, VectorUDAFCountMerge(col 2:bigint) -> bigint, VectorUDAFSumDouble(col 3:double) -> double, VectorUDAFCountMerge(col 4:bigint) -> bigint, VectorUDAFSumDecimal(col 5:decimal(12,0)) -> decimal(12,0), VectorUDAFCountMerge(col 6:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2, 3, 4, 5]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), (_col1 / _col2) (type: double), (_col3 / _col4) (type: double), CAST( (_col5 / _col6) AS decimal(6,4)) (type: decimal(6,4))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 7, 8, 11]
                      selectExpressions: LongColDivideLongColumn(col 1:bigint, col 2:bigint) -> 7:double, DoubleColDivideLongColumn(col 3:double, col 4:bigint) -> 8:double, CastDecimalToDecimal(col 10:decimal(32,20))(children: DecimalColDivideDecimalColumn(col 5:decimal(12,0), col 9:decimal(19,0))(children: CastLongToDecimal(col 6:bigint) -> 9:decimal(19,0)) -> 10:decimal(32,20)) -> 11:decimal(6,4)
                  Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                    Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
                    value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: decimal(6,4))
        Reducer 3 
            Execution mode: vectorized
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: decimal(6,4))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Limit Vectorization:
                      className: VectorLimitOperator
                      native: true
                  Statistics: Num rows: 10 Data size: 2960 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 10 Data size: 2960 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc_n0 GROUP BY i ORDER BY i LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@over1korc_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc_n0 GROUP BY i ORDER BY i LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1korc_n0
#### A masked pattern was here ####
65536	50.0	50.0	50.0000
65537	50.0	50.0	50.0000
65538	50.0	50.0	50.0000
65539	50.0	50.0	50.0000
65540	50.0	50.0	50.0000
65541	50.0	50.0	50.0000
65542	50.0	50.0	50.0000
65543	50.0	50.0	50.0000
65544	50.0	50.0	50.0000
65545	50.0	50.0	50.0000
