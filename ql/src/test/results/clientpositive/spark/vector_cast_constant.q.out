PREHOOK: query: DROP TABLE over1k
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE over1k
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE over1korc
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE over1korc
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1k
POSTHOOK: query: CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1k
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@over1k
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@over1k
PREHOOK: query: CREATE TABLE over1korc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@over1korc
POSTHOOK: query: CREATE TABLE over1korc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           `dec` decimal(4,2),
           bin binary)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@over1korc
PREHOOK: query: INSERT INTO TABLE over1korc SELECT * FROM over1k
PREHOOK: type: QUERY
PREHOOK: Input: default@over1k
PREHOOK: Output: default@over1korc
POSTHOOK: query: INSERT INTO TABLE over1korc SELECT * FROM over1k
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1k
POSTHOOK: Output: default@over1korc
POSTHOOK: Lineage: over1korc.b SIMPLE [(over1k)over1k.FieldSchema(name:b, type:bigint, comment:null), ]
POSTHOOK: Lineage: over1korc.bin SIMPLE [(over1k)over1k.FieldSchema(name:bin, type:binary, comment:null), ]
POSTHOOK: Lineage: over1korc.bo SIMPLE [(over1k)over1k.FieldSchema(name:bo, type:boolean, comment:null), ]
POSTHOOK: Lineage: over1korc.d SIMPLE [(over1k)over1k.FieldSchema(name:d, type:double, comment:null), ]
POSTHOOK: Lineage: over1korc.dec SIMPLE [(over1k)over1k.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]
POSTHOOK: Lineage: over1korc.f SIMPLE [(over1k)over1k.FieldSchema(name:f, type:float, comment:null), ]
POSTHOOK: Lineage: over1korc.i SIMPLE [(over1k)over1k.FieldSchema(name:i, type:int, comment:null), ]
POSTHOOK: Lineage: over1korc.s SIMPLE [(over1k)over1k.FieldSchema(name:s, type:string, comment:null), ]
POSTHOOK: Lineage: over1korc.si SIMPLE [(over1k)over1k.FieldSchema(name:si, type:smallint, comment:null), ]
POSTHOOK: Lineage: over1korc.t SIMPLE [(over1k)over1k.FieldSchema(name:t, type:tinyint, comment:null), ]
POSTHOOK: Lineage: over1korc.ts SIMPLE [(over1k)over1k.FieldSchema(name:ts, type:timestamp, comment:null), ]
PREHOOK: query: EXPLAIN VECTORIZATION EXPRESSION SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION EXPRESSION SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 2)
        Reducer 3 <- Reducer 2 (SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: over1korc
                  Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                  Select Operator
                    expressions: i (type: int)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [2]
                    Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(50), avg(50.0D), avg(50)
                      Group By Vectorization:
                          aggregators: VectorUDAFAvgLong(ConstantVectorExpression(val 50) -> 12:int) -> struct<count:bigint,sum:double,input:int>, VectorUDAFAvgDouble(ConstantVectorExpression(val 50.0) -> 13:double) -> struct<count:bigint,sum:double,input:double>, VectorUDAFAvgDecimal(ConstantVectorExpression(val 50) -> 14:decimal(10,0)) -> struct<count:bigint,sum:decimal(20,0),input:decimal(10,0)>
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 2:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        Statistics: Num rows: 1049 Data size: 311170 Basic stats: COMPLETE Column stats: NONE
                        TopN Hash Memory Usage: 0.1
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: struct<count:bigint,sum:double,input:double>), _col3 (type: struct<count:bigint,sum:decimal(12,0),input:decimal(10,0)>)
            Execution mode: vectorized
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
        Reducer 2 
            Execution mode: vectorized
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFAvgFinal(col 1:struct<count:bigint,sum:double,input:int>) -> double, VectorUDAFAvgFinal(col 2:struct<count:bigint,sum:double,input:double>) -> double, VectorUDAFAvgDecimalFinal(col 3:struct<count:bigint,sum:decimal(12,0),input:decimal(10,0)>) -> decimal(14,4)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Reduce Sink Vectorization:
                      className: VectorReduceSinkObjectHashOperator
                      native: true
                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                  TopN Hash Memory Usage: 0.1
                  value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: decimal(14,4))
        Reducer 3 
            Execution mode: vectorized
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: decimal(14,4))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 524 Data size: 155436 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Limit Vectorization:
                      className: VectorLimitOperator
                      native: true
                  Statistics: Num rows: 10 Data size: 2960 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 10 Data size: 2960 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@over1korc
#### A masked pattern was here ####
POSTHOOK: query: SELECT 
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@over1korc
#### A masked pattern was here ####
65536	50.0	50.0	50.0000
65537	50.0	50.0	50.0000
65538	50.0	50.0	50.0000
65539	50.0	50.0	50.0000
65540	50.0	50.0	50.0000
65541	50.0	50.0	50.0000
65542	50.0	50.0	50.0000
65543	50.0	50.0	50.0000
65544	50.0	50.0	50.0000
65545	50.0	50.0	50.0000
