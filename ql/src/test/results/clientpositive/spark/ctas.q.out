PREHOOK: query: create table nzhang_Tmp(a int, b string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_Tmp
POSTHOOK: query: create table nzhang_Tmp(a int, b string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_Tmp
PREHOOK: query: select * from nzhang_Tmp
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_tmp
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_Tmp
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_tmp
#### A masked pattern was here ####
PREHOOK: query: explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-0
  Stage-2 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      sort order: ++
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.nzhang_CTAS1

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-3
      Create Table Operator:
        Create Table
          columns: k string, value string
          input format: org.apache.hadoop.mapred.TextInputFormat
          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.nzhang_CTAS1

  Stage: Stage-2
    Stats Work
      Basic Stats Work:

PREHOOK: query: create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_CTAS1
POSTHOOK: query: create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_CTAS1
POSTHOOK: Lineage: nzhang_ctas1.k SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from nzhang_CTAS1
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_ctas1
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_CTAS1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_ctas1
#### A masked pattern was here ####
0	val_0
0	val_0
0	val_0
10	val_10
100	val_100
100	val_100
103	val_103
103	val_103
104	val_104
104	val_104
PREHOOK: query: describe formatted nzhang_CTAS1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@nzhang_ctas1
POSTHOOK: query: describe formatted nzhang_CTAS1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@nzhang_ctas1
# col_name            	data_type           	comment             
k                   	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	10                  
	rawDataSize         	96                  
	totalSize           	106                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: explain create table nzhang_ctas2 as select * from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table nzhang_ctas2 as select * from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-0
  Stage-2 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      sort order: ++
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.nzhang_ctas2

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-3
      Create Table Operator:
        Create Table
          columns: key string, value string
          input format: org.apache.hadoop.mapred.TextInputFormat
          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.nzhang_ctas2

  Stage: Stage-2
    Stats Work
      Basic Stats Work:

PREHOOK: query: create table nzhang_ctas2 as select * from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas2
POSTHOOK: query: create table nzhang_ctas2 as select * from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas2
POSTHOOK: Lineage: nzhang_ctas2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from nzhang_ctas2
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_ctas2
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_ctas2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_ctas2
#### A masked pattern was here ####
0	val_0
0	val_0
0	val_0
10	val_10
100	val_100
100	val_100
103	val_103
103	val_103
104	val_104
104	val_104
PREHOOK: query: describe formatted nzhang_CTAS2
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@nzhang_ctas2
POSTHOOK: query: describe formatted nzhang_CTAS2
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@nzhang_ctas2
# col_name            	data_type           	comment             
key                 	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	10                  
	rawDataSize         	96                  
	totalSize           	106                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-0
  Stage-2 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: (key / 2) (type: double), concat(value, '_con') (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: double), _col1 (type: string)
                      sort order: ++
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: double), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: double), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: double), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
                        name: default.nzhang_ctas3

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-3
      Create Table Operator:
        Create Table
          columns: half_key double, conb string
          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
          serde name: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
          name: default.nzhang_ctas3

  Stage: Stage-2
    Stats Work
      Basic Stats Work:

PREHOOK: query: create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas3
POSTHOOK: query: create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas3
POSTHOOK: Lineage: nzhang_ctas3.conb EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas3.half_key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: select * from nzhang_ctas3
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_ctas3
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_ctas3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_ctas3
#### A masked pattern was here ####
0.0	val_0_con
0.0	val_0_con
0.0	val_0_con
1.0	val_2_con
2.0	val_4_con
2.5	val_5_con
2.5	val_5_con
2.5	val_5_con
4.0	val_8_con
4.5	val_9_con
PREHOOK: query: describe formatted nzhang_CTAS3
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@nzhang_ctas3
POSTHOOK: query: describe formatted nzhang_CTAS3
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@nzhang_ctas3
# col_name            	data_type           	comment             
half_key            	double              	                    
conb                	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	10                  
	rawDataSize         	120                 
	totalSize           	199                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe	 
InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:

STAGE PLANS:
PREHOOK: query: create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2
POSTHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: query: select * from nzhang_ctas3
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_ctas3
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_ctas3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_ctas3
#### A masked pattern was here ####
0.0	val_0_con
0.0	val_0_con
0.0	val_0_con
1.0	val_2_con
2.0	val_4_con
2.5	val_5_con
2.5	val_5_con
2.5	val_5_con
4.0	val_8_con
4.5	val_9_con
PREHOOK: query: describe formatted nzhang_CTAS3
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@nzhang_ctas3
POSTHOOK: query: describe formatted nzhang_CTAS3
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@nzhang_ctas3
# col_name            	data_type           	comment             
half_key            	double              	                    
conb                	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	10                  
	rawDataSize         	120                 
	totalSize           	199                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe	 
InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: explain create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-0
  Stage-2 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      sort order: ++
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.nzhang_ctas4

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-3
      Create Table Operator:
        Create Table
          columns: key string, value string
          field delimiter: ,
          input format: org.apache.hadoop.mapred.TextInputFormat
          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.nzhang_ctas4

  Stage: Stage-2
    Stats Work
      Basic Stats Work:

PREHOOK: query: create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas4
POSTHOOK: query: create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas4
POSTHOOK: Lineage: nzhang_ctas4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from nzhang_ctas4
PREHOOK: type: QUERY
PREHOOK: Input: default@nzhang_ctas4
#### A masked pattern was here ####
POSTHOOK: query: select * from nzhang_ctas4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@nzhang_ctas4
#### A masked pattern was here ####
0	val_0
0	val_0
0	val_0
10	val_10
100	val_100
100	val_100
103	val_103
103	val_103
104	val_104
104	val_104
PREHOOK: query: describe formatted nzhang_CTAS4
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@nzhang_ctas4
POSTHOOK: query: describe formatted nzhang_CTAS4
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@nzhang_ctas4
# col_name            	data_type           	comment             
key                 	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	numFiles            	1                   
	numRows             	10                  
	rawDataSize         	96                  
	totalSize           	106                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	,                   
	serialization.format	,                   
PREHOOK: query: explain create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-0
  Stage-2 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 2)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      sort order: ++
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.nzhang_ctas5

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-3
      Create Table Operator:
        Create Table
          columns: key string, value string
          field delimiter: ,
          input format: org.apache.hadoop.mapred.TextInputFormat
          line delimiter: 

          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.nzhang_ctas5

  Stage: Stage-2
    Stats Work
      Basic Stats Work:

PREHOOK: query: create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas5
POSTHOOK: query: create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas5
POSTHOOK: Lineage: nzhang_ctas5.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas5.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table nzhang_ctas6 (key string, `to` string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas6
POSTHOOK: query: create table nzhang_ctas6 (key string, `to` string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas6
PREHOOK: query: insert overwrite table nzhang_ctas6 select key, value from src tablesample (10 rows)
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@nzhang_ctas6
POSTHOOK: query: insert overwrite table nzhang_ctas6 select key, value from src tablesample (10 rows)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@nzhang_ctas6
POSTHOOK: Lineage: nzhang_ctas6.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: nzhang_ctas6.to SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table nzhang_ctas7 as select key, `to` from nzhang_ctas6
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@nzhang_ctas6
PREHOOK: Output: database:default
PREHOOK: Output: default@nzhang_ctas7
POSTHOOK: query: create table nzhang_ctas7 as select key, `to` from nzhang_ctas6
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@nzhang_ctas6
POSTHOOK: Output: database:default
POSTHOOK: Output: default@nzhang_ctas7
POSTHOOK: Lineage: nzhang_ctas7.key SIMPLE [(nzhang_ctas6)nzhang_ctas6.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: nzhang_ctas7.to SIMPLE [(nzhang_ctas6)nzhang_ctas6.FieldSchema(name:to, type:string, comment:null), ]
