PREHOOK: query: create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
PREHOOK: type: CREATETABLE
POSTHOOK: query: create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@tmp_smb_bucket_10
PREHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '1')
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Input: default@tmp_smb_bucket_10
POSTHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '1')
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Input: default@tmp_smb_bucket_10
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
PREHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '2')
PREHOOK: type: ALTERTABLE_ADDPARTS
PREHOOK: Input: default@tmp_smb_bucket_10
POSTHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '2')
POSTHOOK: type: ALTERTABLE_ADDPARTS
POSTHOOK: Input: default@tmp_smb_bucket_10
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
PREHOOK: query: -- add dummy files to make sure that the number of files in each partition is same as number of buckets
 
load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
PREHOOK: type: LOAD
PREHOOK: Output: default@tmp_smb_bucket_10@ds=1
POSTHOOK: query: -- add dummy files to make sure that the number of files in each partition is same as number of buckets
 
load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
POSTHOOK: type: LOAD
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
PREHOOK: type: LOAD
PREHOOK: Output: default@tmp_smb_bucket_10@ds=1
POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
POSTHOOK: type: LOAD
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
PREHOOK: type: LOAD
PREHOOK: Output: default@tmp_smb_bucket_10@ds=2
POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
POSTHOOK: type: LOAD
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
PREHOOK: type: LOAD
PREHOOK: Output: default@tmp_smb_bucket_10@ds=2
POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
POSTHOOK: type: LOAD
POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
PREHOOK: query: explain
select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
on (a.ds = '1' and b.ds = '2' and
    a.userid = b.userid and
    a.pageid = b.pageid and
    a.postid = b.postid and
    a.type = b.type)
PREHOOK: type: QUERY
POSTHOOK: query: explain
select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
on (a.ds = '1' and b.ds = '2' and
    a.userid = b.userid and
    a.pageid = b.pageid and
    a.postid = b.postid and
    a.type = b.type)
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME tmp_smb_bucket_10) a) (TOK_TABREF (TOK_TABNAME tmp_smb_bucket_10) b) (and (and (and (and (and (= (. (TOK_TABLE_OR_COL a) ds) '1') (= (. (TOK_TABLE_OR_COL b) ds) '2')) (= (. (TOK_TABLE_OR_COL a) userid) (. (TOK_TABLE_OR_COL b) userid))) (= (. (TOK_TABLE_OR_COL a) pageid) (. (TOK_TABLE_OR_COL b) pageid))) (= (. (TOK_TABLE_OR_COL a) postid) (. (TOK_TABLE_OR_COL b) postid))) (= (. (TOK_TABLE_OR_COL a) type) (. (TOK_TABLE_OR_COL b) type))))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST a))) (TOK_SELEXPR TOK_ALLCOLREF))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        b 
          TableScan
            alias: b
            Sorted Merge Bucket Map Join Operator
              condition map:
                   Inner Join 0 to 1
              condition expressions:
                0 {userid} {pageid} {postid} {type} {ds}
                1 {userid} {pageid} {postid} {type} {ds}
              handleSkewJoin: false
              keys:
                0 [Column[userid], Column[pageid], Column[postid], Column[type]]
                1 [Column[userid], Column[pageid], Column[postid], Column[type]]
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col7, _col8, _col9, _col10, _col11
              Position of Big Table: 1
              Select Operator
                expressions:
                      expr: _col0
                      type: int
                      expr: _col1
                      type: int
                      expr: _col2
                      type: int
                      expr: _col3
                      type: string
                      expr: _col4
                      type: string
                      expr: _col7
                      type: int
                      expr: _col8
                      type: int
                      expr: _col9
                      type: int
                      expr: _col10
                      type: string
                      expr: _col11
                      type: string
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1


