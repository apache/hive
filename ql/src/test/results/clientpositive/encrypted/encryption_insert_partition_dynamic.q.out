PREHOOK: query: -- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE
PREHOOK: type: DROPTABLE
POSTHOOK: query: -- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE
POSTHOOK: type: DROPTABLE
PREHOOK: query: drop table IF EXISTS unencryptedTable PURGE
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table IF EXISTS unencryptedTable PURGE
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table encryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc
#### A masked pattern was here ####
PREHOOK: type: CREATETABLE
#### A masked pattern was here ####
PREHOOK: Output: database:default
PREHOOK: Output: default@encryptedTable
POSTHOOK: query: create table encryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc
#### A masked pattern was here ####
POSTHOOK: type: CREATETABLE
#### A masked pattern was here ####
POSTHOOK: Output: database:default
POSTHOOK: Output: default@encryptedTable
Encryption key created: 'key_1'
Encryption zone created: '/build/ql/test/data/warehouse/encryptedTable' using key: 'key_1'
PREHOOK: query: create table unencryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@unencryptedTable
POSTHOOK: query: create table unencryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@unencryptedTable
PREHOOK: query: -- insert encrypted table from values
explain extended insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502')
PREHOOK: type: QUERY
POSTHOOK: query: -- insert encrypted table from values
explain extended insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502')
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      null
         null
            Values__Tmp__Table__1
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               encryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  key
      TOK_SELECT
         TOK_SELEXPR
            TOK_ALLCOLREF


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: values__tmp__table__1
            Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: tmp_values_col1 (type: string), tmp_values_col2 (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                sort order: 
                Map-reduce partition columns: _col0 (type: string)
                Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col0 (type: string), _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
      Path -> Partition:
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
          Partition
            base file name: Values__Tmp__Table__1
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              columns tmp_values_col1,tmp_values_col2
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
              name default.values__tmp__table__1
              serialization.ddl struct values__tmp__table__1 { string tmp_values_col1, string tmp_values_col2}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns tmp_values_col1,tmp_values_col2
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
                name default.values__tmp__table__1
                serialization.ddl struct values__tmp__table__1 { string tmp_values_col1, string tmp_values_col2}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.values__tmp__table__1
            name: default.values__tmp__table__1
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
            NumFilesPerFileSink: 1
            Statistics: Num rows: 1 Data size: 24 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name value
                  columns value
                  columns.comments 
                  columns.types string
#### A masked pattern was here ####
                  name default.encryptedtable
                  partition_columns key
                  partition_columns.types string
                  serialization.ddl struct encryptedtable { string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.encryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            key 
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable

  Stage: Stage-2
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging

PREHOOK: query: insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502')
PREHOOK: type: QUERY
PREHOOK: Input: default@values__tmp__table__2
PREHOOK: Output: default@encryptedtable
POSTHOOK: query: insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@values__tmp__table__2
POSTHOOK: Output: default@encryptedtable@key=501
POSTHOOK: Output: default@encryptedtable@key=502
POSTHOOK: Lineage: encryptedtable PARTITION(key=501).value SIMPLE [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
POSTHOOK: Lineage: encryptedtable PARTITION(key=502).value SIMPLE [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
PREHOOK: query: select * from encryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@key=501
PREHOOK: Input: default@encryptedtable@key=502
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
POSTHOOK: query: select * from encryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@key=501
POSTHOOK: Input: default@encryptedtable@key=502
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
val_501	501
val_502	502
PREHOOK: query: -- insert encrypted table from unencrypted source
explain extended from src
insert into table encryptedTable partition (key)
    select value, key limit 2
PREHOOK: type: QUERY
POSTHOOK: query: -- insert encrypted table from unencrypted source
explain extended from src
insert into table encryptedTable partition (key)
    select value, key limit 2
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               encryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  key
      TOK_SELECT
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               key
      TOK_LIMIT
         2


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: value (type: string), key (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
              Limit
                Number of rows: 2
                Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col0 (type: string), _col1 (type: string)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 'default','default'
              columns.types string:string
#### A masked pattern was here ####
              name default.src
              numFiles 1
              numRows 0
              rawDataSize 0
              serialization.ddl struct src { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 5812
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 'default','default'
                columns.types string:string
#### A masked pattern was here ####
                name default.src
                numFiles 1
                numRows 0
                rawDataSize 0
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 5812
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src
            name: default.src
      Truncated Path -> Alias:
        /src [src]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
          Limit
            Number of rows: 2
            Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string,string
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              sort order: 
              Map-reduce partition columns: _col0 (type: string)
              Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col0 (type: string), _col1 (type: string)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10001
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types string,string
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types string,string
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
            NumFilesPerFileSink: 1
            Statistics: Num rows: 2 Data size: 400 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name value
                  columns value
                  columns.comments 
                  columns.types string
#### A masked pattern was here ####
                  name default.encryptedtable
                  partition_columns key
                  partition_columns.types string
                  serialization.ddl struct encryptedtable { string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.encryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            key 
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable

  Stage: Stage-3
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging

PREHOOK: query: from src
insert into table encryptedTable partition (key)
    select value, key limit 2
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@encryptedtable
POSTHOOK: query: from src
insert into table encryptedTable partition (key)
    select value, key limit 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@encryptedtable@key=238
POSTHOOK: Output: default@encryptedtable@key=86
POSTHOOK: Lineage: encryptedtable PARTITION(key=238).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: encryptedtable PARTITION(key=86).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from encryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@key=238
PREHOOK: Input: default@encryptedtable@key=501
PREHOOK: Input: default@encryptedtable@key=502
PREHOOK: Input: default@encryptedtable@key=86
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
POSTHOOK: query: select * from encryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@key=238
POSTHOOK: Input: default@encryptedtable@key=501
POSTHOOK: Input: default@encryptedtable@key=502
POSTHOOK: Input: default@encryptedtable@key=86
#### A PARTIAL masked pattern was here #### data/warehouse/encryptedTable/.hive-staging
val_238	238
val_501	501
val_502	502
val_86	86
PREHOOK: query: -- insert unencrypted table from encrypted source
explain extended from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key
PREHOOK: type: QUERY
POSTHOOK: query: -- insert unencrypted table from encrypted source
explain extended from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            encryptedTable
   TOK_INSERT
      TOK_INSERT_INTO
         TOK_TAB
            TOK_TABNAME
               unencryptedTable
            TOK_PARTSPEC
               TOK_PARTVAL
                  key
      TOK_SELECT
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               value
         TOK_SELEXPR
            TOK_TABLE_OR_COL
               key


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: encryptedtable
            Statistics: Num rows: 24 Data size: 2447 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: value (type: string), key (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 24 Data size: 2447 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                sort order: 
                Map-reduce partition columns: _col0 (type: string)
                Statistics: Num rows: 24 Data size: 2447 Basic stats: COMPLETE Column stats: NONE
                tag: -1
                value expressions: _col0 (type: string), _col1 (type: string)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: key=238
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              key 238
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name value
              columns value
              columns.comments 
              columns.types string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 1
              numRows 0
              partition_columns key
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 618
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
#### A masked pattern was here ####
          Partition
            base file name: key=501
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              key 501
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name value
              columns value
              columns.comments 
              columns.types string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 1
              numRows 0
              partition_columns key
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 611
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
#### A masked pattern was here ####
          Partition
            base file name: key=502
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              key 502
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name value
              columns value
              columns.comments 
              columns.types string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 1
              numRows 0
              partition_columns key
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 611
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
#### A masked pattern was here ####
          Partition
            base file name: key=86
            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
            partition values:
              key 86
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count 2
              bucket_field_name value
              columns value
              columns.comments 
              columns.types string
#### A masked pattern was here ####
              name default.encryptedtable
              numFiles 1
              numRows 0
              partition_columns key
              partition_columns.types string
              rawDataSize 0
              serialization.ddl struct encryptedtable { string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
              totalSize 607
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.encryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct encryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.encryptedtable
            name: default.encryptedtable
      Truncated Path -> Alias:
        /encryptedTable/key=238 [encryptedtable]
        /encryptedTable/key=501 [encryptedtable]
        /encryptedTable/key=502 [encryptedtable]
        /encryptedTable/key=86 [encryptedtable]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 24 Data size: 2447 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 1
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/.hive-staging
            NumFilesPerFileSink: 1
            Statistics: Num rows: 24 Data size: 2447 Basic stats: COMPLETE Column stats: NONE
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/.hive-staging
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                properties:
                  bucket_count 2
                  bucket_field_name value
                  columns value
                  columns.comments 
                  columns.types string
#### A masked pattern was here ####
                  name default.unencryptedtable
                  partition_columns key
                  partition_columns.types string
                  serialization.ddl struct unencryptedtable { string value}
                  serialization.format 1
                  serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                  transactional true
#### A masked pattern was here ####
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.unencryptedtable
            TotalFiles: 1
            GatherStats: true
            MultiFileSpray: false

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            key 
          replace: false
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/.hive-staging
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              properties:
                bucket_count 2
                bucket_field_name value
                columns value
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                name default.unencryptedtable
                partition_columns key
                partition_columns.types string
                serialization.ddl struct unencryptedtable { string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                transactional true
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.unencryptedtable

  Stage: Stage-2
    Stats-Aggr Operator
#### A PARTIAL masked pattern was here #### data/warehouse/unencryptedtable/.hive-staging

PREHOOK: query: from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key
PREHOOK: type: QUERY
PREHOOK: Input: default@encryptedtable
PREHOOK: Input: default@encryptedtable@key=238
PREHOOK: Input: default@encryptedtable@key=501
PREHOOK: Input: default@encryptedtable@key=502
PREHOOK: Input: default@encryptedtable@key=86
PREHOOK: Output: default@unencryptedtable
POSTHOOK: query: from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Input: default@encryptedtable@key=238
POSTHOOK: Input: default@encryptedtable@key=501
POSTHOOK: Input: default@encryptedtable@key=502
POSTHOOK: Input: default@encryptedtable@key=86
POSTHOOK: Output: default@unencryptedtable@key=238
POSTHOOK: Output: default@unencryptedtable@key=501
POSTHOOK: Output: default@unencryptedtable@key=502
POSTHOOK: Output: default@unencryptedtable@key=86
POSTHOOK: Lineage: unencryptedtable PARTITION(key=238).value SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: unencryptedtable PARTITION(key=501).value SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: unencryptedtable PARTITION(key=502).value SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: unencryptedtable PARTITION(key=86).value SIMPLE [(encryptedtable)encryptedtable.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: select * from unencryptedTable order by key
PREHOOK: type: QUERY
PREHOOK: Input: default@unencryptedtable
PREHOOK: Input: default@unencryptedtable@key=238
PREHOOK: Input: default@unencryptedtable@key=501
PREHOOK: Input: default@unencryptedtable@key=502
PREHOOK: Input: default@unencryptedtable@key=86
#### A masked pattern was here ####
POSTHOOK: query: select * from unencryptedTable order by key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@unencryptedtable
POSTHOOK: Input: default@unencryptedtable@key=238
POSTHOOK: Input: default@unencryptedtable@key=501
POSTHOOK: Input: default@unencryptedtable@key=502
POSTHOOK: Input: default@unencryptedtable@key=86
#### A masked pattern was here ####
val_238	238
val_501	501
val_502	502
val_86	86
PREHOOK: query: -- clean up
drop table encryptedTable PURGE
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@encryptedtable
PREHOOK: Output: default@encryptedtable
POSTHOOK: query: -- clean up
drop table encryptedTable PURGE
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@encryptedtable
POSTHOOK: Output: default@encryptedtable
Encryption key deleted: 'key_1'
PREHOOK: query: drop table unencryptedTable PURGE
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@unencryptedtable
PREHOOK: Output: default@unencryptedtable
POSTHOOK: query: drop table unencryptedTable PURGE
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@unencryptedtable
POSTHOOK: Output: default@unencryptedtable
