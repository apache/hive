PREHOOK: query: DROP TABLE parquet_types_staging
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE parquet_types_staging
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE parquet_types
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE parquet_types
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS parquet_type_nodict
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS parquet_type_nodict
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE parquet_types_staging (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary string,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date,
  cdecimal decimal(4,2)
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@parquet_types_staging
POSTHOOK: query: CREATE TABLE parquet_types_staging (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary string,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date,
  cdecimal decimal(4,2)
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@parquet_types_staging
PREHOOK: query: CREATE TABLE parquet_types (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary binary,
  cdecimal decimal(4,2)
) STORED AS PARQUET
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@parquet_types
POSTHOOK: query: CREATE TABLE parquet_types (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary binary,
  cdecimal decimal(4,2)
) STORED AS PARQUET
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@parquet_types
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_types.txt' OVERWRITE INTO TABLE parquet_types_staging
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@parquet_types_staging
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_types.txt' OVERWRITE INTO TABLE parquet_types_staging
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@parquet_types_staging
PREHOOK: query: INSERT OVERWRITE TABLE parquet_types
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
unhex(cbinary), cdecimal FROM parquet_types_staging
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_types_staging
PREHOOK: Output: default@parquet_types
POSTHOOK: query: INSERT OVERWRITE TABLE parquet_types
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
unhex(cbinary), cdecimal FROM parquet_types_staging
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_types_staging
POSTHOOK: Output: default@parquet_types
POSTHOOK: Lineage: parquet_types.cbinary EXPRESSION [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cbinary, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_types.cchar SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cchar, type:char(5), comment:null), ]
POSTHOOK: Lineage: parquet_types.cdecimal SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cdecimal, type:decimal(4,2), comment:null), ]
POSTHOOK: Lineage: parquet_types.cdouble SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cdouble, type:double, comment:null), ]
POSTHOOK: Lineage: parquet_types.cfloat SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cfloat, type:float, comment:null), ]
POSTHOOK: Lineage: parquet_types.cint SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cint, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_types.csmallint SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:csmallint, type:smallint, comment:null), ]
POSTHOOK: Lineage: parquet_types.cstring1 SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cstring1, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_types.ctinyint SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]
POSTHOOK: Lineage: parquet_types.cvarchar SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:cvarchar, type:varchar(10), comment:null), ]
POSTHOOK: Lineage: parquet_types.t SIMPLE [(parquet_types_staging)parquet_types_staging.FieldSchema(name:t, type:timestamp, comment:null), ]
PREHOOK: query: explain vectorization expression
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization expression
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_types
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
            Select Operator
              expressions: cint (type: int), ctinyint (type: tinyint), csmallint (type: smallint), cfloat (type: float), cdouble (type: double), cstring1 (type: string), t (type: timestamp), cchar (type: char(5)), cvarchar (type: varchar(10)), hex(cbinary) (type: string), cdecimal (type: decimal(4,2))
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumnNums: [0, 1, 2, 3, 4, 5, 6, 7, 8, 12, 10]
                  selectExpressions: VectorUDFAdaptor(hex(cbinary)) -> 12:string
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                File Sink Vectorization:
                    className: VectorFileSinkOperator
                    native: false
                Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: true
          vectorized: true

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_types
#### A masked pattern was here ####
POSTHOOK: query: SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_types
#### A masked pattern was here ####
100	1	1	1.0	0.0	abc	2011-01-01 01:01:01.111111111	a    	a  	B4F3CAFDBEDD	48.88
101	2	2	1.1	0.3	def	2012-02-02 02:02:02.222222222	ab   	ab 	68692CCAC0BDE7	8.72
102	3	3	1.2	0.6	ghi	2013-03-03 03:03:03.333333333	abc  	abc	B4F3CAFDBEDD	90.21
103	1	4	1.3	0.9	jkl	2014-04-04 04:04:04.444444444	abcd 	abcd	68692CCAC0BDE7	3.89
104	2	5	1.4	1.2	mno	2015-05-05 05:05:05.555555555	abcde	abcde	B4F3CAFDBEDD	56.23
105	3	1	1.0	1.5	pqr	2016-06-06 06:06:06.666666666	abcde	abcdef	68692CCAC0BDE7	90.21
106	1	2	1.1	1.8	stu	2017-07-07 07:07:07.777777777	abcde	abcdefg	B4F3CAFDBEDD	6.09
107	2	3	1.2	2.1	vwx	2018-08-08 08:08:08.888888888	bcdef	abcdefgh	68692CCAC0BDE7	9.44
108	3	4	1.3	2.4	yza	2019-09-09 09:09:09.999999999	cdefg	B4F3CAFDBE	68656C6C6F	77.54
109	1	5	1.4	2.7	bcd	2020-10-10 10:10:10.101010101	klmno	abcdedef	68692CCAC0BDE7	25.42
110	2	1	1.0	3.0	efg	2021-11-11 11:11:11.111111111	pqrst	abcdede	B4F3CAFDBEDD	60.12
111	3	2	1.1	3.3	hij	2022-12-12 12:12:12.121212121	nopqr	abcded	68692CCAC0BDE7	49.56
112	1	3	1.2	3.6	klm	2023-01-02 13:13:13.131313131	opqrs	abcdd	B4F3CAFDBEDD	80.76
113	2	4	1.3	3.9	nop	2024-02-02 14:14:14.141414141	pqrst	abc	68692CCAC0BDE7	23.23
114	3	5	1.4	4.2	qrs	2025-03-03 15:15:15.151515151	qrstu	b	B4F3CAFDBEDD	1.01
115	1	1	1.0	4.5	qrs	2026-04-04 16:16:16.161616161	rstuv	abcded	68692CCAC0BDE7	5.98
116	2	2	1.1	4.8	wxy	2027-05-05 17:17:17.171717171	stuvw	abcded	B4F3CAFDBEDD	11.22
117	3	3	1.2	5.1	zab	2028-06-06 18:18:18.181818181	tuvwx	abcded	68692CCAC0BDE7	9.88
118	1	4	1.3	5.4	cde	2029-07-07 19:19:19.191919191	uvwzy	abcdede	B4F3CAFDBEDD	4.76
119	2	5	1.4	5.7	fgh	2030-08-08 20:20:20.202020202	vwxyz	abcdede	68692CCAC0BDE7	12.83
120	3	1	1.0	6.0	ijk	2031-09-09 21:21:21.212121212	wxyza	abcde	B4F3CAFDBEDD	73.04
121	1	2	1.1	6.3	lmn	2032-10-10 22:22:22.222222222	bcdef	abcde		90.33
PREHOOK: query: explain vectorization expression
SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization expression
SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_types
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
            Select Operator
              expressions: cchar (type: char(5)), length(cchar) (type: int), cvarchar (type: varchar(10)), length(cvarchar) (type: int), cdecimal (type: decimal(4,2)), sign(cdecimal) (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumnNums: [7, 12, 8, 13, 10, 14]
                  selectExpressions: StringLength(col 7:char(5)) -> 12:int, StringLength(col 8:varchar(10)) -> 13:int, FuncSignDecimalToLong(col 10:decimal(4,2)) -> 14:int
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                File Sink Vectorization:
                    className: VectorFileSinkOperator
                    native: false
                Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_types
#### A masked pattern was here ####
POSTHOOK: query: SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_types
#### A masked pattern was here ####
a    	1	a  	3	48.88	1
ab   	2	ab 	3	8.72	1
abc  	3	abc	3	90.21	1
abcd 	4	abcd	4	3.89	1
abcde	5	abcde	5	56.23	1
abcde	5	abcdef	6	90.21	1
abcde	5	abcdefg	7	6.09	1
bcdef	5	abcdefgh	8	9.44	1
cdefg	5	B4F3CAFDBE	10	77.54	1
klmno	5	abcdedef	8	25.42	1
pqrst	5	abcdede	7	60.12	1
nopqr	5	abcded	6	49.56	1
opqrs	5	abcdd	5	80.76	1
pqrst	5	abc	3	23.23	1
qrstu	5	b	1	1.01	1
rstuv	5	abcded	6	5.98	1
stuvw	5	abcded	6	11.22	1
tuvwx	5	abcded	6	9.88	1
uvwzy	5	abcdede	7	4.76	1
vwxyz	5	abcdede	7	12.83	1
wxyza	5	abcde	5	73.04	1
bcdef	5	abcde	5	90.33	1
PREHOOK: query: explain vectorization expression
SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization expression
SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_types
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
            Select Operator
              expressions: cint (type: int), ctinyint (type: tinyint), csmallint (type: smallint), cfloat (type: float), cdouble (type: double), cstring1 (type: string), cdecimal (type: decimal(4,2))
              outputColumnNames: cint, ctinyint, csmallint, cfloat, cdouble, cstring1, cdecimal
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumnNums: [0, 1, 2, 3, 4, 5, 10]
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: max(cint), min(csmallint), count(cstring1), avg(cfloat), stddev_pop(cdouble), max(cdecimal)
                Group By Vectorization:
                    aggregators: VectorUDAFMaxLong(col 0:int) -> int, VectorUDAFMinLong(col 2:smallint) -> smallint, VectorUDAFCount(col 5:string) -> bigint, VectorUDAFAvgDouble(col 3:float) -> struct<count:bigint,sum:double,input:float>, VectorUDAFVarDouble(col 4:double) -> struct<count:bigint,sum:double,variance:double> aggregation: stddev_pop, VectorUDAFMaxDecimal(col 10:decimal(4,2)) -> decimal(4,2)
                    className: VectorGroupByOperator
                    groupByMode: HASH
                    keyExpressions: col 1:tinyint
                    native: false
                    vectorProcessingMode: HASH
                    projectedOutputColumnNums: [0, 1, 2, 3, 4, 5]
                keys: ctinyint (type: tinyint)
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: tinyint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: tinyint)
                  Reduce Sink Vectorization:
                      className: VectorReduceSinkOperator
                      native: false
                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                  Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: int), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: struct<count:bigint,sum:double,input:float>), _col5 (type: struct<count:bigint,sum:double,variance:double>), _col6 (type: decimal(4,2))
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), min(VALUE._col1), count(VALUE._col2), avg(VALUE._col3), stddev_pop(VALUE._col4), max(VALUE._col5)
          keys: KEY._col0 (type: tinyint)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
          Statistics: Num rows: 11 Data size: 121 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            TableScan Vectorization:
                native: true
            Reduce Output Operator
              key expressions: _col0 (type: tinyint)
              sort order: +
              Reduce Sink Vectorization:
                  className: VectorReduceSinkOperator
                  native: false
                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
              Statistics: Num rows: 11 Data size: 121 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col1 (type: int), _col2 (type: smallint), _col3 (type: bigint), _col4 (type: double), _col5 (type: double), _col6 (type: decimal(4,2))
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: tinyint), VALUE._col0 (type: int), VALUE._col1 (type: smallint), VALUE._col2 (type: bigint), VALUE._col3 (type: double), VALUE._col4 (type: double), VALUE._col5 (type: decimal(4,2))
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
          Statistics: Num rows: 11 Data size: 121 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 11 Data size: 121 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_types
#### A masked pattern was here ####
POSTHOOK: query: SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_types
#### A masked pattern was here ####
1	121	1	8	1.1749999970197678	2.0621590627301285	90.33
2	119	1	7	1.2142857142857142	1.8	60.12
3	120	1	7	1.171428578240531	1.7999999999999996	90.21
PREHOOK: query: create table parquet_type_nodict like parquet_types
stored as parquet tblproperties ("parquet.enable.dictionary"="false")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@parquet_type_nodict
POSTHOOK: query: create table parquet_type_nodict like parquet_types
stored as parquet tblproperties ("parquet.enable.dictionary"="false")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@parquet_type_nodict
PREHOOK: query: insert into parquet_type_nodict
select * from parquet_types
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_types
PREHOOK: Output: default@parquet_type_nodict
POSTHOOK: query: insert into parquet_type_nodict
select * from parquet_types
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_types
POSTHOOK: Output: default@parquet_type_nodict
POSTHOOK: Lineage: parquet_type_nodict.cbinary SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cbinary, type:binary, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cchar SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cchar, type:char(5), comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cdecimal SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cdecimal, type:decimal(4,2), comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cdouble SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cdouble, type:double, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cfloat SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cfloat, type:float, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cint SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cint, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.csmallint SIMPLE [(parquet_types)parquet_types.FieldSchema(name:csmallint, type:smallint, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cstring1 SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cstring1, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.ctinyint SIMPLE [(parquet_types)parquet_types.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.cvarchar SIMPLE [(parquet_types)parquet_types.FieldSchema(name:cvarchar, type:varchar(10), comment:null), ]
POSTHOOK: Lineage: parquet_type_nodict.t SIMPLE [(parquet_types)parquet_types.FieldSchema(name:t, type:timestamp, comment:null), ]
PREHOOK: query: explain vectorization expression
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_type_nodict
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization expression
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_type_nodict
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_type_nodict
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
            Select Operator
              expressions: cint (type: int), ctinyint (type: tinyint), csmallint (type: smallint), cfloat (type: float), cdouble (type: double), cstring1 (type: string), t (type: timestamp), cchar (type: char(5)), cvarchar (type: varchar(10)), hex(cbinary) (type: string), cdecimal (type: decimal(4,2))
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumnNums: [0, 1, 2, 3, 4, 5, 6, 7, 8, 12, 10]
                  selectExpressions: VectorUDFAdaptor(hex(cbinary)) -> 12:string
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                File Sink Vectorization:
                    className: VectorFileSinkOperator
                    native: false
                Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: true
          vectorized: true

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_type_nodict
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
POSTHOOK: query: SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_type_nodict
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
100	1	1	1.0	0.0	abc	2011-01-01 01:01:01.111111111	a    	a  	B4F3CAFDBEDD	48.88
101	2	2	1.1	0.3	def	2012-02-02 02:02:02.222222222	ab   	ab 	68692CCAC0BDE7	8.72
102	3	3	1.2	0.6	ghi	2013-03-03 03:03:03.333333333	abc  	abc	B4F3CAFDBEDD	90.21
103	1	4	1.3	0.9	jkl	2014-04-04 04:04:04.444444444	abcd 	abcd	68692CCAC0BDE7	3.89
104	2	5	1.4	1.2	mno	2015-05-05 05:05:05.555555555	abcde	abcde	B4F3CAFDBEDD	56.23
105	3	1	1.0	1.5	pqr	2016-06-06 06:06:06.666666666	abcde	abcdef	68692CCAC0BDE7	90.21
106	1	2	1.1	1.8	stu	2017-07-07 07:07:07.777777777	abcde	abcdefg	B4F3CAFDBEDD	6.09
107	2	3	1.2	2.1	vwx	2018-08-08 08:08:08.888888888	bcdef	abcdefgh	68692CCAC0BDE7	9.44
108	3	4	1.3	2.4	yza	2019-09-09 09:09:09.999999999	cdefg	B4F3CAFDBE	68656C6C6F	77.54
109	1	5	1.4	2.7	bcd	2020-10-10 10:10:10.101010101	klmno	abcdedef	68692CCAC0BDE7	25.42
110	2	1	1.0	3.0	efg	2021-11-11 11:11:11.111111111	pqrst	abcdede	B4F3CAFDBEDD	60.12
111	3	2	1.1	3.3	hij	2022-12-12 12:12:12.121212121	nopqr	abcded	68692CCAC0BDE7	49.56
112	1	3	1.2	3.6	klm	2023-01-02 13:13:13.131313131	opqrs	abcdd	B4F3CAFDBEDD	80.76
113	2	4	1.3	3.9	nop	2024-02-02 14:14:14.141414141	pqrst	abc	68692CCAC0BDE7	23.23
114	3	5	1.4	4.2	qrs	2025-03-03 15:15:15.151515151	qrstu	b	B4F3CAFDBEDD	1.01
115	1	1	1.0	4.5	qrs	2026-04-04 16:16:16.161616161	rstuv	abcded	68692CCAC0BDE7	5.98
116	2	2	1.1	4.8	wxy	2027-05-05 17:17:17.171717171	stuvw	abcded	B4F3CAFDBEDD	11.22
117	3	3	1.2	5.1	zab	2028-06-06 18:18:18.181818181	tuvwx	abcded	68692CCAC0BDE7	9.88
118	1	4	1.3	5.4	cde	2029-07-07 19:19:19.191919191	uvwzy	abcdede	B4F3CAFDBEDD	4.76
119	2	5	1.4	5.7	fgh	2030-08-08 20:20:20.202020202	vwxyz	abcdede	68692CCAC0BDE7	12.83
120	3	1	1.0	6.0	ijk	2031-09-09 21:21:21.212121212	wxyza	abcde	B4F3CAFDBEDD	73.04
121	1	2	1.1	6.3	lmn	2032-10-10 22:22:22.222222222	bcdef	abcde		90.33
PREHOOK: query: explain vectorization expression
SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_type_nodict
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization expression
SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_type_nodict
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_type_nodict
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
            Select Operator
              expressions: cchar (type: char(5)), length(cchar) (type: int), cvarchar (type: varchar(10)), length(cvarchar) (type: int), cdecimal (type: decimal(4,2)), sign(cdecimal) (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumnNums: [7, 12, 8, 13, 10, 14]
                  selectExpressions: StringLength(col 7:char(5)) -> 12:int, StringLength(col 8:varchar(10)) -> 13:int, FuncSignDecimalToLong(col 10:decimal(4,2)) -> 14:int
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                File Sink Vectorization:
                    className: VectorFileSinkOperator
                    native: false
                Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_type_nodict
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
POSTHOOK: query: SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_type_nodict
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
a    	1	a  	3	48.88	1
ab   	2	ab 	3	8.72	1
abc  	3	abc	3	90.21	1
abcd 	4	abcd	4	3.89	1
abcde	5	abcde	5	56.23	1
abcde	5	abcdef	6	90.21	1
abcde	5	abcdefg	7	6.09	1
bcdef	5	abcdefgh	8	9.44	1
cdefg	5	B4F3CAFDBE	10	77.54	1
klmno	5	abcdedef	8	25.42	1
pqrst	5	abcdede	7	60.12	1
nopqr	5	abcded	6	49.56	1
opqrs	5	abcdd	5	80.76	1
pqrst	5	abc	3	23.23	1
qrstu	5	b	1	1.01	1
rstuv	5	abcded	6	5.98	1
stuvw	5	abcded	6	11.22	1
tuvwx	5	abcded	6	9.88	1
uvwzy	5	abcdede	7	4.76	1
vwxyz	5	abcdede	7	12.83	1
wxyza	5	abcde	5	73.04	1
bcdef	5	abcde	5	90.33	1
PREHOOK: query: explain vectorization select max(t), min(t) from parquet_type_nodict
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization select max(t), min(t) from parquet_type_nodict
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: parquet_type_nodict
            Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: t (type: timestamp)
              outputColumnNames: t
              Statistics: Num rows: 22 Data size: 242 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: max(t), min(t)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: timestamp), _col1 (type: timestamp)
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), min(VALUE._col1)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 80 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select max(t), min(t) from parquet_type_nodict
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
POSTHOOK: query: select max(t), min(t) from parquet_type_nodict
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_type_nodict
#### A masked pattern was here ####
2032-10-10 22:22:22.222222222	2011-01-01 01:01:01.111111111
PREHOOK: query: create table test (id int, ts timestamp) stored as parquet tblproperties ("parquet.enable.dictionary"="false")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@test
POSTHOOK: query: create table test (id int, ts timestamp) stored as parquet tblproperties ("parquet.enable.dictionary"="false")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@test
PREHOOK: query: insert into test values (1, '2019-01-01 23:12:45.123456'), (2, '2019-01-01 23:12:45.123456'), (3, '2019-01-01 23:12:45.123456')
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@test
POSTHOOK: query: insert into test values (1, '2019-01-01 23:12:45.123456'), (2, '2019-01-01 23:12:45.123456'), (3, '2019-01-01 23:12:45.123456')
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@test
POSTHOOK: Lineage: test.id SCRIPT []
POSTHOOK: Lineage: test.ts SCRIPT []
PREHOOK: query: select ts from test where id > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@test
#### A masked pattern was here ####
POSTHOOK: query: select ts from test where id > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test
#### A masked pattern was here ####
2019-01-01 23:12:45.123456
2019-01-01 23:12:45.123456
PREHOOK: query: insert into test values (3, NULL)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@test
POSTHOOK: query: insert into test values (3, NULL)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@test
POSTHOOK: Lineage: test.id SCRIPT []
POSTHOOK: Lineage: test.ts EXPRESSION []
PREHOOK: query: select ts from test where id > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@test
#### A masked pattern was here ####
POSTHOOK: query: select ts from test where id > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test
#### A masked pattern was here ####
2019-01-01 23:12:45.123456
2019-01-01 23:12:45.123456
NULL
PREHOOK: query: DROP TABLE parquet_type_nodict
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@parquet_type_nodict
PREHOOK: Output: default@parquet_type_nodict
POSTHOOK: query: DROP TABLE parquet_type_nodict
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@parquet_type_nodict
POSTHOOK: Output: default@parquet_type_nodict
PREHOOK: query: DROP TABLE test
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@test
PREHOOK: Output: default@test
POSTHOOK: query: DROP TABLE test
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@test
POSTHOOK: Output: default@test
