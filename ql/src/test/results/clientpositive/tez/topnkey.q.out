PREHOOK: query: EXPLAIN EXTENDED
SELECT key, SUM(CAST(SUBSTR(value,5) AS INT)) FROM src GROUP BY key ORDER BY key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN EXTENDED
SELECT key, SUM(CAST(SUBSTR(value,5) AS INT)) FROM src GROUP BY key ORDER BY key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
OPTIMIZED SQL: SELECT `key` AS `$f0`, SUM(CAST(SUBSTR(`value`, 5) AS INTEGER)) AS `$f1`
FROM `default`.`src`
GROUP BY `key`
ORDER BY `key`
LIMIT 5
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    expressions: key (type: string), UDFToInteger(substr(value, 5)) (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Top N Key Operator
                      sort order: +
                      keys: _col0 (type: string)
                      null sort order: z
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      top n: 5
                      Group By Operator
                        aggregations: sum(_col1)
                        keys: _col0 (type: string)
                        minReductionHashAggr: 0.5
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string)
                          null sort order: z
                          sort order: +
                          Map-reduce partition columns: _col0 (type: string)
                          Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          TopN: 5
                          TopN Hash Memory Usage: 0.1
                          value expressions: _col1 (type: bigint)
                          auto parallelism: true
            Path -> Alias:
              hdfs://### HDFS PATH ### [src]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,value
                    columns.comments 'default','default'
                    columns.types string:string
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src
                    numFiles 1
                    numRows 500
                    rawDataSize 5312
                    serialization.ddl struct src { string key, string value}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 5812
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,value
                      columns.comments 'default','default'
                      columns.types string:string
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src
                      numFiles 1
                      numRows 500
                      rawDataSize 5312
                      serialization.ddl struct src { string key, string value}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 5812
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src
                  name: default.src
            Truncated Path -> Alias:
              /src [src]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: z
                  sort order: +
                  Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: -1
                  TopN: 5
                  TopN Hash Memory Usage: 0.1
                  value expressions: _col1 (type: bigint)
                  auto parallelism: false
        Reducer 3 
            Needs Tagging: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 5
                  Statistics: Num rows: 5 Data size: 475 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://### HDFS PATH ###
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 5 Data size: 475 Basic stats: COMPLETE Column stats: COMPLETE
                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: 5
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, SUM(CAST(SUBSTR(value,5) AS INT)) FROM src GROUP BY key ORDER BY key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT key, SUM(CAST(SUBSTR(value,5) AS INT)) FROM src GROUP BY key ORDER BY key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	0
10	10
100	200
103	206
104	208
PREHOOK: query: EXPLAIN
SELECT key FROM src GROUP BY key ORDER BY key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT key FROM src GROUP BY key ORDER BY key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
Plan optimized by CBO.

Vertex dependency in root stage
Reducer 2 <- Map 1 (SIMPLE_EDGE)
Reducer 3 <- Reducer 2 (SIMPLE_EDGE)

Stage-0
  Fetch Operator
    limit:5
    Stage-1
      Reducer 3
      File Output Operator [FS_9]
        Limit [LIM_8] (rows=5 width=87)
          Number of rows:5
          Select Operator [SEL_7] (rows=250 width=87)
            Output:["_col0"]
          <-Reducer 2 [SIMPLE_EDGE]
            SHUFFLE [RS_6]
              Group By Operator [GBY_4] (rows=250 width=87)
                Output:["_col0"],keys:KEY._col0
              <-Map 1 [SIMPLE_EDGE]
                SHUFFLE [RS_3]
                  PartitionCols:_col0
                  Group By Operator [GBY_2] (rows=250 width=87)
                    Output:["_col0"],keys:key
                    Top N Key Operator [TNK_10] (rows=500 width=87)
                      keys:key,sort order:+,top n:5
                      Select Operator [SEL_1] (rows=500 width=87)
                        Output:["key"]
                        TableScan [TS_0] (rows=500 width=87)
                          default@src,src,Tbl:COMPLETE,Col:COMPLETE,Output:["key"]

PREHOOK: query: SELECT key FROM src GROUP BY key ORDER BY key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT key FROM src GROUP BY key ORDER BY key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
0
10
100
103
104
PREHOOK: query: explain vectorization detail
SELECT src1.key, src2.value FROM src src1 JOIN src src2 ON (src1.key = src2.key) ORDER BY src1.key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain vectorization detail
SELECT src1.key, src2.value FROM src src1 JOIN src src2 ON (src1.key = src2.key) ORDER BY src1.key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
PLAN VECTORIZATION:
  enabled: false
  enabledConditionsNotMet: [hive.vectorized.execution.enabled IS false]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src1
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src2
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col0 (type: string)
                  1 _col0 (type: string)
                outputColumnNames: _col0, _col2
                Statistics: Num rows: 791 Data size: 140798 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), _col2 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 791 Data size: 140798 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string)
                    sort order: +
                    Statistics: Num rows: 791 Data size: 140798 Basic stats: COMPLETE Column stats: COMPLETE
                    TopN Hash Memory Usage: 0.1
                    value expressions: _col1 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 791 Data size: 140798 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 5
                  Statistics: Num rows: 5 Data size: 890 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 5 Data size: 890 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 5
      Processor Tree:
        ListSink

PREHOOK: query: SELECT src1.key, src2.value FROM src src1 JOIN src src2 ON (src1.key = src2.key) ORDER BY src1.key LIMIT 5
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT src1.key, src2.value FROM src src1 JOIN src src2 ON (src1.key = src2.key) ORDER BY src1.key LIMIT 5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0
0	val_0
0	val_0
0	val_0
0	val_0
PREHOOK: query: CREATE TABLE t_test(
  a int,
  b int,
  c int
)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t_test
POSTHOOK: query: CREATE TABLE t_test(
  a int,
  b int,
  c int
)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t_test
PREHOOK: query: INSERT INTO t_test VALUES
(NULL, NULL, NULL),
(5, 2, 3),
(NULL, NULL, NULL),
(NULL, NULL, NULL),
(6, 2, 1),
(7, 8, 4), (7, 8, 4), (7, 8, 4),
(5, 1, 2), (5, 1, 2), (5, 1, 2),
(NULL, NULL, NULL)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@t_test
POSTHOOK: query: INSERT INTO t_test VALUES
(NULL, NULL, NULL),
(5, 2, 3),
(NULL, NULL, NULL),
(NULL, NULL, NULL),
(6, 2, 1),
(7, 8, 4), (7, 8, 4), (7, 8, 4),
(5, 1, 2), (5, 1, 2), (5, 1, 2),
(NULL, NULL, NULL)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@t_test
POSTHOOK: Lineage: t_test.a SCRIPT []
POSTHOOK: Lineage: t_test.b SCRIPT []
POSTHOOK: Lineage: t_test.c SCRIPT []
PREHOOK: query: EXPLAIN
SELECT a, b FROM t_test GROUP BY a, b ORDER BY a, b LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT a, b FROM t_test GROUP BY a, b ORDER BY a, b LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
Plan optimized by CBO.

Vertex dependency in root stage
Reducer 2 <- Map 1 (SIMPLE_EDGE)
Reducer 3 <- Reducer 2 (SIMPLE_EDGE)

Stage-0
  Fetch Operator
    limit:3
    Stage-1
      Reducer 3
      File Output Operator [FS_9]
        Limit [LIM_8] (rows=3 width=5)
          Number of rows:3
          Select Operator [SEL_7] (rows=6 width=4)
            Output:["_col0","_col1"]
          <-Reducer 2 [SIMPLE_EDGE]
            SHUFFLE [RS_6]
              Group By Operator [GBY_4] (rows=6 width=4)
                Output:["_col0","_col1"],keys:KEY._col0, KEY._col1
              <-Map 1 [SIMPLE_EDGE]
                SHUFFLE [RS_3]
                  PartitionCols:_col0, _col1
                  Group By Operator [GBY_2] (rows=6 width=4)
                    Output:["_col0","_col1"],keys:a, b
                    Top N Key Operator [TNK_10] (rows=12 width=6)
                      keys:a, b,sort order:++,top n:3
                      Select Operator [SEL_1] (rows=12 width=6)
                        Output:["a","b"]
                        TableScan [TS_0] (rows=12 width=6)
                          default@t_test,t_test,Tbl:COMPLETE,Col:COMPLETE,Output:["a","b"]

PREHOOK: query: SELECT a, b FROM t_test GROUP BY a, b ORDER BY a, b LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, b FROM t_test GROUP BY a, b ORDER BY a, b LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
5	1
5	2
6	2
PREHOOK: query: EXPLAIN
SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
Plan optimized by CBO.

Vertex dependency in root stage
Reducer 2 <- Map 1 (SIMPLE_EDGE)
Reducer 3 <- Reducer 2 (SIMPLE_EDGE)

Stage-0
  Fetch Operator
    limit:2
    Stage-1
      Reducer 3
      File Output Operator [FS_9]
        Limit [LIM_8] (rows=2 width=10)
          Number of rows:2
          Select Operator [SEL_7] (rows=4 width=10)
            Output:["_col0","_col1"]
          <-Reducer 2 [SIMPLE_EDGE]
            SHUFFLE [RS_6]
              Group By Operator [GBY_4] (rows=4 width=10)
                Output:["_col0","_col1"],aggregations:["count(VALUE._col0)"],keys:KEY._col0
              <-Map 1 [SIMPLE_EDGE]
                SHUFFLE [RS_3]
                  PartitionCols:_col0
                  Group By Operator [GBY_2] (rows=4 width=10)
                    Output:["_col0","_col1"],aggregations:["count(b)"],keys:a
                    Top N Key Operator [TNK_10] (rows=12 width=6)
                      keys:a,sort order:+,top n:2
                      Select Operator [SEL_1] (rows=12 width=6)
                        Output:["a","b"]
                        TableScan [TS_0] (rows=12 width=6)
                          default@t_test,t_test,Tbl:COMPLETE,Col:COMPLETE,Output:["a","b"]

PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
5	4
6	1
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a NULLS FIRST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a NULLS FIRST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	0
5	4
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a NULLS LAST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a NULLS LAST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
5	4
6	1
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
5	4
6	1
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC NULLS FIRST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC NULLS FIRST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	0
5	4
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC NULLS LAST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a ASC NULLS LAST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
5	4
6	1
PREHOOK: query: DROP TABLE IF EXISTS t_test
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@t_test
PREHOOK: Output: default@t_test
POSTHOOK: query: DROP TABLE IF EXISTS t_test
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@t_test
POSTHOOK: Output: default@t_test
PREHOOK: query: CREATE TABLE t_test(
  a int,
  b int,
  c int
)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t_test
POSTHOOK: query: CREATE TABLE t_test(
  a int,
  b int,
  c int
)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t_test
PREHOOK: query: INSERT INTO t_test VALUES
(7, 8, 4), (7, 8, 4), (7, 8, 4),
(NULL, NULL, NULL),
(5, 2, 3),
(NULL, NULL, NULL),
(NULL, NULL, NULL),
(6, 2, 1),
(5, 1, 2), (5, 1, 2), (5, 1, 2),
(NULL, NULL, NULL)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@t_test
POSTHOOK: query: INSERT INTO t_test VALUES
(7, 8, 4), (7, 8, 4), (7, 8, 4),
(NULL, NULL, NULL),
(5, 2, 3),
(NULL, NULL, NULL),
(NULL, NULL, NULL),
(6, 2, 1),
(5, 1, 2), (5, 1, 2), (5, 1, 2),
(NULL, NULL, NULL)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@t_test
POSTHOOK: Lineage: t_test.a SCRIPT []
POSTHOOK: Lineage: t_test.b SCRIPT []
POSTHOOK: Lineage: t_test.c SCRIPT []
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
7	3
6	1
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC NULLS FIRST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC NULLS FIRST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	0
7	3
PREHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC NULLS LAST LIMIT 2
PREHOOK: type: QUERY
PREHOOK: Input: default@t_test
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT a, count(b) FROM t_test GROUP BY a ORDER BY a DESC NULLS LAST LIMIT 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t_test
POSTHOOK: Output: hdfs://### HDFS PATH ###
7	3
6	1
PREHOOK: query: DROP TABLE IF EXISTS t_test
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@t_test
PREHOOK: Output: default@t_test
POSTHOOK: query: DROP TABLE IF EXISTS t_test
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@t_test
POSTHOOK: Output: default@t_test
