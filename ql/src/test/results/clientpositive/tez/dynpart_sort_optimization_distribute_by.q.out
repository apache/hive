PREHOOK: query: create table table1 (col1 string, datekey int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table1
POSTHOOK: query: create table table1 (col1 string, datekey int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table1
PREHOOK: query: insert into table1 values ('ROW1', 1), ('ROW2', 2), ('ROW3', 1)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@table1
POSTHOOK: query: insert into table1 values ('ROW1', 1), ('ROW2', 2), ('ROW3', 1)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@table1
POSTHOOK: Lineage: table1.col1 SCRIPT []
POSTHOOK: Lineage: table1.datekey SCRIPT []
PREHOOK: query: create table table2 (col1 string) partitioned by (datekey int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table2
POSTHOOK: query: create table table2 (col1 string) partitioned by (datekey int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table2
PREHOOK: query: explain extended insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey
PREHOOK: type: QUERY
PREHOOK: Input: default@table1
PREHOOK: Output: default@table2
POSTHOOK: query: explain extended insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table1
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: table1
                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    expressions: col1 (type: string), datekey (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                      tag: -1
                      value expressions: _col0 (type: string)
                      auto parallelism: true
            Path -> Alias:
              hdfs://### HDFS PATH ### [table1]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: table1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"col1":"true","datekey":"true"}}
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns col1,datekey
                    columns.comments 
                    columns.types string:int
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.table1
                    numFiles 1
                    numRows 3
                    rawDataSize 18
                    serialization.ddl struct table1 { string col1, i32 datekey}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 21
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"col1":"true","datekey":"true"}}
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns col1,datekey
                      columns.comments 
                      columns.types string:int
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.table1
                      numFiles 1
                      numRows 3
                      rawDataSize 18
                      serialization.ddl struct table1 { string col1, i32 datekey}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 21
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.table1
                  name: default.table1
            Truncated Path -> Alias:
              /table1 [table1]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: string), KEY._col1 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  GlobalTableId: 1
                  directory: hdfs://### HDFS PATH ###
                  Dp Sort State: PARTITION_SORTED
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      properties:
                        bucket_count -1
                        bucketing_version 2
                        column.name.delimiter ,
                        columns col1
                        columns.comments 
                        columns.types string
#### A masked pattern was here ####
                        location hdfs://### HDFS PATH ###
                        name default.table2
                        partition_columns datekey
                        partition_columns.types int
                        serialization.ddl struct table2 { string col1}
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.table2
                  TotalFiles: 1
                  GatherStats: true
                  MultiFileSpray: false

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            datekey 
          replace: false
          source: hdfs://### HDFS PATH ###
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns col1
                columns.comments 
                columns.types string
#### A masked pattern was here ####
                location hdfs://### HDFS PATH ###
                name default.table2
                partition_columns datekey
                partition_columns.types int
                serialization.ddl struct table2 { string col1}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.table2

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: hdfs://### HDFS PATH ###
      Column Stats Desc:
          Columns: col1
          Column Types: string
          Table: default.table2
          Is Table Level Stats: false

PREHOOK: query: insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey
PREHOOK: type: QUERY
PREHOOK: Input: default@table1
PREHOOK: Output: default@table2
POSTHOOK: query: insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table1
POSTHOOK: Output: default@table2@datekey=1
POSTHOOK: Output: default@table2@datekey=2
POSTHOOK: Lineage: table2 PARTITION(datekey=1).col1 SIMPLE [(table1)table1.FieldSchema(name:col1, type:string, comment:null), ]
POSTHOOK: Lineage: table2 PARTITION(datekey=2).col1 SIMPLE [(table1)table1.FieldSchema(name:col1, type:string, comment:null), ]
PREHOOK: query: select * from table1
PREHOOK: type: QUERY
PREHOOK: Input: default@table1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from table1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table1
POSTHOOK: Output: hdfs://### HDFS PATH ###
ROW1	1
ROW2	2
ROW3	1
PREHOOK: query: select * from table2
PREHOOK: type: QUERY
PREHOOK: Input: default@table2
PREHOOK: Input: default@table2@datekey=1
PREHOOK: Input: default@table2@datekey=2
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from table2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table2
POSTHOOK: Input: default@table2@datekey=1
POSTHOOK: Input: default@table2@datekey=2
POSTHOOK: Output: hdfs://### HDFS PATH ###
ROW1	1
ROW3	1
ROW2	2
