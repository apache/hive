PREHOOK: query: create table emps_imp0 (
  empid int,
  deptno int,
  name varchar(256),
  salary float,
  commission int)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@emps_imp0
POSTHOOK: query: create table emps_imp0 (
  empid int,
  deptno int,
  name varchar(256),
  salary float,
  commission int)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@emps_imp0
PREHOOK: query: create table depts_imp0 (
  deptno int,
  name varchar(256),
  locationid int)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@depts_imp0
POSTHOOK: query: create table depts_imp0 (
  deptno int,
  name varchar(256),
  locationid int)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@depts_imp0
HMS CALL get_table_req FOR emps_imp0
PREHOOK: query: alter table emps_imp0 add constraint pk1 primary key (empid) disable novalidate rely
PREHOOK: type: ALTERTABLE_ADDCONSTRAINT
POSTHOOK: query: alter table emps_imp0 add constraint pk1 primary key (empid) disable novalidate rely
POSTHOOK: type: ALTERTABLE_ADDCONSTRAINT
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: alter table depts_imp0 add constraint pk2 primary key (deptno) disable novalidate rely
PREHOOK: type: ALTERTABLE_ADDCONSTRAINT
POSTHOOK: query: alter table depts_imp0 add constraint pk2 primary key (deptno) disable novalidate rely
POSTHOOK: type: ALTERTABLE_ADDCONSTRAINT
HMS CALL get_table_req FOR emps_imp0
PREHOOK: query: alter table emps_imp0 add constraint fk1 foreign key (deptno) references depts_imp0(deptno) disable novalidate rely
PREHOOK: type: ALTERTABLE_ADDCONSTRAINT
POSTHOOK: query: alter table emps_imp0 add constraint fk1 foreign key (deptno) references depts_imp0(deptno) disable novalidate rely
POSTHOOK: type: ALTERTABLE_ADDCONSTRAINT
HMS CALL get_table_req FOR tab1_imp0
HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR depts_imp0
HMS CALL get_foreign_keys FOR emps_imp0
HMS CALL get_foreign_keys FOR depts_imp0
PREHOOK: query: explain
create table tab1_imp0
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only') as
select empid, depts_imp0.deptno from emps_imp0
join depts_imp0 using (deptno) where depts_imp0.deptno > cast(ltrim('10', 'a') as integer)
group by empid, depts_imp0.deptno
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@depts_imp0
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: database:default
PREHOOK: Output: default@tab1_imp0
POSTHOOK: query: explain
create table tab1_imp0
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only') as
select empid, depts_imp0.deptno from emps_imp0
join depts_imp0 using (deptno) where depts_imp0.deptno > cast(ltrim('10', 'a') as integer)
group by empid, depts_imp0.deptno
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@depts_imp0
POSTHOOK: Input: default@emps_imp0
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tab1_imp0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Impala
      Impala Plan: 
F00:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=1.95MB mem-reservation=1.94MB thread-reservation=1
  WRITE TO HDFS [default.tab1_imp0, OVERWRITE=false]
  |  partitions=1
  |  output exprs: default.emps_imp0.empid, default.depts_imp0.deptno
  |  mem-estimate=12B mem-reservation=0B thread-reservation=0
  |
  02:HASH JOIN [INNER JOIN, BROADCAST]
  |  hash predicates: default.emps_imp0.deptno = default.depts_imp0.deptno
  |  fk/pk conjuncts: assumed fk/pk
  |  mem-estimate=1.94MB mem-reservation=1.94MB spill-buffer=64.00KB thread-reservation=0
  |  tuple-ids=0,2 row-size=12B cardinality=0
  |  in pipelines: 00(GETNEXT), 01(OPEN)
  |
  |--03:EXCHANGE [BROADCAST]
  |     mem-estimate=16.00KB mem-reservation=0B thread-reservation=0
  |     tuple-ids=2 row-size=4B cardinality=0
  |     in pipelines: 01(GETNEXT)
  |
  00:SCAN HDFS [default.emps_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     predicates: default.emps_imp0.deptno > casttoint(ltrim('10', 'a'))
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     parquet statistics predicates: default.emps_imp0.deptno > casttoint(ltrim('10', 'a'))
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=0 row-size=8B cardinality=0
     in pipelines: 00(GETNEXT)

F01:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=0B mem-reservation=0B thread-reservation=1
  DATASTREAM SINK [FRAGMENT=F00, EXCHANGE=03, BROADCAST]
  |  mem-estimate=0B mem-reservation=0B thread-reservation=0
  01:SCAN HDFS [default.depts_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     predicates: default.depts_imp0.deptno > casttoint(ltrim('10', 'a'))
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     parquet statistics predicates: default.depts_imp0.deptno > casttoint(ltrim('10', 'a'))
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=2 row-size=4B cardinality=0
     in pipelines: 01(GETNEXT)


  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
          destination: hdfs://### HDFS PATH ###

  Stage: Stage-2
    Create Table
      columns: empid int, deptno int
      name: default.tab1_imp0
      input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
      output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
      serde name: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
      table properties:
        transactional true
        transactional_properties insert_only

PREHOOK: query: create table tab1_imp0 (
  empid int,
  deptno int
)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@tab1_imp0
POSTHOOK: query: create table tab1_imp0 (
  empid int,
  deptno int
)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tab1_imp0
HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR tab1_imp0
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: explain
insert into tab1_imp0
select empid, deptno
from emps_imp0
PREHOOK: type: QUERY
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: default@tab1_imp0
POSTHOOK: query: explain
insert into tab1_imp0
select empid, deptno
from emps_imp0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@emps_imp0
POSTHOOK: Output: default@tab1_imp0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Impala
      Impala Plan: 
F00:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=8B mem-reservation=0B thread-reservation=1
  WRITE TO HDFS [default.tab1_imp0, OVERWRITE=false]
  |  partitions=1
  |  output exprs: default.emps_imp0.empid, default.emps_imp0.deptno
  |  mem-estimate=8B mem-reservation=0B thread-reservation=0
  |
  00:SCAN HDFS [default.emps_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=0 row-size=8B cardinality=0
     in pipelines: 00(GETNEXT)


  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
              name: default.tab1_imp0
          Write Type: INSERT
          micromanaged table: true

HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR tab1_imp0
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: explain
insert into tab1_imp0
select empid, deptno
from emps_imp0
limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: default@tab1_imp0
POSTHOOK: query: explain
insert into tab1_imp0
select empid, deptno
from emps_imp0
limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@emps_imp0
POSTHOOK: Output: default@tab1_imp0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Impala
      Impala Plan: 
F02:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
Per-Host Resources: mem-estimate=16.01KB mem-reservation=0B thread-reservation=1
  WRITE TO HDFS [default.tab1_imp0, OVERWRITE=false]
  |  partitions=1
  |  output exprs: default.emps_imp0.empid, default.emps_imp0.deptno
  |  mem-estimate=8B mem-reservation=0B thread-reservation=0
  |
  02:EXCHANGE [UNPARTITIONED]
     limit: 10
     mem-estimate=16.00KB mem-reservation=0B thread-reservation=0
     tuple-ids=2 row-size=8B cardinality=0
     in pipelines: 01(GETNEXT)

F01:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=0B mem-reservation=0B thread-reservation=1
  DATASTREAM SINK [FRAGMENT=F02, EXCHANGE=02, UNPARTITIONED]
  |  mem-estimate=0B mem-reservation=0B thread-reservation=0
  00:UNION
  |  pass-through-operands: all
  |  limit: 10
  |  mem-estimate=0B mem-reservation=0B thread-reservation=0
  |  tuple-ids=2 row-size=8B cardinality=0
  |  in pipelines: 01(GETNEXT)
  |
  01:SCAN HDFS [default.emps_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=0 row-size=8B cardinality=0
     in pipelines: 01(GETNEXT)


  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
              name: default.tab1_imp0
          Write Type: INSERT
          micromanaged table: true

PREHOOK: query: create table tab1_imp0_part (
  empid int,
  deptno int
)
PARTITIONED BY (partcol string)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@tab1_imp0_part
POSTHOOK: query: create table tab1_imp0_part (
  empid int,
  deptno int
)
PARTITIONED BY (partcol string)
stored as parquet TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tab1_imp0_part
HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR tab1_imp0_part
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: explain
insert into tab1_imp0_part
select empid, deptno, name
from emps_imp0
PREHOOK: type: QUERY
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: default@tab1_imp0_part
POSTHOOK: query: explain
insert into tab1_imp0_part
select empid, deptno, name
from emps_imp0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@emps_imp0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Impala
      Impala Plan: 
F00:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=12.00MB mem-reservation=12.00MB thread-reservation=1
  WRITE TO HDFS [default.tab1_imp0_part, OVERWRITE=false, PARTITION-KEYS=(name)]
  |  partitions=unavailable
  |  output exprs: empid, deptno, name
  |  mem-estimate=20B mem-reservation=0B thread-reservation=0
  |
  01:SORT
  |  order by: name ASC NULLS LAST
  |  mem-estimate=12.00MB mem-reservation=12.00MB spill-buffer=2.00MB thread-reservation=0
  |  tuple-ids=2 row-size=20B cardinality=0
  |  in pipelines: 01(GETNEXT), 00(OPEN)
  |
  00:SCAN HDFS [default.emps_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=0 row-size=20B cardinality=0
     in pipelines: 00(GETNEXT)


  Stage: Stage-0
    Move Operator
      tables:
          partition:
            partcol 
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
              name: default.tab1_imp0_part
          Write Type: INSERT
          micromanaged table: true

HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR tab1_imp0_part
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: explain
insert into tab1_imp0_part
select empid, deptno, name
from emps_imp0
limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: default@tab1_imp0_part
POSTHOOK: query: explain
insert into tab1_imp0_part
select empid, deptno, name
from emps_imp0
limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@emps_imp0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Impala
      Impala Plan: 
F02:PLAN FRAGMENT [UNPARTITIONED] hosts=1 instances=1
Per-Host Resources: mem-estimate=12.02MB mem-reservation=12.00MB thread-reservation=1
  WRITE TO HDFS [default.tab1_imp0_part, OVERWRITE=false, PARTITION-KEYS=(default.emps_imp0.name)]
  |  partitions=unavailable
  |  output exprs: default.emps_imp0.empid, default.emps_imp0.deptno, default.emps_imp0.name
  |  mem-estimate=20B mem-reservation=0B thread-reservation=0
  |
  03:SORT
  |  order by: default.emps_imp0.name ASC NULLS LAST
  |  mem-estimate=12.00MB mem-reservation=12.00MB spill-buffer=2.00MB thread-reservation=0
  |  tuple-ids=3 row-size=20B cardinality=0
  |  in pipelines: 03(GETNEXT), 01(OPEN)
  |
  02:EXCHANGE [UNPARTITIONED]
     limit: 10
     mem-estimate=24.00KB mem-reservation=0B thread-reservation=0
     tuple-ids=2 row-size=20B cardinality=0
     in pipelines: 01(GETNEXT)

F01:PLAN FRAGMENT [RANDOM] hosts=1 instances=1
Per-Host Resources: mem-estimate=0B mem-reservation=0B thread-reservation=1
  DATASTREAM SINK [FRAGMENT=F02, EXCHANGE=02, UNPARTITIONED]
  |  mem-estimate=0B mem-reservation=0B thread-reservation=0
  00:UNION
  |  pass-through-operands: all
  |  limit: 10
  |  mem-estimate=0B mem-reservation=0B thread-reservation=0
  |  tuple-ids=2 row-size=20B cardinality=0
  |  in pipelines: 01(GETNEXT)
  |
  01:SCAN HDFS [default.emps_imp0, RANDOM]
     HDFS partitions=1/1 files=0 size=0B
     stored statistics:
       table: rows=unavailable size=unavailable
       columns: unavailable
     extrapolated-rows=disabled max-scan-range-rows=0
     file formats: [PARQUET]
     mem-estimate=0B mem-reservation=0B thread-reservation=0
     tuple-ids=0 row-size=20B cardinality=0
     in pipelines: 01(GETNEXT)


  Stage: Stage-0
    Move Operator
      tables:
          partition:
            partcol 
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
              output format: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
              serde: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
              name: default.tab1_imp0_part
          Write Type: INSERT
          micromanaged table: true

HMS CALL get_table_req FOR tab1_imp0
PREHOOK: query: drop table tab1_imp0
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@tab1_imp0
PREHOOK: Output: default@tab1_imp0
HMS CALL get_table_req FOR tab1_imp0
HMS CALL get_table_req FOR tab1_imp0
POSTHOOK: query: drop table tab1_imp0
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@tab1_imp0
POSTHOOK: Output: default@tab1_imp0
HMS CALL get_table_req FOR tab1_imp0_part
PREHOOK: query: drop table tab1_imp0_part
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@tab1_imp0_part
PREHOOK: Output: default@tab1_imp0_part
HMS CALL get_table_req FOR tab1_imp0_part
HMS CALL get_table_req FOR tab1_imp0_part
POSTHOOK: query: drop table tab1_imp0_part
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@tab1_imp0_part
POSTHOOK: Output: default@tab1_imp0_part
HMS CALL get_table_req FOR emps_imp0
PREHOOK: query: drop table emps_imp0
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@emps_imp0
PREHOOK: Output: default@emps_imp0
HMS CALL get_table_req FOR emps_imp0
HMS CALL get_table_req FOR emps_imp0
POSTHOOK: query: drop table emps_imp0
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@emps_imp0
POSTHOOK: Output: default@emps_imp0
HMS CALL get_table_req FOR depts_imp0
PREHOOK: query: drop table depts_imp0
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@depts_imp0
PREHOOK: Output: default@depts_imp0
HMS CALL get_table_req FOR depts_imp0
HMS CALL get_table_req FOR depts_imp0
POSTHOOK: query: drop table depts_imp0
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@depts_imp0
POSTHOOK: Output: default@depts_imp0
