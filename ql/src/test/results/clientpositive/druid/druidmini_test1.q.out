PREHOOK: query: CREATE TABLE druid_table
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@alltypesorc
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table
POSTHOOK: query: CREATE TABLE druid_table
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@alltypesorc
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table
POSTHOOK: Lineage: druid_table.__time EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]
POSTHOOK: Lineage: druid_table.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
POSTHOOK: Lineage: druid_table.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
POSTHOOK: Lineage: druid_table.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]
POSTHOOK: Lineage: druid_table.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]
POSTHOOK: Lineage: druid_table.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]
POSTHOOK: Lineage: druid_table.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]
PREHOOK: query: explain select count(*) FROM druid_table
PREHOOK: type: QUERY
POSTHOOK: query: explain select count(*) FROM druid_table
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table","descending":false,"granularity":"all","aggregations":[{"type":"count","name":"$f0"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT count(*) FROM druid_table
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
6105
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table","descending":false,"granularity":"year","aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cfloat"},{"type":"doubleSum","name":"$f2","fieldName":"cdouble"},{"type":"longSum","name":"$f3","fieldName":"ctinyint"},{"type":"longSum","name":"$f4","fieldName":"csmallint"},{"type":"longSum","name":"$f5","fieldName":"cint"},{"type":"longSum","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: __time (type: timestamp with local time zone), $f1 (type: float), $f2 (type: float), $f3 (type: bigint), $f4 (type: bigint), $f5 (type: bigint), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1968-12-31 16:00:00.0 US/Pacific	-4532.57	3660538.8	-4611	3658030	688783835691	8060200254
1969-12-31 16:00:00.0 US/Pacific	-35057.676	2.3648124E7	-35356	4123059	719285966109	2932345033
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table","descending":false,"granularity":"year","aggregations":[{"type":"doubleMin","name":"$f1","fieldName":"cfloat"},{"type":"doubleMin","name":"$f2","fieldName":"cdouble"},{"type":"longMin","name":"$f3","fieldName":"ctinyint"},{"type":"longMin","name":"$f4","fieldName":"csmallint"},{"type":"longMin","name":"$f5","fieldName":"cint"},{"type":"longMin","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: __time (type: timestamp with local time zone), $f1 (type: float), $f2 (type: float), $f3 (type: bigint), $f4 (type: bigint), $f5 (type: bigint), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1968-12-31 16:00:00.0 US/Pacific	-1790.778	-308691.84	-1790	-313425	0	-8577981133
1969-12-31 16:00:00.0 US/Pacific	-964.719	-287404.84	-1051	-292138	-1073279343	-2147311592
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table","descending":false,"granularity":"year","aggregations":[{"type":"doubleMax","name":"$f1","fieldName":"cfloat"},{"type":"doubleMax","name":"$f2","fieldName":"cdouble"},{"type":"longMax","name":"$f3","fieldName":"ctinyint"},{"type":"longMax","name":"$f4","fieldName":"csmallint"},{"type":"longMax","name":"$f5","fieldName":"cint"},{"type":"longMax","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: __time (type: timestamp with local time zone), $f1 (type: float), $f2 (type: float), $f3 (type: bigint), $f4 (type: bigint), $f5 (type: bigint), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1968-12-31 16:00:00.0 US/Pacific	62.0	57235.0	62	57235	314088763179	2144274348
1969-12-31 16:00:00.0 US/Pacific	769.164	1.9565518E7	723	57435	319104152611	4923772860
PREHOOK: query: EXPLAIN SELECT cstring1, SUM(cdouble) as s FROM druid_table GROUP BY cstring1 ORDER BY s ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT cstring1, SUM(cdouble) as s FROM druid_table GROUP BY cstring1 ORDER BY s ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table","granularity":"all","dimensions":[{"type":"default","dimension":"cstring1"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"$f1","direction":"ascending","dimensionOrder":"numeric"}]},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cstring1 (type: string), $f1 (type: float)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cstring1, SUM(cdouble) as s FROM druid_table GROUP BY cstring1 ORDER BY s ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring1, SUM(cdouble) as s FROM druid_table GROUP BY cstring1 ORDER BY s ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1cGVWH7n1QU	-596096.7
821UdmGbkEf4j	-14161.827
00iT08	0.0
02v8WnLuYDos3Cq	0.0
yv1js	0.0
02VRbSC5I	0.0
014ILGhXxNY7g02hl0Xw	0.0
02vDyIVT752	0.0
00PafC7v	0.0
ytpx1RL8F2I	0.0
PREHOOK: query: EXPLAIN SELECT cstring2, MAX(cdouble) FROM druid_table GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT cstring2, MAX(cdouble) FROM druid_table GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table
          properties:
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"cstring2","direction":"ascending","dimensionOrder":"alphanumeric"}]},"aggregations":[{"type":"doubleMax","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cstring2 (type: string), $f1 (type: float)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cstring2, MAX(cdouble) FROM druid_table GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring2, MAX(cdouble) FROM druid_table GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	1.9565518E7
0AAE3daA78MISbsRsHJrp2PI	0.0
0amu3m60U20Xa3	-200.0
0aO3Lwer	0.0
0aQBRP67JY0gpi	15601.0
0b1WvXy	0.0
0b03cuG3B4ASx4es1411336I	-7196.0
0B5S310g	0.0
0bffMd8KSbW32A8A5	0.0
0bke07kBhD1s33AV3R1X7j7j	0.0
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1900-01-01T00:00:00.000Z/1970-03-01T08:00:00.000Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1968-01-01T08:00:00.000Z/1970-03-01T08:00:00.001Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1968-01-01T08:00:00.000Z/1970-03-01T08:00:00.001Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1968-01-01T08:00:00.000Z/1970-01-01T08:00:00.001Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table
                  properties:
                    druid.query.json {"queryType":"select","dataSource":"default.druid_table","descending":false,"intervals":["1968-01-01T08:00:00.000Z/1970-04-01T08:00:00.001Z"],"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                    druid.query.type select
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: __time (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
