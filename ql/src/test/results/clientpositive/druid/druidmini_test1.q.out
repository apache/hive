PREHOOK: query: CREATE TABLE druid_table_n3
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@alltypesorc
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_n3
POSTHOOK: query: CREATE TABLE druid_table_n3
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@alltypesorc
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_n3
POSTHOOK: Lineage: druid_table_n3.__time EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table_n3.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]
PREHOOK: query: explain select count(*) FROM druid_table_n3
PREHOOK: type: QUERY
POSTHOOK: query: explain select count(*) FROM druid_table_n3
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames $f0
            druid.fieldTypes bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n3","descending":false,"granularity":"all","aggregations":[{"type":"count","name":"$f0"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":false}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT count(*) FROM druid_table_n3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table_n3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
6105
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames timestamp,$f1,$f2,$f3,$f4,$f5,$f6
            druid.fieldTypes timestamp with local time zone,double,double,bigint,bigint,bigint,bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n3","descending":false,"granularity":{"type":"period","period":"P1Y","timeZone":"US/Pacific"},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cfloat"},{"type":"doubleSum","name":"$f2","fieldName":"cdouble"},{"type":"longSum","name":"$f3","fieldName":"ctinyint"},{"type":"longSum","name":"$f4","fieldName":"csmallint"},{"type":"longSum","name":"$f5","fieldName":"cint"},{"type":"longSum","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: timestamp (type: timestamp with local time zone), $f1 (type: double), $f2 (type: double), $f3 (type: bigint), $f4 (type: bigint), $f5 (type: bigint), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), SUM(cfloat), SUM(cdouble), SUM(ctinyint), SUM(csmallint),SUM(cint), SUM(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01 00:00:00.0 US/Pacific	-39590.24724686146	2.7308662809692383E7	-39967	7781089	1408069801800	10992545287
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames timestamp,$f1,$f2,$f3,$f4,$f5,$f6
            druid.fieldTypes timestamp with local time zone,float,double,tinyint,smallint,int,bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n3","descending":false,"granularity":{"type":"period","period":"P1Y","timeZone":"US/Pacific"},"aggregations":[{"type":"doubleMin","name":"$f1","fieldName":"cfloat"},{"type":"doubleMin","name":"$f2","fieldName":"cdouble"},{"type":"longMin","name":"$f3","fieldName":"ctinyint"},{"type":"longMin","name":"$f4","fieldName":"csmallint"},{"type":"longMin","name":"$f5","fieldName":"cint"},{"type":"longMin","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: timestamp (type: timestamp with local time zone), $f1 (type: float), $f2 (type: double), $f3 (type: tinyint), $f4 (type: smallint), $f5 (type: int), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), MIN(cfloat), MIN(cdouble), MIN(ctinyint), MIN(csmallint),MIN(cint), MIN(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01 00:00:00.0 US/Pacific	-1790.7781	-308691.84375	2	14255	-1073279343	-8577981133
PREHOOK: query: EXPLAIN SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames timestamp,$f1,$f2,$f3,$f4,$f5,$f6
            druid.fieldTypes timestamp with local time zone,float,double,tinyint,smallint,int,bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n3","descending":false,"granularity":{"type":"period","period":"P1Y","timeZone":"US/Pacific"},"aggregations":[{"type":"doubleMax","name":"$f1","fieldName":"cfloat"},{"type":"doubleMax","name":"$f2","fieldName":"cdouble"},{"type":"longMax","name":"$f3","fieldName":"ctinyint"},{"type":"longMax","name":"$f4","fieldName":"csmallint"},{"type":"longMax","name":"$f5","fieldName":"cint"},{"type":"longMax","name":"$f6","fieldName":"cbigint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: timestamp (type: timestamp with local time zone), $f1 (type: float), $f2 (type: double), $f3 (type: tinyint), $f4 (type: smallint), $f5 (type: int), $f6 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            ListSink

PREHOOK: query: SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT floor_year(`__time`), MAX(cfloat), MAX(cdouble), MAX(ctinyint), MAX(csmallint),MAX(cint), MAX(cbigint)
FROM druid_table_n3 GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01 00:00:00.0 US/Pacific	769.16394	1.9565518E7	-45	-8101	1276572707	4923772860
PREHOOK: query: EXPLAIN SELECT cstring1, SUM(cdouble) as s FROM druid_table_n3 GROUP BY cstring1 ORDER BY s ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT cstring1, SUM(cdouble) as s FROM druid_table_n3 GROUP BY cstring1 ORDER BY s ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames cstring1,$f1
            druid.fieldTypes string,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n3","granularity":"all","dimensions":[{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"$f1","direction":"ascending","dimensionOrder":"numeric"}]},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cstring1 (type: string), $f1 (type: double)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cstring1, SUM(cdouble) as s FROM druid_table_n3 GROUP BY cstring1 ORDER BY s ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring1, SUM(cdouble) as s FROM druid_table_n3 GROUP BY cstring1 ORDER BY s ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1cGVWH7n1QU	-596096.6875
821UdmGbkEf4j	-14161.827026367188
00iT08	0.0
02v8WnLuYDos3Cq	0.0
yv1js	0.0
02VRbSC5I	0.0
014ILGhXxNY7g02hl0Xw	0.0
02vDyIVT752	0.0
00PafC7v	0.0
ytpx1RL8F2I	0.0
PREHOOK: query: EXPLAIN SELECT cstring2, MAX(cdouble) FROM druid_table_n3 GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT cstring2, MAX(cdouble) FROM druid_table_n3 GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames cstring2,$f1
            druid.fieldTypes string,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n3","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"cstring2","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[{"type":"doubleMax","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cstring2 (type: string), $f1 (type: double)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cstring2, MAX(cdouble) FROM druid_table_n3 GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring2, MAX(cdouble) FROM druid_table_n3 GROUP BY cstring2 ORDER BY cstring2 ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	1.9565518E7
0034fkcXMQI3	15601.0
004J8y	0.0
00GNm	-200.0
00GW4dnb6Wgj52	-200.0
00PBhB1Iefgk	0.0
00d5kr1wEB7evExG	15601.0
00qccwt8n	0.0
017fFeQ3Gcsa83Xj2Vo0	0.0
01EfkvNk6mjG44uxs	0.0
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3 ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3 ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3 ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3 ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1900-01-01T00:00:00.000Z/1970-03-01T08:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` < '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1968-01-01T08:00:00.000Z/1970-03-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1968-01-01T08:00:00.000Z/1970-03-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` >= '1968-01-01 00:00:00' AND `__time` <= '1970-03-01 00:00:00'
    AND `__time` < '2011-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1968-01-01T08:00:00.000Z/1970-01-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE `__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00' ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`
FROM druid_table_n3
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n3
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1968-01-01T08:00:00.000Z/1970-04-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n3
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: explain select (cstring1 is null ) AS is_null, (cint is not null ) as isnotnull FROM druid_table_n3
PREHOOK: type: QUERY
POSTHOOK: query: explain select (cstring1 is null ) AS is_null, (cint is not null ) as isnotnull FROM druid_table_n3
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n3
          properties:
            druid.fieldNames vc,vc0
            druid.fieldTypes boolean,boolean
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n3","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"(\"cstring1\" isnull)","outputType":"FLOAT"},{"type":"expression","name":"vc0","expression":"(\"cint\" notnull)","outputType":"FLOAT"}],"columns":["vc","vc0"],"resultFormat":"compactedList"}
            druid.query.type scan
          Select Operator
            expressions: vc (type: boolean), vc0 (type: boolean)
            outputColumnNames: _col0, _col1
            ListSink

