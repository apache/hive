PREHOOK: query: CREATE EXTERNAL TABLE druid_table_1_n2
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_1_n2
POSTHOOK: query: CREATE EXTERNAL TABLE druid_table_1_n2
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_1_n2
PREHOOK: query: DESCRIBE FORMATTED druid_table_1_n2
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_table_1_n2
POSTHOOK: query: DESCRIBE FORMATTED druid_table_1_n2
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_table_1_n2
# col_name            	data_type           	comment             
__time              	timestamp with local time zone	from deserializer   
robot               	string              	from deserializer   
namespace           	string              	from deserializer   
anonymous           	string              	from deserializer   
unpatrolled         	string              	from deserializer   
page                	string              	from deserializer   
language            	string              	from deserializer   
newpage             	string              	from deserializer   
user                	string              	from deserializer   
count               	float               	from deserializer   
added               	float               	from deserializer   
delta               	float               	from deserializer   
variation           	float               	from deserializer   
deleted             	float               	from deserializer   
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"__time\":\"true\",\"added\":\"true\",\"anonymous\":\"true\",\"count\":\"true\",\"deleted\":\"true\",\"delta\":\"true\",\"language\":\"true\",\"namespace\":\"true\",\"newpage\":\"true\",\"page\":\"true\",\"robot\":\"true\",\"unpatrolled\":\"true\",\"user\":\"true\",\"variation\":\"true\"}}
	EXTERNAL            	TRUE                
	bucketing_version   	2                   
	druid.datasource    	wikipedia           
	numFiles            	0                   
	numRows             	0                   
	rawDataSize         	0                   
	storage_handler     	org.apache.hadoop.hive.druid.QTestDruidStorageHandler
	totalSize           	0                   
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.druid.QTestDruidSerDe	 
InputFormat:        	null                	 
OutputFormat:       	null                	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1_n2
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1_n2
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["robot"],"resultFormat":"compactedList"}
            druid.query.type scan
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1_n2
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1_n2
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames delta
            druid.fieldTypes float
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["delta"],"resultFormat":"compactedList"}
            druid.query.type scan
          GatherStats: false
          Select Operator
            expressions: delta (type: float)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1_n2
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1_n2
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
            druid.query.type scan
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1_n2
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1_n2
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"}],"limitSpec":{"type":"default"},"filter":{"type":"selector","dimension":"language","value":"en"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1_n2) a
  JOIN
  (SELECT language
  FROM druid_table_1_n2) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1_n2) a
  JOIN
  (SELECT language
  FROM druid_table_1_n2) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_1_n2
                  properties:
                    druid.fieldNames robot,language
                    druid.fieldTypes string,string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["robot","language"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 1 Data size: 368 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Select Operator
                    expressions: robot (type: string), language (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 368 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: string)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col1 (type: string)
                      Statistics: Num rows: 1 Data size: 368 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      value expressions: _col0 (type: string)
                      auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_1_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_1_n2
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                    columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                    columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                    druid.datasource wikipedia
                    druid.fieldNames robot,language
                    druid.fieldTypes string,string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["robot","language"],"resultFormat":"compactedList"}
                    druid.query.type scan
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_1_n2
                    numFiles 0
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                      columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                      columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                      druid.datasource wikipedia
                      druid.fieldNames robot,language
                      druid.fieldTypes string,string
                      druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["robot","language"],"resultFormat":"compactedList"}
                      druid.query.type scan
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_1_n2
                      numFiles 0
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                    name: default.druid_table_1_n2
                  name: default.druid_table_1_n2
            Truncated Path -> Alias:
              /druid_table_1_n2 [druid_table_1_n2]
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: druid_table_1_n2
                  properties:
                    druid.fieldNames language
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["language"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: language (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: language (type: string)
                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                    tag: 1
                    auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_1_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_1_n2
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                    columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                    columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                    druid.datasource wikipedia
                    druid.fieldNames language
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["language"],"resultFormat":"compactedList"}
                    druid.query.type scan
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_1_n2
                    numFiles 0
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                      columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                      columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                      druid.datasource wikipedia
                      druid.fieldNames language
                      druid.fieldTypes string
                      druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["language"],"resultFormat":"compactedList"}
                      druid.query.type scan
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_1_n2
                      numFiles 0
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                    name: default.druid_table_1_n2
                  name: default.druid_table_1_n2
            Truncated Path -> Alias:
              /druid_table_1_n2 [druid_table_1_n2]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 language (type: string)
                outputColumnNames: _col0, _col2
                Position of Big Table: 0
                Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), _col2 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://### HDFS PATH ###
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 404 Basic stats: COMPLETE Column stats: NONE
                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:string
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Warning: Shuffle Join MERGEJOIN[8][tables = [$hdt$_0, druid_table_1_n2]] in Stage 'Reducer 2' is a cross product
PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1_n2
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1_n2) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1_n2
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1_n2) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (XPROD_EDGE), Map 3 (XPROD_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_1_n2
                  properties:
                    druid.fieldNames robot
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Select Operator
                    expressions: robot (type: string)
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      value expressions: _col0 (type: string)
                      auto parallelism: false
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_1_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_1_n2
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                    columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                    columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                    druid.datasource wikipedia
                    druid.fieldNames robot
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
                    druid.query.type scan
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_1_n2
                    numFiles 0
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                      columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                      columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                      druid.datasource wikipedia
                      druid.fieldNames robot
                      druid.fieldTypes string
                      druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
                      druid.query.type scan
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_1_n2
                      numFiles 0
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                    name: default.druid_table_1_n2
                  name: default.druid_table_1_n2
            Truncated Path -> Alias:
              /druid_table_1_n2 [druid_table_1_n2]
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: druid_table_1_n2
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"virtualColumns":[{"type":"expression","name":"vc","expression":"'en'","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                  GatherStats: false
                  Reduce Output Operator
                    null sort order: 
                    sort order: 
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                    tag: 1
                    auto parallelism: false
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_1_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_1_n2
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                    columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                    columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                    druid.datasource wikipedia
                    druid.fieldNames vc
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"virtualColumns":[{"type":"expression","name":"vc","expression":"'en'","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_1_n2
                    numFiles 0
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                      columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                      columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                      druid.datasource wikipedia
                      druid.fieldNames vc
                      druid.fieldTypes string
                      druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"virtualColumns":[{"type":"expression","name":"vc","expression":"'en'","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
                      druid.query.type scan
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_1_n2
                      numFiles 0
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                    name: default.druid_table_1_n2
                  name: default.druid_table_1_n2
            Truncated Path -> Alias:
              /druid_table_1_n2 [druid_table_1_n2]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 
                  1 
                outputColumnNames: _col0
                Position of Big Table: 0
                Statistics: Num rows: 1 Data size: 185 Basic stats: PARTIAL Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), 'en' (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 185 Basic stats: PARTIAL Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://### HDFS PATH ###
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 185 Basic stats: PARTIAL Column stats: NONE
                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:string
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1_n2
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1_n2
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot,floor_day,$f3,$f4,(tok_function tok_int (tok_table_or_col robot))
            druid.fieldTypes string,timestamp with local time zone,float,double,int
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"default","dimension":"language","outputName":"language","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"UTC","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"(tok_function tok_int (tok_table_or_col robot))","direction":"ascending","dimensionOrder":"numeric"},{"dimension":"$f3","direction":"descending","dimensionOrder":"numeric"}]},"aggregations":[{"type":"doubleMax","name":"$f3","fieldName":"added"},{"type":"doubleSum","name":"$f4","fieldName":"delta"}],"postAggregations":[{"type":"expression","name":"(tok_function tok_int (tok_table_or_col robot))","expression":"CAST(\"robot\", 'LONG')"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          GatherStats: false
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone), $f3 (type: float), $f4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3
            ListSink

PREHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1_n2
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1_n2
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"substring(\"namespace\", (CAST(\"deleted\", 'LONG') - 1), 4)","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
            druid.query.type scan
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1_n2
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1_n2
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot,floor_day
            druid.fieldTypes string,timestamp with local time zone
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"UTC","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1_n2
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1_n2
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames extract,robot
            druid.fieldTypes timestamp with local time zone,string
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: robot (type: string), extract (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1_n2
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1_n2
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1_n2
          properties:
            druid.fieldNames robot,floor_day
            druid.fieldTypes string,timestamp with local time zone
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"UTC","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1_n2
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1_n2
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_1_n2
                  Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Select Operator
                    expressions: __time (type: timestamp with local time zone), robot (type: string), language (type: string), added (type: float), delta (type: float)
                    outputColumnNames: __time, robot, language, added, delta
                    Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: max(added), sum(delta)
                      keys: robot (type: string), language (type: string), floor_day(__time) (type: timestamp with local time zone)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4
                      Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp with local time zone)
                        null sort order: aaa
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp with local time zone)
                        Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                        tag: -1
                        value expressions: _col3 (type: float), _col4 (type: double)
                        auto parallelism: true
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_1_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_1_n2
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                    columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                    columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                    druid.datasource wikipedia
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_1_n2
                    numFiles 0
                    numRows 0
                    rawDataSize 0
                    serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                      columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                      columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                      druid.datasource wikipedia
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_1_n2
                      numFiles 0
                      numRows 0
                      rawDataSize 0
                      serialization.ddl struct druid_table_1_n2 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
                    name: default.druid_table_1_n2
                  name: default.druid_table_1_n2
            Truncated Path -> Alias:
              /druid_table_1_n2 [druid_table_1_n2]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: timestamp with local time zone)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4
                Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), _col2 (type: timestamp with local time zone), _col3 (type: float), _col4 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: UDFToInteger(_col0) (type: int), _col2 (type: float)
                    null sort order: az
                    sort order: +-
                    Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    TopN: 10
                    TopN Hash Memory Usage: 0.1
                    value expressions: _col0 (type: string), _col1 (type: timestamp with local time zone), _col3 (type: double)
                    auto parallelism: false
        Reducer 3 
            Needs Tagging: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: string), VALUE._col1 (type: timestamp with local time zone), KEY.reducesinkkey1 (type: float), VALUE._col2 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://### HDFS PATH ###
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 416 Basic stats: COMPLETE Column stats: NONE
                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1,_col2,_col3
                          columns.types string:timestamp with local time zone:float:double
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

