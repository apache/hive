PREHOOK: query: -- MATH AND STRING functions

SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- MATH AND STRING functions

SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
2081
PREHOOK: query: SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
7602
PREHOOK: query: SELECT count(*) FROM druid_table_alltypesorc WHERE power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table_alltypesorc WHERE power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
2081
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
35881.09400522709	23378637	28946	7.57221208318E11	757217375155	68151649880
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
78620.21200561523	-9973483	74628	3.54463845353E11	354473882701	92859255924
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE  ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE  ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
-2195.4589961767197	27912403	-6517	5.33299976914E11	533291579903	13774723379
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE  SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_alltypesorc WHERE  SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
0.0	0	1	0.0	0	0
PREHOOK: query: SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_alltypesorc WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
 GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_alltypesorc WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
 GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
8Nj7qpHBTH1GUkMM1BXr2_YyROa06YMuK3C2eg85d	yRO	YYROA06YMUK3C2EG85D	8nj7qphbth1gukmm1bxr2	0.0
k7rg3Vw6IpwU6_YyKI8Rb72WP5dP1BMSPoT	yKI	YYKI8RB72WP5DP1BMSPOT	k7rg3vw6ipwu6	0.0
b0r8g21X6I2TvvPj623IKR_YxSwHWr	xSw	YXSWHWR	b0r8g21x6i2tvvpj623ikr	0.0
ox4gTH52_YsjDHuPsD2	sjD	YSJDHUPSD2	ox4gth52	0.0
NEGa0N8MJ2dnn3MKAfl6u_Yr4e3n	r4e	YR4E3N	nega0n8mj2dnn3mkafl6u	0.0
767fOfF1Oj8fyOv6YFI16rM_YqdbA5	qdb	YQDBA5	767foff1oj8fyov6yfi16rm	0.0
kM4k0y1fqwton_YpK3CTDWEXOV	pK3	YPK3CTDWEXOV	km4k0y1fqwton	0.0
TBI20Ba2YuO44754E2BM_YpB20i4	pB2	YPB20I4	tbi20ba2yuo44754e2bm	0.0
jiqEpNs7qXo0y37_Ynnw5opXqf6BU	nnw	YNNW5OPXQF6BU	jiqepns7qxo0y37	0.0
TgS6dAlI2w4y_Ynh42DscA373RX27nBkft	nh4	YNH42DSCA373RX27NBKFT	tgs6dali2w4y	0.0
PREHOOK: query: EXPLAIN SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT count(*) FROM druid_table_alltypesorc WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames $f0
            druid.fieldTypes bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_alltypesorc","descending":false,"granularity":"all","filter":{"type":"and","fields":[{"type":"expression","expression":"(sqrt(abs(\"ctinyint\")) > 3)"},{"type":"expression","expression":"((pow(\"cfloat\",2) * pow(\"csmallint\",3)) > 1)"},{"type":"expression","expression":"(strlen(CAST(\"ctinyint\", 'STRING')) > 1)"},{"type":"expression","expression":"(strlen(CAST(\"ctinyint\", 'STRING')) < 10)"}]},"aggregations":[{"type":"count","name":"$f0"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":false}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN SELECT SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
        FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
        FROM druid_table_alltypesorc WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames $f0,_o__c1,_o__c2,_o__c3,$f4,$f5
            druid.fieldTypes double,int,bigint,double,bigint,bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_alltypesorc","descending":false,"granularity":"all","filter":{"type":"and","fields":[{"type":"or","fields":[{"type":"expression","expression":"((floor(\"cdouble\") * 2) < 1000)"},{"type":"expression","expression":"((log(\"cdouble\") / 1.0) > 0)"},{"type":"expression","expression":"(sin(\"cdouble\") > 1)"}]},{"type":"or","fields":[{"type":"and","fields":[{"type":"expression","expression":"(ceil(\"cfloat\") > 0)"},{"type":"expression","expression":"((floor(\"cdouble\") * 2) < 1000)"}]},{"type":"and","fields":[{"type":"expression","expression":"((log(\"cdouble\") / 1.0) > 0)"},{"type":"expression","expression":"(cos(\"cint\") > 0)"}]},{"type":"expression","expression":"(sin(\"cdouble\") > 1)"}]}]},"aggregations":[{"type":"doubleSum","name":"$f0","expression":"(\"cfloat\" + 1)"},{"type":"doubleSum","name":"$f1","expression":"(\"cdouble\" + CAST(\"ctinyint\", 'DOUBLE'))"},{"type":"longSum","name":"$f2","fieldName":"ctinyint"},{"type":"longSum","name":"$f3","fieldName":"csmallint"},{"type":"longSum","name":"$f4","fieldName":"cint"},{"type":"longSum","name":"$f5","fieldName":"cbigint"}],"postAggregations":[{"type":"expression","name":"_o__c1","expression":"CAST(\"$f1\", 'LONG')"},{"type":"expression","name":"_o__c2","expression":"(\"$f2\" + 1)"},{"type":"expression","name":"_o__c3","expression":"CAST((\"$f3\" + \"$f4\"), 'DOUBLE')"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":false}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: double), _o__c1 (type: int), _o__c2 (type: bigint), _o__c3 (type: double), $f4 (type: bigint), $f5 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            ListSink

PREHOOK: query: EXPLAIN SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_alltypesorc WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
         GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_alltypesorc WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
         GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc,vc0,vc1,vc2,$f4
            druid.fieldTypes string,string,string,string,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"},{"type":"default","dimension":"vc0","outputName":"vc0","outputType":"STRING"},{"type":"default","dimension":"vc1","outputName":"vc1","outputType":"STRING"},{"type":"default","dimension":"vc2","outputName":"vc2","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring1\",'_'),\"cstring2\")","outputType":"STRING"},{"type":"expression","name":"vc0","expression":"substring(\"cstring2\", 1, 3)","outputType":"STRING"},{"type":"expression","name":"vc1","expression":"upper(\"cstring2\")","outputType":"STRING"},{"type":"expression","name":"vc2","expression":"lower(\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"vc0","direction":"descending","dimensionOrder":"lexicographic"}]},"filter":{"type":"and","fields":[{"type":"expression","expression":"like(\"cstring2\",'Y%')"},{"type":"not","field":{"type":"selector","dimension":"cstring1","value":null}},{"type":"not","field":{"type":"selector","dimension":"cstring2","value":null}}]},"aggregations":[{"type":"doubleSum","name":"$f4","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string), vc0 (type: string), vc1 (type: string), vc2 (type: string), $f4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4
            ListSink

PREHOOK: query: explain extended select count(*) from (select `__time` from druid_table_alltypesorc limit 1) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain extended select count(*) from (select `__time` from druid_table_alltypesorc limit 1) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                        tag: -1
                        value expressions: _col0 (type: bigint)
                        auto parallelism: false
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_alltypesorc]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_alltypesorc
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    EXTERNAL TRUE
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,cstring1,cstring2,cdouble,cfloat,ctinyint,csmallint,cint,cbigint,cboolean1,cboolean2,cintstring,cfloatstring,cdoublestring
                    columns.comments 
                    columns.types timestamp with local time zone:string:string:double:float:tinyint:smallint:int:bigint:boolean:boolean:string:string:string
                    druid.datasource default.druid_table_alltypesorc
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.granularity MINUTE
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                    druid.query.type scan
                    druid.segment.granularity HOUR
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_alltypesorc
                    numFiles 0
                    numRows 9173
                    rawDataSize 0
                    serialization.ddl struct druid_table_alltypesorc { timestamp with local time zone __time, string cstring1, string cstring2, double cdouble, float cfloat, byte ctinyint, i16 csmallint, i32 cint, i64 cbigint, bool cboolean1, bool cboolean2, string cintstring, string cfloatstring, string cdoublestring}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.serde.DruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.DruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.serde.DruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      EXTERNAL TRUE
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,cstring1,cstring2,cdouble,cfloat,ctinyint,csmallint,cint,cbigint,cboolean1,cboolean2,cintstring,cfloatstring,cdoublestring
                      columns.comments 
                      columns.types timestamp with local time zone:string:string:double:float:tinyint:smallint:int:bigint:boolean:boolean:string:string:string
                      druid.datasource default.druid_table_alltypesorc
                      druid.fieldNames vc
                      druid.fieldTypes int
                      druid.query.granularity MINUTE
                      druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                      druid.query.type scan
                      druid.segment.granularity HOUR
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_alltypesorc
                      numFiles 0
                      numRows 9173
                      rawDataSize 0
                      serialization.ddl struct druid_table_alltypesorc { timestamp with local time zone __time, string cstring1, string cstring2, double cdouble, float cfloat, byte ctinyint, i16 csmallint, i32 cint, i64 cbigint, bool cboolean1, bool cboolean2, string cintstring, string cfloatstring, string cdoublestring}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.serde.DruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.DruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.serde.DruidSerDe
                    name: default.druid_table_alltypesorc
                  name: default.druid_table_alltypesorc
            Truncated Path -> Alias:
              /druid_table_alltypesorc [druid_table_alltypesorc]
        Reducer 2 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0
                        columns.types bigint
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain
SELECT `__time`
FROM druid_table_alltypesorc
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
SELECT `__time`
FROM druid_table_alltypesorc
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1968-01-01T08:00:00.000Z/1970-04-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_alltypesorc
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_alltypesorc
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: explain
SELECT `__time`
FROM druid_table_alltypesorc
WHERE ('1968-01-01 00:00:00' <= `__time` AND `__time` <= '1970-01-01 00:00:00')
    OR ('1968-02-01 00:00:00' <= `__time` AND `__time` <= '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
SELECT `__time`
FROM druid_table_alltypesorc
WHERE ('1968-01-01 00:00:00' <= `__time` AND `__time` <= '1970-01-01 00:00:00')
    OR ('1968-02-01 00:00:00' <= `__time` AND `__time` <= '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1968-01-01T08:00:00.000Z/1970-04-01T08:00:00.001Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_alltypesorc
WHERE ('1968-01-01 00:00:00' <= `__time` AND `__time` <= '1970-01-01 00:00:00')
    OR ('1968-02-01 00:00:00' <= `__time` AND `__time` <= '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_alltypesorc
WHERE ('1968-01-01 00:00:00' <= `__time` AND `__time` <= '1970-01-01 00:00:00')
    OR ('1968-02-01 00:00:00' <= `__time` AND `__time` <= '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames extract,cstring1,cstring2,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring2), sum($f3)
                    keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col2 (type: bigint), _col3 (type: double)
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cdouble), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(distinct cdouble), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames extract,cstring1,cdouble,$f3
                    druid.fieldTypes timestamp with local time zone,string,double,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cdouble","outputName":"cdouble","outputType":"DOUBLE"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cdouble), sum($f3)
                    keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col2 (type: bigint), _col3 (type: double)
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames extract,cstring1,cstring2,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","expression":"(2 * \"cdouble\")"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring2), sum($f3)
                    keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col2 (type: bigint), _col3 (type: double)
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames extract,cstring1,vc,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring2\",'_'),\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(vc), sum($f3)
                    keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col2 (type: bigint), _col3 (type: double)
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(DISTINCT cstring2) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames cstring2
                    druid.fieldTypes string
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1603744 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring2)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      sort order: 
                      Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames cstring2,$f1
                    druid.fieldTypes string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1673472 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring2), sum($f1)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      sort order: 
                      Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: bigint), _col1 (type: double)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc,$f1,$f2
                    druid.fieldTypes string,double,int
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring2\",'_'),\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"},{"type":"longMin","name":"$f2","fieldName":"cint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1708336 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(vc), sum($f1), min($f2)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      sort order: 
                      Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: bigint), _col1 (type: double), _col2 (type: int)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1), min(VALUE._col2)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1025}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc GROUP  BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	2.73086627938E7
PREHOOK: query: select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_alltypesorc GROUP  BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_alltypesorc GROUP  BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	5.46173255876E7
PREHOOK: query: select count(DISTINCT cstring2) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078
PREHOOK: query: explain select count(DISTINCT cstring1) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select count(DISTINCT cstring1) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames cstring1
                    druid.fieldTypes string
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1603744 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring1)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      sort order: 
                      Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select count(DISTINCT cstring1) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring1) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
3025
PREHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	2.73086627938E7
PREHOOK: query: select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6095	2.7308662793799996E7	-1073279343
PREHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1025
PREHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 200000) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 200000) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
9173
PREHOOK: query: select count(`__time`) from (select `__time` from druid_table_alltypesorc limit 200) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(`__time`) from (select `__time` from druid_table_alltypesorc limit 200) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
200
PREHOOK: query: select count(distinct `__time`) from druid_table_alltypesorc
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct `__time`) from druid_table_alltypesorc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
2
PREHOOK: query: select count(distinct `ctimestamp1`) from alltypesorc1  where ctimestamp1 IS NOT NULL
PREHOOK: type: QUERY
PREHOOK: Input: default@alltypesorc1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct `ctimestamp1`) from alltypesorc1  where ctimestamp1 IS NOT NULL
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alltypesorc1
POSTHOOK: Output: hdfs://### HDFS PATH ###
5523
PREHOOK: query: explain select `timets` from (select `__time` as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select `timets` from (select `__time` as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp with local time zone)
                    sort order: +
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC')","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp)
                    sort order: +
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp)
                outputColumnNames: _col0
                Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select `timets_with_tz` from (select `__time` as timets_with_tz from druid_table_alltypesorc order by timets_with_tz limit 10)  as src order by `timets_with_tz`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select `timets_with_tz` from (select `__time` as timets_with_tz from druid_table_alltypesorc order by timets_with_tz limit 10)  as src order by `timets_with_tz`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
PREHOOK: query: explain select unix_timestamp(from_unixtime(1396681200)) from druid_table_alltypesorc limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select unix_timestamp(from_unixtime(1396681200)) from druid_table_alltypesorc limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes bigint
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"unix_timestamp(timestamp_format((1396681200 * '1000'),'yyyy-MM-dd HH:mm:ss','UTC'),'yyyy-MM-dd HH:mm:ss')","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
            druid.query.type scan
          Select Operator
            expressions: vc (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select unix_timestamp(from_unixtime(1396681200)) from druid_table_alltypesorc limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select unix_timestamp(from_unixtime(1396681200)) from druid_table_alltypesorc limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1396681200
PREHOOK: query: explain select unix_timestamp(`__time`) from druid_table_alltypesorc limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select unix_timestamp(`__time`) from druid_table_alltypesorc limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes bigint
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"div(\"__time\",1000)","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
            druid.query.type scan
          Select Operator
            expressions: vc (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select unix_timestamp(`__time`) from druid_table_alltypesorc limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select unix_timestamp(`__time`) from druid_table_alltypesorc limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
-60
PREHOOK: query: explain select FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_alltypesorc
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_alltypesorc
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format((div(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),1000) * '1000'),'yyyy-MM-dd HH:mm:ss','UTC')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select FROM_UNIXTIME(UNIX_TIMESTAMP (CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_alltypesorc
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select FROM_UNIXTIME(UNIX_TIMESTAMP (CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_alltypesorc
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00
1969-12-31 16:00:00
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1Y','','UTC'),'yyyy-MM-dd','UTC')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YEAR') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YEAR')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YEAR') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YEAR')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YYYY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YYYY')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YYYY') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'YYYY')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1M','','UTC'),'yyyy-MM-dd','UTC')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MM') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MM')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MM') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MM')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MON') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MON')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MON') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'MON')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P3M','','UTC'),'yyyy-MM-dd','UTC')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-10-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'Q') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'Q')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'Q') from druid_table_alltypesorc GROUP BY TRUNC(cast(`__time` as timestamp), 'Q')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-10-01
PREHOOK: query: explain select TO_DATE(`__time`) from druid_table_alltypesorc GROUP BY TO_DATE(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select TO_DATE(`__time`) from druid_table_alltypesorc GROUP BY TO_DATE(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc
            druid.fieldTypes date
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_floor(\"__time\",'P1D','','US/Pacific')","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: date)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TO_DATE(`__time`) from druid_table_alltypesorc GROUP BY TO_DATE(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TO_DATE(`__time`) from druid_table_alltypesorc GROUP BY TO_DATE(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31
PREHOOK: query: EXPLAIN SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_alltypesorc` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_alltypesorc` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alias
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes date,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_floor(timestamp_parse(timestamp_format(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1M','','UTC'),'yyyy-MM-dd','UTC'),'','UTC'),'P1D','','UTC')","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","expression":"(\"cdouble\" * \"cdouble\")"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: $f1 (type: double), vc (type: date)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_alltypesorc` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_alltypesorc` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1.91216621994122E14	1969-12-01
PREHOOK: query: explain SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2, cast(`__time` as date) as orig_date, CAST((cdouble / 1000) AS INT) as offset from druid_table_alltypesorc  order by date_1, date_2 limit 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2, cast(`__time` as date) as orig_date, CAST((cdouble / 1000) AS INT) as offset from druid_table_alltypesorc  order by date_1, date_2 limit 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc,vc0,vc1,vc2
                    druid.fieldTypes date,date,date,int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_shift(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1D','','UTC'),'P1D',CAST((\"cdouble\" / 1000), 'LONG'),'UTC')","outputType":"LONG"},{"type":"expression","name":"vc0","expression":"timestamp_shift(timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1D','','UTC'),'P1D',-( CAST((\"cdouble\" / 1000), 'LONG') ),'UTC')","outputType":"LONG"},{"type":"expression","name":"vc1","expression":"timestamp_floor(timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC'),'P1D','','UTC')","outputType":"LONG"},{"type":"expression","name":"vc2","expression":"CAST((\"cdouble\" / 1000), 'LONG')","outputType":"LONG"}],"columns":["vc","vc0","vc1","vc2"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 1499152 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: date), vc0 (type: date), vc1 (type: date), vc2 (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 9173 Data size: 1499152 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: date), _col1 (type: date)
                      sort order: ++
                      Statistics: Num rows: 9173 Data size: 1499152 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
                      value expressions: _col2 (type: date), _col3 (type: int)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: date), KEY.reducesinkkey1 (type: date), VALUE._col0 (type: date), VALUE._col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 9173 Data size: 1499152 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 3
                  Statistics: Num rows: 3 Data size: 489 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 3 Data size: 489 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 3
      Processor Tree:
        ListSink

PREHOOK: query: SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2, cast(`__time` as date) as orig_date, CAST((cdouble / 1000) AS INT) as offset from druid_table_alltypesorc  order by date_1, date_2 limit 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2, cast(`__time` as date) as orig_date, CAST((cdouble / 1000) AS INT) as offset from druid_table_alltypesorc  order by date_1, date_2 limit 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-15	1970-01-16	1969-12-31	-16
1969-12-15	1970-01-16	1969-12-31	-16
1969-12-15	1970-01-16	1969-12-31	-16
PREHOOK: query: -- Boolean Values


 EXPLAIN SELECT cboolean2, count(*) from druid_table_alltypesorc GROUP BY cboolean2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- Boolean Values


 EXPLAIN SELECT cboolean2, count(*) from druid_table_alltypesorc GROUP BY cboolean2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames cboolean2,$f1
            druid.fieldTypes boolean,bigint
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"cboolean2","outputName":"cboolean2","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"count","name":"$f1"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cboolean2 (type: boolean), $f1 (type: bigint)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cboolean2, count(*) from druid_table_alltypesorc GROUP BY cboolean2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cboolean2, count(*) from druid_table_alltypesorc GROUP BY cboolean2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
false	6214
true	2959
PREHOOK: query: -- Expected results of this query are wrong due to https://issues.apache.org/jira/browse/CALCITE-2319
  -- It should get fixed once we upgrade calcite
 SELECT ctinyint > 2, count(*) from druid_table_alltypesorc GROUP BY ctinyint > 2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- Expected results of this query are wrong due to https://issues.apache.org/jira/browse/CALCITE-2319
  -- It should get fixed once we upgrade calcite
 SELECT ctinyint > 2, count(*) from druid_table_alltypesorc GROUP BY ctinyint > 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
false	4280
true	4893
PREHOOK: query: EXPLAIN SELECT ctinyint > 2, count(*) from druid_table_alltypesorc GROUP BY ctinyint > 2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT ctinyint > 2, count(*) from druid_table_alltypesorc GROUP BY ctinyint > 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes boolean,bigint
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"(\"ctinyint\" > 2)","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"count","name":"$f1"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: boolean), $f1 (type: bigint)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: EXPLAIN SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes boolean,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"1","outputType":"LONG"}],"limitSpec":{"type":"default"},"filter":{"type":"not","field":{"type":"selector","dimension":"cstring1","value":"en"}},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cfloat"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: $f1 (type: double)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
-39590.24699282646
PREHOOK: query: EXPLAIN SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011, 3.40
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011, 3.40
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes boolean,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"1","outputType":"LONG"}],"limitSpec":{"type":"default"},"filter":{"type":"not","field":{"type":"selector","dimension":"cstring1","value":"en"}},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cfloat"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: $f1 (type: double)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011, 3.40
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT sum(cfloat) FROM druid_table_alltypesorc WHERE cstring1 != 'en' group by 1.011, 3.40
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
-39590.24699282646
PREHOOK: query: EXPLAIN SELECT sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes boolean,bigint
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"1","outputType":"LONG"}],"limitSpec":{"type":"default"},"filter":{"type":"bound","dimension":"cfloat","lower":"0.011","lowerStrict":false,"upper":"0.011","upperStrict":false,"ordering":"numeric"},"aggregations":[{"type":"longSum","name":"$f1","fieldName":"cint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: $f1 (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
PREHOOK: query: EXPLAIN SELECT cfloat, sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT cfloat, sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alltypesorc
          properties:
            druid.fieldNames cfloat,$f1
            druid.fieldTypes float,bigint
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"1","outputType":"LONG"}],"limitSpec":{"type":"default"},"filter":{"type":"bound","dimension":"cfloat","lower":"0.011","lowerStrict":false,"upper":"0.011","upperStrict":false,"ordering":"numeric"},"aggregations":[{"type":"longSum","name":"$f1","fieldName":"cint"}],"postAggregations":[{"type":"expression","name":"cfloat","expression":"0.011"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: cfloat (type: float), $f1 (type: bigint)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT cfloat, sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cfloat, sum(cint) FROM druid_table_alltypesorc WHERE cfloat= 0.011 group by cfloat
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
PREHOOK: query: CREATE TABLE druid_table_n1
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
  SELECT cast (current_timestamp() as timestamp with local time zone) as `__time`,
cast(datetime1 as string) as datetime1,
cast(date1 as string) as date1,
cast(time1 as string) as time1
FROM TABLE (
VALUES
('2004-04-09 22:20:14', '2004-04-09','22:20:14'),
('2004-04-04 22:50:16', '2004-04-04', '22:50:16'),
('2004-04-12 04:40:49', '2004-04-12', '04:40:49'),
('2004-04-11 00:00:00', '2004-04-11', null),
('00:00:00 18:58:41', null, '18:58:41')) as q (datetime1, date1, time1)
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_n1
POSTHOOK: query: CREATE TABLE druid_table_n1
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
  SELECT cast (current_timestamp() as timestamp with local time zone) as `__time`,
cast(datetime1 as string) as datetime1,
cast(date1 as string) as date1,
cast(time1 as string) as time1
FROM TABLE (
VALUES
('2004-04-09 22:20:14', '2004-04-09','22:20:14'),
('2004-04-04 22:50:16', '2004-04-04', '22:50:16'),
('2004-04-12 04:40:49', '2004-04-12', '04:40:49'),
('2004-04-11 00:00:00', '2004-04-11', null),
('00:00:00 18:58:41', null, '18:58:41')) as q (datetime1, date1, time1)
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_n1
POSTHOOK: Lineage: druid_table_n1.__time SIMPLE []
POSTHOOK: Lineage: druid_table_n1.date1 SCRIPT []
POSTHOOK: Lineage: druid_table_n1.datetime1 SCRIPT []
POSTHOOK: Lineage: druid_table_n1.time1 SCRIPT []
PREHOOK: query: EXPLAIN SELECT TO_DATE(date1), TO_DATE(datetime1) FROM druid_table_n1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN SELECT TO_DATE(date1), TO_DATE(datetime1) FROM druid_table_n1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n1
          properties:
            druid.fieldNames vc,vc0
            druid.fieldTypes date,date
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n1","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_floor(timestamp_parse(\"date1\",'','UTC'),'P1D','','UTC')","outputType":"LONG"},{"type":"expression","name":"vc0","expression":"timestamp_floor(timestamp_parse(\"datetime1\",'','UTC'),'P1D','','UTC')","outputType":"LONG"}],"columns":["vc","vc0"],"resultFormat":"compactedList"}
            druid.query.type scan
          Select Operator
            expressions: vc (type: date), vc0 (type: date)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT TO_DATE(date1), TO_DATE(datetime1) FROM druid_table_n1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT TO_DATE(date1), TO_DATE(datetime1) FROM druid_table_n1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	NULL
2004-04-04	2004-04-04
2004-04-09	2004-04-09
2004-04-11	2004-04-11
2004-04-12	2004-04-12
PREHOOK: query: EXPLAIN select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1025}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from (select `__time` from druid_table_alltypesorc limit 1025) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1025
PREHOOK: query: explain select `timets` from (select `__time` as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select `timets` from (select `__time` as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp with local time zone
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp with local time zone)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp with local time zone)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp with local time zone)
                    sort order: +
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp with local time zone)
                outputColumnNames: _col0
                Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes timestamp
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_parse(timestamp_format(\"__time\",'yyyy-MM-dd\\u0027T\\u0027HH:mm:ss.SSS\\u0027Z\\u0027','US/Pacific'),'','UTC')","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: timestamp)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: timestamp)
                      sort order: +
                      Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp)
                outputColumnNames: _col0
                Statistics: Num rows: 9173 Data size: 348640 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp)
                    sort order: +
                    Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: timestamp)
                outputColumnNames: _col0
                Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 380 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select `timets_with_tz` from (select `__time` as timets_with_tz from druid_table_alltypesorc order by timets_with_tz limit 10)  as src order by `timets_with_tz`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select `timets_with_tz` from (select `__time` as timets_with_tz from druid_table_alltypesorc order by timets_with_tz limit 10)  as src order by `timets_with_tz`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select `timets` from (select cast(`__time` as timestamp ) as timets from druid_table_alltypesorc order by timets limit 10)  as src order by `timets`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
1969-12-31 15:59:00
PREHOOK: query: select count(cfloat) from (select `cfloat`, `cstring1` from druid_table_alltypesorc limit 1025) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(cfloat) from (select `cfloat`, `cstring1` from druid_table_alltypesorc limit 1025) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
1025
PREHOOK: query: select count(cstring1) from (select `cfloat`, `cstring1` from druid_table_alltypesorc limit 90000) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(cstring1) from (select `cfloat`, `cstring1` from druid_table_alltypesorc limit 90000) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
6059
PREHOOK: query: explain select count(cstring1) from (select `cfloat`, `cstring1`, `cint` from druid_table_alltypesorc limit 90000) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select count(cstring1) from (select `cfloat`, `cstring1`, `cint` from druid_table_alltypesorc limit 90000) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames cstring1
                    druid.fieldTypes string
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["cstring1"],"resultFormat":"compactedList","limit":90000}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 1603744 Basic stats: COMPLETE Column stats: NONE
                  Group By Operator
                    aggregations: count(cstring1)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      sort order: 
                      Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select max(cint * cdouble) from (select `cfloat`, `cstring1`, `cint`, `cdouble` from druid_table_alltypesorc limit 90000) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select max(cint * cdouble) from (select `cfloat`, `cstring1`, `cint`, `cdouble` from druid_table_alltypesorc limit 90000) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
5.16019886323666E15
PREHOOK: query: explain select max(cint * cfloat) from (select `cfloat`, `cstring1`, `cint`, `cdouble` from druid_table_alltypesorc limit 90000) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select max(cint * cfloat) from (select `cfloat`, `cstring1`, `cint`, `cdouble` from druid_table_alltypesorc limit 90000) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames cfloat,cint
                    druid.fieldTypes float,int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_alltypesorc","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["cfloat","cint"],"resultFormat":"compactedList","limit":90000}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 69728 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: (UDFToFloat(cint) * cfloat) (type: float)
                    outputColumnNames: _col0
                    Statistics: Num rows: 9173 Data size: 69728 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: max(_col0)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: float)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select count(distinct `__time`, cint) from (select * from druid_table_alltypesorc) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select count(distinct `__time`, cint) from (select * from druid_table_alltypesorc) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_alltypesorc
                  properties:
                    druid.fieldNames extract,cint
                    druid.fieldTypes timestamp with local time zone,int
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_alltypesorc","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"cint","outputName":"cint","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 34864 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cint (type: int)
                    outputColumnNames: cint
                    Statistics: Num rows: 9173 Data size: 34864 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cint)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select count(distinct `__time`, cint) from (select * from druid_table_alltypesorc) as src
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_alltypesorc
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct `__time`, cint) from (select * from druid_table_alltypesorc) as src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_alltypesorc
POSTHOOK: Output: hdfs://### HDFS PATH ###
3027
