PREHOOK: query: CREATE TABLE druid_table_n0
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@alltypesorc
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_n0
POSTHOOK: query: CREATE TABLE druid_table_n0
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES ("druid.segment.granularity" = "HOUR", "druid.query.granularity" = "MINUTE")
AS
SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
  cstring1,
  cstring2,
  cdouble,
  cfloat,
  ctinyint,
  csmallint,
  cint,
  cbigint,
  cboolean1,
  cboolean2
  FROM alltypesorc where ctimestamp1 IS NOT NULL
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@alltypesorc
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_n0
POSTHOOK: Lineage: druid_table_n0.__time EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]
POSTHOOK: Lineage: druid_table_n0.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]
PREHOOK: query: -- MATH AND STRING functions

SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- MATH AND STRING functions

SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
837
PREHOOK: query: SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
4838
PREHOOK: query: SELECT count(*) FROM druid_table_n0 WHERE power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT count(*) FROM druid_table_n0 WHERE power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
837
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
19884.64700973034	27373419	14472	8.51628242804E11	851620413654	68151649880
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
54430.27000427246	-3740445	51268	1.31919188502E11	131922984948	92160895030
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE  ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE  ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
-2389.5169917345047	27640645	-5707	7.19705549994E11	719697428706	13774723379
PREHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE  SIN(cdouble) > 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT  SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
FROM druid_table_n0 WHERE  SIN(cdouble) > 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
PREHOOK: query: SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_n0 WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
 GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_n0 WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
 GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
8Nj7qpHBTH1GUkMM1BXr2_YyROa06YMuK3C2eg85d	yRO	YYROA06YMUK3C2EG85D	8nj7qphbth1gukmm1bxr2	0.0
k7rg3Vw6IpwU6_YyKI8Rb72WP5dP1BMSPoT	yKI	YYKI8RB72WP5DP1BMSPOT	k7rg3vw6ipwu6	0.0
b0r8g21X6I2TvvPj623IKR_YxSwHWr	xSw	YXSWHWR	b0r8g21x6i2tvvpj623ikr	0.0
ox4gTH52_YsjDHuPsD2	sjD	YSJDHUPSD2	ox4gth52	0.0
NEGa0N8MJ2dnn3MKAfl6u_Yr4e3n	r4e	YR4E3N	nega0n8mj2dnn3mkafl6u	0.0
767fOfF1Oj8fyOv6YFI16rM_YqdbA5	qdb	YQDBA5	767foff1oj8fyov6yfi16rm	0.0
kM4k0y1fqwton_YpK3CTDWEXOV	pK3	YPK3CTDWEXOV	km4k0y1fqwton	0.0
TBI20Ba2YuO44754E2BM_YpB20i4	pB2	YPB20I4	tbi20ba2yuo44754e2bm	0.0
jiqEpNs7qXo0y37_Ynnw5opXqf6BU	nnw	YNNW5OPXQF6BU	jiqepns7qxo0y37	0.0
TgS6dAlI2w4y_Ynh42DscA373RX27nBkft	nh4	YNH42DSCA373RX27NBKFT	tgs6dali2w4y	0.0
PREHOOK: query: EXPLAIN SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT count(*) FROM druid_table_n0 WHERE character_length(CAST(ctinyint AS STRING)) > 1 AND char_length(CAST(ctinyint AS STRING)) < 10 AND power(cfloat, 2) * pow(csmallint, 3) > 1 AND SQRT(ABS(ctinyint)) > 3
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames $f0
            druid.fieldTypes bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n0","descending":false,"granularity":"all","filter":{"type":"and","fields":[{"type":"expression","expression":"(strlen(CAST(\"ctinyint\", 'STRING')) > 1)"},{"type":"expression","expression":"(strlen(CAST(\"ctinyint\", 'STRING')) < 10)"},{"type":"expression","expression":"((pow(\"cfloat\",2) * pow(\"csmallint\",3)) > 1)"},{"type":"expression","expression":"(sqrt(abs(\"ctinyint\")) > 3)"}]},"aggregations":[{"type":"count","name":"$f0"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":false}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: EXPLAIN SELECT SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
        FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT SUM(cfloat + 1), CAST(SUM(cdouble + ctinyint) AS INTEGER), SUM(ctinyint) + 1 , CAST(SUM(csmallint) + SUM(cint) AS DOUBLE), SUM(cint), SUM(cbigint)
        FROM druid_table_n0 WHERE ceil(cfloat) > 0 AND floor(cdouble) * 2 < 1000 OR ln(cdouble) / log10(10) > 0 AND COS(cint) > 0 OR SIN(cdouble) > 1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames $f0,_o__c1,_o__c2,_o__c3,$f4,$f5
            druid.fieldTypes double,int,bigint,double,bigint,bigint
            druid.query.json {"queryType":"timeseries","dataSource":"default.druid_table_n0","descending":false,"granularity":"all","filter":{"type":"or","fields":[{"type":"and","fields":[{"type":"expression","expression":"(ceil(\"cfloat\") > 0)"},{"type":"expression","expression":"((floor(\"cdouble\") * 2) < 1000)"}]},{"type":"and","fields":[{"type":"expression","expression":"((log(\"cdouble\") / 1.0) > 0)"},{"type":"expression","expression":"(cos(\"cint\") > 0)"}]},{"type":"expression","expression":"(sin(\"cdouble\") > 1)"}]},"aggregations":[{"type":"doubleSum","name":"$f0","expression":"(\"cfloat\" + CAST(1, 'DOUBLE'))"},{"type":"doubleSum","name":"$f1","expression":"(\"cdouble\" + CAST(\"ctinyint\", 'DOUBLE'))"},{"type":"longSum","name":"$f2","fieldName":"ctinyint"},{"type":"longSum","name":"$f3","fieldName":"csmallint"},{"type":"longSum","name":"$f4","fieldName":"cint"},{"type":"longSum","name":"$f5","fieldName":"cbigint"}],"postAggregations":[{"type":"expression","name":"_o__c1","expression":"CAST(\"$f1\", 'LONG')"},{"type":"expression","name":"_o__c2","expression":"(\"$f2\" + 1)"},{"type":"expression","name":"_o__c3","expression":"CAST((\"$f3\" + \"$f4\"), 'DOUBLE')"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"context":{"skipEmptyBuckets":true}}
            druid.query.type timeseries
          Select Operator
            expressions: $f0 (type: double), _o__c1 (type: int), _o__c2 (type: bigint), _o__c3 (type: double), $f4 (type: bigint), $f5 (type: bigint)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            ListSink

PREHOOK: query: EXPLAIN SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_n0 WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
         GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT cstring1 || '_'|| cstring2, substring(cstring2, 2, 3) as concat , upper(cstring2), lower(cstring1), SUM(cdouble) as s FROM druid_table_n0 WHERE cstring1 IS NOT NULL AND cstring2 IS NOT NULL AND cstring2 like 'Y%'
         GROUP BY cstring1 || '_'|| cstring2, substring(cstring2, 2, 3), upper(cstring2), lower(cstring1) ORDER BY concat DESC LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc,vc0,vc1,vc2,$f4
            druid.fieldTypes string,string,string,string,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"},{"type":"default","dimension":"vc0","outputName":"vc0","outputType":"STRING"},{"type":"default","dimension":"vc1","outputName":"vc1","outputType":"STRING"},{"type":"default","dimension":"vc2","outputName":"vc2","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring1\",'_'),\"cstring2\")","outputType":"STRING"},{"type":"expression","name":"vc0","expression":"substring(\"cstring2\", 1, 3)","outputType":"STRING"},{"type":"expression","name":"vc1","expression":"upper(\"cstring2\")","outputType":"STRING"},{"type":"expression","name":"vc2","expression":"lower(\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"vc0","direction":"descending","dimensionOrder":"lexicographic"}]},"filter":{"type":"and","fields":[{"type":"expression","expression":"like(\"cstring2\",'Y%')"},{"type":"not","field":{"type":"selector","dimension":"cstring1","value":null}},{"type":"not","field":{"type":"selector","dimension":"cstring2","value":null}}]},"aggregations":[{"type":"doubleSum","name":"$f4","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string), vc0 (type: string), vc1 (type: string), vc2 (type: string), $f4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4
            ListSink

PREHOOK: query: explain extended select count(*) from (select `__time` from druid_table_n0 limit 1) as src
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select count(*) from (select `__time` from druid_table_n0 limit 1) as src
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    Statistics: Num rows: 9173 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                        tag: -1
                        value expressions: _col0 (type: bigint)
                        auto parallelism: false
            Path -> Alias:
              hdfs://### HDFS PATH ### [druid_table_n0]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: druid_table_n0
                  input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                  output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns __time,cstring1,cstring2,cdouble,cfloat,ctinyint,csmallint,cint,cbigint,cboolean1,cboolean2
                    columns.comments 
                    columns.types timestamp with local time zone:string:string:double:float:tinyint:smallint:int:bigint:boolean:boolean
                    druid.datasource default.druid_table_n0
                    druid.fieldNames vc
                    druid.fieldTypes int
                    druid.query.granularity MINUTE
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                    druid.query.type scan
                    druid.segment.granularity HOUR
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.druid_table_n0
                    numFiles 0
                    numRows 9173
                    rawDataSize 0
                    serialization.ddl struct druid_table_n0 { timestamp with local time zone __time, string cstring1, string cstring2, double cdouble, float cfloat, byte ctinyint, i16 csmallint, i32 cint, i64 cbigint, bool cboolean1, bool cboolean2}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.druid.serde.DruidSerDe
                    storage_handler org.apache.hadoop.hive.druid.DruidStorageHandler
                    totalSize 0
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.druid.serde.DruidSerDe
                
                    input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
                    output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns __time,cstring1,cstring2,cdouble,cfloat,ctinyint,csmallint,cint,cbigint,cboolean1,cboolean2
                      columns.comments 
                      columns.types timestamp with local time zone:string:string:double:float:tinyint:smallint:int:bigint:boolean:boolean
                      druid.datasource default.druid_table_n0
                      druid.fieldNames vc
                      druid.fieldTypes int
                      druid.query.granularity MINUTE
                      druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"0","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
                      druid.query.type scan
                      druid.segment.granularity HOUR
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.druid_table_n0
                      numFiles 0
                      numRows 9173
                      rawDataSize 0
                      serialization.ddl struct druid_table_n0 { timestamp with local time zone __time, string cstring1, string cstring2, double cdouble, float cfloat, byte ctinyint, i16 csmallint, i32 cint, i64 cbigint, bool cboolean1, bool cboolean2}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.druid.serde.DruidSerDe
                      storage_handler org.apache.hadoop.hive.druid.DruidStorageHandler
                      totalSize 0
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.druid.serde.DruidSerDe
                    name: default.druid_table_n0
                  name: default.druid_table_n0
            Truncated Path -> Alias:
              /druid_table_n0 [druid_table_n0]
        Reducer 2 
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 1 Data size: 8 Basic stats: PARTIAL Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0
                        columns.types bigint
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT `__time`
FROM druid_table_n0
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT `__time`
FROM druid_table_n0
WHERE (`__time` BETWEEN '1968-01-01 00:00:00' AND '1970-01-01 00:00:00')
    OR (`__time` BETWEEN '1968-02-01 00:00:00' AND '1970-04-01 00:00:00') ORDER BY `__time` ASC LIMIT 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
1969-12-31 15:59:00.0 US/Pacific
PREHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames extract,cstring1,cstring2,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"US/Pacific"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: extract (type: timestamp with local time zone), cstring1 (type: string), cstring2 (type: string), $f3 (type: double)
                    outputColumnNames: extract, cstring1, cstring2, $f3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cstring2), sum($f3)
                      keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: bigint), _col3 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cdouble), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(distinct cdouble), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames extract,cstring1,cdouble,$f3
                    druid.fieldTypes timestamp with local time zone,string,double,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"US/Pacific"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cdouble","outputName":"cdouble","outputType":"DOUBLE"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: extract (type: timestamp with local time zone), cstring1 (type: string), cdouble (type: double), $f3 (type: double)
                    outputColumnNames: extract, cstring1, cdouble, $f3
                    Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cdouble), sum($f3)
                      keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        Statistics: Num rows: 9173 Data size: 2091840 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: bigint), _col3 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1045805 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames extract,cstring1,cstring2,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"US/Pacific"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","expression":"(CAST(2, 'DOUBLE') * \"cdouble\")"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: extract (type: timestamp with local time zone), cstring1 (type: string), cstring2 (type: string), $f3 (type: double)
                    outputColumnNames: extract, cstring1, cstring2, $f3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cstring2), sum($f3)
                      keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: bigint), _col3 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble) FROM druid_table_n0 GROUP  BY `__time`, `cstring1`
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames extract,cstring1,vc,$f3
                    druid.fieldTypes timestamp with local time zone,string,string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"US/Pacific"}},{"type":"default","dimension":"cstring1","outputName":"cstring1","outputType":"STRING"},{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring2\",'_'),\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f3","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: extract (type: timestamp with local time zone), cstring1 (type: string), vc (type: string), $f3 (type: double)
                    outputColumnNames: extract, cstring1, vc, $f3
                    Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(vc), sum($f3)
                      keys: extract (type: timestamp with local time zone), cstring1 (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: timestamp with local time zone), _col1 (type: string)
                        Statistics: Num rows: 9173 Data size: 3625856 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: bigint), _col3 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: timestamp with local time zone), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: bigint), _col3 (type: double)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 4586 Data size: 1812730 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(DISTINCT cstring2) FROM druid_table_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2) FROM druid_table_n0
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames cstring2
                    druid.fieldTypes string
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1603744 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cstring2 (type: string)
                    outputColumnNames: cstring2
                    Statistics: Num rows: 9173 Data size: 1603744 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cstring2)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 192 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames cstring2,$f1
                    druid.fieldTypes string,double
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"cstring2","outputName":"cstring2","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1673472 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cstring2 (type: string), $f1 (type: double)
                    outputColumnNames: cstring2, $f1
                    Statistics: Num rows: 9173 Data size: 1673472 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(cstring2), sum($f1)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint), _col1 (type: double)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 208 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_n0
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames vc,$f1,$f2
                    druid.fieldTypes string,double,int
                    druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"concat(concat(\"cstring2\",'_'),\"cstring1\")","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","fieldName":"cdouble"},{"type":"longMin","name":"$f2","fieldName":"cint"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
                    druid.query.type groupBy
                  Statistics: Num rows: 9173 Data size: 1708336 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: string), $f1 (type: double), $f2 (type: int)
                    outputColumnNames: vc, $f1, $f2
                    Statistics: Num rows: 9173 Data size: 1708336 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(vc), sum($f1), min($f2)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint), _col1 (type: double), _col2 (type: int)
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), sum(VALUE._col1), min(VALUE._col2)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0 GROUP  BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0 GROUP  BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	2.7308662809692383E7
PREHOOK: query: select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_n0 GROUP  BY floor_year(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct cstring2), sum(2 * cdouble) FROM druid_table_n0 GROUP  BY floor_year(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	5.4617325619384766E7
PREHOOK: query: select count(DISTINCT cstring2) FROM druid_table_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2) FROM druid_table_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078
PREHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(DISTINCT cstring2), sum(cdouble) FROM druid_table_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
6078	2.7308662809692383E7
PREHOOK: query: select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(distinct cstring2 || '_'|| cstring1), sum(cdouble), min(cint) FROM druid_table_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
6095	2.7308662809692383E7	-1073279343
PREHOOK: query: explain select unix_timestamp(from_unixtime(1396681200)) from druid_table_n0 limit 1
PREHOOK: type: QUERY
POSTHOOK: query: explain select unix_timestamp(from_unixtime(1396681200)) from druid_table_n0 limit 1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes bigint
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"unix_timestamp(timestamp_format((1396681200 * '1000'),'yyyy-MM-dd HH:mm:ss','US/Pacific'),'yyyy-MM-dd HH:mm:ss')","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
            druid.query.type scan
          Select Operator
            expressions: vc (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select unix_timestamp(from_unixtime(1396681200)) from druid_table_n0 limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select unix_timestamp(from_unixtime(1396681200)) from druid_table_n0 limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1396656000
PREHOOK: query: explain select unix_timestamp(`__time`) from druid_table_n0 limit 1
PREHOOK: type: QUERY
POSTHOOK: query: explain select unix_timestamp(`__time`) from druid_table_n0 limit 1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes bigint
            druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"div(\"__time\",1000)","outputType":"LONG"}],"columns":["vc"],"resultFormat":"compactedList","limit":1}
            druid.query.type scan
          Select Operator
            expressions: vc (type: bigint)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select unix_timestamp(`__time`) from druid_table_n0 limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select unix_timestamp(`__time`) from druid_table_n0 limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
-60
PREHOOK: query: explain select FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_n0
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
PREHOOK: type: QUERY
POSTHOOK: query: explain select FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_n0
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format((div(\"__time\",1000) * '1000'),'yyyy-MM-dd HH:mm:ss','US/Pacific')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select FROM_UNIXTIME(UNIX_TIMESTAMP (CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_n0
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select FROM_UNIXTIME(UNIX_TIMESTAMP (CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
from druid_table_n0
GROUP BY FROM_UNIXTIME(UNIX_TIMESTAMP(CAST(`__time` as timestamp ),'yyyy-MM-dd HH:mm:ss' ),'yyyy-MM-dd HH:mm:ss')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31 15:59:00
1969-12-31 16:00:00
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
PREHOOK: type: QUERY
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(\"__time\",'P1Y','','US/Pacific'),'yyyy-MM-dd','US/Pacific')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YY')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YEAR') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YEAR')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YEAR') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YEAR')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YYYY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YYYY')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'YYYY') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'YYYY')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-01-01
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
PREHOOK: type: QUERY
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(\"__time\",'P1M','','US/Pacific'),'yyyy-MM-dd','US/Pacific')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MONTH') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MONTH')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MM') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MM')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MM') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MM')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MON') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MON')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'MON') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'MON')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-01
PREHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
PREHOOK: type: QUERY
POSTHOOK: query: explain select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_format(timestamp_floor(\"__time\",'P3M','','US/Pacific'),'yyyy-MM-dd','US/Pacific')","outputType":"STRING"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'QUARTER') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'QUARTER')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-10-01
PREHOOK: query: select TRUNC(cast(`__time` as timestamp), 'Q') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'Q')
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TRUNC(cast(`__time` as timestamp), 'Q') from druid_table_n0 GROUP BY TRUNC(cast(`__time` as timestamp), 'Q')
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-10-01
PREHOOK: query: explain select TO_DATE(`__time`) from druid_table_n0 GROUP BY TO_DATE(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: explain select TO_DATE(`__time`) from druid_table_n0 GROUP BY TO_DATE(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc
            druid.fieldTypes date
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_floor(\"__time\",'P1D','','US/Pacific')","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: date)
            outputColumnNames: _col0
            ListSink

PREHOOK: query: select TO_DATE(`__time`) from druid_table_n0 GROUP BY TO_DATE(`__time`)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select TO_DATE(`__time`) from druid_table_n0 GROUP BY TO_DATE(`__time`)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-12-31
PREHOOK: query: EXPLAIN SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_n0` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_n0` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_alias
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes date,double
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"LONG"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_floor(timestamp_parse(timestamp_format(timestamp_floor(\"__time\",'P1M','','US/Pacific'),'yyyy-MM-dd','US/Pacific'),'','US/Pacific'),'P1D','','US/Pacific')","outputType":"LONG"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleSum","name":"$f1","expression":"(\"cdouble\" * \"cdouble\")"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: $f1 (type: double), vc (type: date)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_n0` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT SUM((`druid_table_alias`.`cdouble` * `druid_table_alias`.`cdouble`)) AS `sum_calculation_4998925219892510720_ok`,
  CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`
FROM `default`.`druid_table_n0` `druid_table_alias`
GROUP BY CAST(TRUNC(CAST(`druid_table_alias`.`__time` AS TIMESTAMP),'MM') AS DATE)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
3.832948323764436E14	1969-12-01
PREHOOK: query: explain SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2 from druid_table_n0  order by date_1, date_2 limit 3
PREHOOK: type: QUERY
POSTHOOK: query: explain SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2 from druid_table_n0  order by date_1, date_2 limit 3
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: druid_table_n0
                  properties:
                    druid.fieldNames vc,vc0
                    druid.fieldTypes date,date
                    druid.query.json {"queryType":"scan","dataSource":"default.druid_table_n0","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"timestamp_shift(timestamp_floor(\"__time\",'P1D','','US/Pacific'),'P1D',CAST((\"cdouble\" / CAST(1000, 'DOUBLE')), 'LONG'),'US/Pacific')","outputType":"LONG"},{"type":"expression","name":"vc0","expression":"timestamp_shift(timestamp_floor(\"__time\",'P1D','','US/Pacific'),'P1D',-( CAST((\"cdouble\" / CAST(1000, 'DOUBLE')), 'LONG') ),'US/Pacific')","outputType":"LONG"}],"columns":["vc","vc0"],"resultFormat":"compactedList"}
                    druid.query.type scan
                  Statistics: Num rows: 9173 Data size: 976192 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: vc (type: date), vc0 (type: date)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 9173 Data size: 976192 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: date), _col1 (type: date)
                      sort order: ++
                      Statistics: Num rows: 9173 Data size: 976192 Basic stats: COMPLETE Column stats: NONE
                      TopN Hash Memory Usage: 0.1
        Reducer 2 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: date), KEY.reducesinkkey1 (type: date)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 9173 Data size: 976192 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 3
                  Statistics: Num rows: 3 Data size: 318 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 3 Data size: 318 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 3
      Processor Tree:
        ListSink

PREHOOK: query: SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2 from druid_table_n0  order by date_1, date_2 limit 3
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DATE_ADD(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_1,  DATE_SUB(cast(`__time` as date), CAST((cdouble / 1000) AS INT)) as date_2 from druid_table_n0  order by date_1, date_2 limit 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
1969-02-26	1970-11-04
1969-03-19	1970-10-14
1969-11-13	1970-02-17
PREHOOK: query: -- Boolean Values
 SELECT cboolean2, count(*) from druid_table_n0 GROUP BY cboolean2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- Boolean Values
 SELECT cboolean2, count(*) from druid_table_n0 GROUP BY cboolean2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
NULL	8
false	3140
true	2957
PREHOOK: query: -- Expected results of this query are wrong due to https://issues.apache.org/jira/browse/CALCITE-2319
  -- It should get fixed once we upgrade calcite
 SELECT ctinyint > 2, count(*) from druid_table_n0 GROUP BY ctinyint > 2
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: -- Expected results of this query are wrong due to https://issues.apache.org/jira/browse/CALCITE-2319
  -- It should get fixed once we upgrade calcite
 SELECT ctinyint > 2, count(*) from druid_table_n0 GROUP BY ctinyint > 2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
false	2653
false	3452
PREHOOK: query: EXPLAIN SELECT ctinyint > 2, count(*) from druid_table_n0 GROUP BY ctinyint > 2
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT ctinyint > 2, count(*) from druid_table_n0 GROUP BY ctinyint > 2
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_n0
          properties:
            druid.fieldNames vc,$f1
            druid.fieldTypes boolean,bigint
            druid.query.json {"queryType":"groupBy","dataSource":"default.druid_table_n0","granularity":"all","dimensions":[{"type":"default","dimension":"vc","outputName":"vc","outputType":"STRING"}],"virtualColumns":[{"type":"expression","name":"vc","expression":"(\"ctinyint\" > 2)","outputType":"FLOAT"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"count","name":"$f1"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Select Operator
            expressions: vc (type: boolean), $f1 (type: bigint)
            outputColumnNames: _col0, _col1
            ListSink

PREHOOK: query: DROP TABLE druid_table_n0
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@druid_table_n0
PREHOOK: Output: default@druid_table_n0
POSTHOOK: query: DROP TABLE druid_table_n0
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@druid_table_n0
POSTHOOK: Output: default@druid_table_n0
