PREHOOK: query: CREATE EXTERNAL TABLE druid_kafka_test_avro(`__time` timestamp , `page` string, `user` string, `language` string,
                                            `country` string,`continent` string, `namespace` string, `newPage` boolean, `unpatrolled` boolean,
                                            `anonymous` boolean, `robot` boolean, added int, deleted int, delta bigint)
        STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
        TBLPROPERTIES (
        "druid.segment.granularity" = "MONTH",
        "druid.query.granularity" = "MINUTE",
        "kafka.bootstrap.servers" = "localhost:9092",
        "kafka.topic" = "wiki_kafka_avro_table",
        "druid.kafka.ingestion.useEarliestOffset" = "true",
        "druid.kafka.ingestion.maxRowsInMemory" = "5",
        "druid.kafka.ingestion.startDelay" = "PT1S",
        "druid.kafka.ingestion.taskDuration" = "PT30S",
        "druid.kafka.ingestion.period" = "PT5S",
        "druid.kafka.ingestion.consumer.retries" = "2",
        "druid.kafka.ingestion.reportParseExceptions" = "true",
        "druid.timestamp.column" = "timestamp",
        "druid.timestamp.format" = "MM/dd/yyyy HH:mm:ss",
        "druid.parseSpec.format" = "avro",
        'avro.schema.literal'='{
          "type" : "record",
          "name" : "Wikipedia",
          "namespace" : "org.apache.hive.kafka",
          "version": "1",
          "fields" : [ {
            "name" : "isrobot",
            "type" : "boolean"
          }, {
            "name" : "channel",
            "type" : "string"
          }, {
            "name" : "timestamp",
            "type" : "string"
          }, {
            "name" : "flags",
            "type" : "string"
          }, {
            "name" : "isunpatrolled",
            "type" : "boolean"
          }, {
            "name" : "page",
            "type" : "string"
          }, {
            "name" : "diffurl",
            "type" : "string"
          }, {
            "name" : "added",
            "type" : "long"
          }, {
            "name" : "comment",
            "type" : "string"
          }, {
            "name" : "commentlength",
            "type" : "long"
          }, {
            "name" : "isnew",
            "type" : "boolean"
          }, {
            "name" : "isminor",
            "type" : "boolean"
          }, {
            "name" : "delta",
            "type" : "long"
          }, {
            "name" : "isanonymous",
            "type" : "boolean"
          }, {
            "name" : "user",
            "type" : "string"
          }, {
            "name" : "deltabucket",
            "type" : "double"
          }, {
            "name" : "deleted",
            "type" : "long"
          }, {
            "name" : "namespace",
            "type" : "string"
          } ]
        }'
        )
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_kafka_test_avro
POSTHOOK: query: CREATE EXTERNAL TABLE druid_kafka_test_avro(`__time` timestamp , `page` string, `user` string, `language` string,
                                            `country` string,`continent` string, `namespace` string, `newPage` boolean, `unpatrolled` boolean,
                                            `anonymous` boolean, `robot` boolean, added int, deleted int, delta bigint)
        STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
        TBLPROPERTIES (
        "druid.segment.granularity" = "MONTH",
        "druid.query.granularity" = "MINUTE",
        "kafka.bootstrap.servers" = "localhost:9092",
        "kafka.topic" = "wiki_kafka_avro_table",
        "druid.kafka.ingestion.useEarliestOffset" = "true",
        "druid.kafka.ingestion.maxRowsInMemory" = "5",
        "druid.kafka.ingestion.startDelay" = "PT1S",
        "druid.kafka.ingestion.taskDuration" = "PT30S",
        "druid.kafka.ingestion.period" = "PT5S",
        "druid.kafka.ingestion.consumer.retries" = "2",
        "druid.kafka.ingestion.reportParseExceptions" = "true",
        "druid.timestamp.column" = "timestamp",
        "druid.timestamp.format" = "MM/dd/yyyy HH:mm:ss",
        "druid.parseSpec.format" = "avro",
        'avro.schema.literal'='{
          "type" : "record",
          "name" : "Wikipedia",
          "namespace" : "org.apache.hive.kafka",
          "version": "1",
          "fields" : [ {
            "name" : "isrobot",
            "type" : "boolean"
          }, {
            "name" : "channel",
            "type" : "string"
          }, {
            "name" : "timestamp",
            "type" : "string"
          }, {
            "name" : "flags",
            "type" : "string"
          }, {
            "name" : "isunpatrolled",
            "type" : "boolean"
          }, {
            "name" : "page",
            "type" : "string"
          }, {
            "name" : "diffurl",
            "type" : "string"
          }, {
            "name" : "added",
            "type" : "long"
          }, {
            "name" : "comment",
            "type" : "string"
          }, {
            "name" : "commentlength",
            "type" : "long"
          }, {
            "name" : "isnew",
            "type" : "boolean"
          }, {
            "name" : "isminor",
            "type" : "boolean"
          }, {
            "name" : "delta",
            "type" : "long"
          }, {
            "name" : "isanonymous",
            "type" : "boolean"
          }, {
            "name" : "user",
            "type" : "string"
          }, {
            "name" : "deltabucket",
            "type" : "double"
          }, {
            "name" : "deleted",
            "type" : "long"
          }, {
            "name" : "namespace",
            "type" : "string"
          } ]
        }'
        )
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_kafka_test_avro
PREHOOK: query: ALTER TABLE druid_kafka_test_avro SET TBLPROPERTIES('druid.kafka.ingestion' = 'START')
PREHOOK: type: ALTERTABLE_PROPERTIES
PREHOOK: Input: default@druid_kafka_test_avro
PREHOOK: Output: default@druid_kafka_test_avro
POSTHOOK: query: ALTER TABLE druid_kafka_test_avro SET TBLPROPERTIES('druid.kafka.ingestion' = 'START')
POSTHOOK: type: ALTERTABLE_PROPERTIES
POSTHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: Output: default@druid_kafka_test_avro
["default.druid_kafka_test_avro"]
PREHOOK: query: DESCRIBE druid_kafka_test_avro
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: query: DESCRIBE druid_kafka_test_avro
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_kafka_test_avro
__time              	timestamp           	from deserializer   
page                	string              	from deserializer   
user                	string              	from deserializer   
language            	string              	from deserializer   
country             	string              	from deserializer   
continent           	string              	from deserializer   
namespace           	string              	from deserializer   
newpage             	boolean             	from deserializer   
unpatrolled         	boolean             	from deserializer   
anonymous           	boolean             	from deserializer   
robot               	boolean             	from deserializer   
added               	int                 	from deserializer   
deleted             	int                 	from deserializer   
delta               	bigint              	from deserializer   
PREHOOK: query: DESCRIBE EXTENDED druid_kafka_test_avro
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: query: DESCRIBE EXTENDED druid_kafka_test_avro
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_kafka_test_avro
__time              	timestamp           	from deserializer   
page                	string              	from deserializer   
user                	string              	from deserializer   
language            	string              	from deserializer   
country             	string              	from deserializer   
continent           	string              	from deserializer   
namespace           	string              	from deserializer   
newpage             	boolean             	from deserializer   
unpatrolled         	boolean             	from deserializer   
anonymous           	boolean             	from deserializer   
robot               	boolean             	from deserializer   
added               	int                 	from deserializer   
deleted             	int                 	from deserializer   
delta               	bigint              	from deserializer   
	 	 
#### A masked pattern was here ####
StorageHandlerInfo	 	 
Druid Storage Handler Runtime Status for default.druid_kafka_test_avro	 	 
kafkaPartitions=1	 	 
activeTasks=[]	 	 
publishingTasks=[]	 	 
#### A masked pattern was here ####
aggregateLag=0	 	 
#### A masked pattern was here ####
PREHOOK: query: Select count(*) FROM druid_kafka_test_avro
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_kafka_test_avro
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: Select count(*) FROM druid_kafka_test_avro
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: Output: hdfs://### HDFS PATH ###
11
PREHOOK: query: Select page FROM druid_kafka_test_avro
PREHOOK: type: QUERY
PREHOOK: Input: default@druid_kafka_test_avro
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: Select page FROM druid_kafka_test_avro
POSTHOOK: type: QUERY
POSTHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: Output: hdfs://### HDFS PATH ###
page is 0
page is 100
page is 200
page is 300
page is 400
page is 500
page is 600
page is 700
page is 800
page is 900
page is 1000
PREHOOK: query: DROP TABLE druid_kafka_test_avro
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@druid_kafka_test_avro
PREHOOK: Output: default@druid_kafka_test_avro
POSTHOOK: query: DROP TABLE druid_kafka_test_avro
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@druid_kafka_test_avro
POSTHOOK: Output: default@druid_kafka_test_avro
