PREHOOK: query: create table src2 as select key, count(1) as count from src group by key
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@src2
POSTHOOK: query: create table src2 as select key, count(1) as count from src group by key
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src2
POSTHOOK: Lineage: src2.count EXPRESSION [(src)src.null, ]
POSTHOOK: Lineage: src2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: create table src3 as select * from src2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2
PREHOOK: Output: database:default
PREHOOK: Output: default@src3
POSTHOOK: query: create table src3 as select * from src2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src3
POSTHOOK: Lineage: src3.count SIMPLE [(src2)src2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src3.key SIMPLE [(src2)src2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: create table src4 as select * from src2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2
PREHOOK: Output: database:default
PREHOOK: Output: default@src4
POSTHOOK: query: create table src4 as select * from src2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src4
POSTHOOK: Lineage: src4.count SIMPLE [(src2)src2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src4.key SIMPLE [(src2)src2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: create table src5 as select * from src2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2
PREHOOK: Output: database:default
PREHOOK: Output: default@src5
POSTHOOK: query: create table src5 as select * from src2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src5
POSTHOOK: Lineage: src5.count SIMPLE [(src2)src2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src5.key SIMPLE [(src2)src2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
PREHOOK: type: QUERY
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-5 is a root stage
  Stage-2 depends on stages: Stage-5
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src5
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: count()
                keys: key (type: string)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col1 (type: bigint)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src5
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src5
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src5 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src5
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src5 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src5
            name: default.src5
      Truncated Path -> Alias:
        /src5 [null-subquery2:$hdt$_2-subquery2:src5]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 51 Data size: 244 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types string,bigint
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src2
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            alias: src3
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            alias: src4
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            GatherStats: false
            Union
              Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                Statistics: Num rows: 360 Data size: 1726 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      columns _col0,_col1
                      columns.types string:bigint
                      escape.delim \
                      hive.serialization.extend.additional.nesting.levels true
                      serialization.escape.crlf true
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types string,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types string,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: src2
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src2
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src2 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src2
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src2 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src2
            name: default.src2
#### A masked pattern was here ####
          Partition
            base file name: src3
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src3
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src3 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src3
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src3 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src3
            name: default.src3
#### A masked pattern was here ####
          Partition
            base file name: src4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src4
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src4 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src4
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src4 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src4
            name: default.src4
      Truncated Path -> Alias:
        /src2 [null-subquery1-subquery1-subquery1:$hdt$_2-subquery1-subquery1-subquery1:src2]
        /src3 [null-subquery1-subquery1-subquery2:$hdt$_2-subquery1-subquery1-subquery2:src3]
        /src4 [null-subquery1-subquery2:$hdt$_2-subquery1-subquery2:src4]
#### A masked pattern was here ####

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5
#### A masked pattern was here ####
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5
#### A masked pattern was here ####
0	1
0	3
0	3
0	3
2	1
2	1
2	1
2	1
4	1
4	1
4	1
4	1
5	1
5	3
5	3
5	3
8	1
8	1
8	1
8	1
9	1
9	1
9	1
9	1
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
PREHOOK: type: QUERY
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-4 is a root stage
  Stage-2 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-4
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                  tag: 0
                  auto parallelism: false
          TableScan
            alias: b
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                  tag: 1
                  value expressions: _col1 (type: bigint)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src4
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src4 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src4
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src4 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src4
            name: default.src4
#### A masked pattern was here ####
          Partition
            base file name: src5
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src5
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src5 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src5
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src5 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src5
            name: default.src5
      Truncated Path -> Alias:
        /src4 [null-subquery2:$hdt$_1-subquery2:$hdt$_1:a]
        /src5 [null-subquery2:$hdt$_1-subquery2:$hdt$_2:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: string)
            1 _col0 (type: string)
          outputColumnNames: _col0, _col2
          Statistics: Num rows: 113 Data size: 543 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col2 (type: bigint)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 113 Data size: 543 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    column.name.delimiter ,
                    columns _col0,_col1
                    columns.types string,bigint
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src2
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            alias: src3
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            GatherStats: false
            Union
              Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                Statistics: Num rows: 319 Data size: 1531 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      columns _col0,_col1
                      columns.types string:bigint
                      escape.delim \
                      hive.serialization.extend.additional.nesting.levels true
                      serialization.escape.crlf true
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types string,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types string,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: src2
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src2
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src2 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src2
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src2 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src2
            name: default.src2
#### A masked pattern was here ####
          Partition
            base file name: src3
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src3
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src3 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src3
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src3 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src3
            name: default.src3
      Truncated Path -> Alias:
        /src2 [null-subquery1-subquery1:$hdt$_1-subquery1-subquery1:src2]
        /src3 [null-subquery1-subquery2:$hdt$_1-subquery1-subquery2:src3]
#### A masked pattern was here ####

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5
#### A masked pattern was here ####
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5
#### A masked pattern was here ####
0	3
0	3
0	3
2	1
2	1
2	1
4	1
4	1
4	1
5	3
5	3
5	3
8	1
8	1
8	1
9	1
9	1
9	1
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
PREHOOK: type: QUERY
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-4 is a root stage
  Stage-5 depends on stages: Stage-4
  Stage-2 depends on stages: Stage-5
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-4
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                  tag: 0
                  auto parallelism: false
          TableScan
            alias: b
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                  tag: 1
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src4
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src4 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src4
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src4 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src4
            name: default.src4
#### A masked pattern was here ####
          Partition
            base file name: src5
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src5
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src5 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src5
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src5 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src5
            name: default.src5
      Truncated Path -> Alias:
        /src4 [null-subquery2:$hdt$_1-subquery2:$hdt$_1:a]
        /src5 [null-subquery2:$hdt$_1-subquery2:$hdt$_2:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: string)
            1 _col0 (type: string)
          outputColumnNames: _col0
          Statistics: Num rows: 113 Data size: 543 Basic stats: COMPLETE Column stats: NONE
          Group By Operator
            aggregations: count()
            keys: _col0 (type: string)
            mode: hash
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 113 Data size: 543 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    column.name.delimiter ,
                    columns _col0,_col1
                    columns.types string,bigint
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: string)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col0 (type: string)
              Statistics: Num rows: 113 Data size: 543 Basic stats: COMPLETE Column stats: NONE
              tag: -1
              value expressions: _col1 (type: bigint)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types string,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types string,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 56 Data size: 269 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1
                  columns.types string,bigint
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src2
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            alias: src3
            Statistics: Num rows: 309 Data size: 1482 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
              Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), count (type: bigint)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 494 Basic stats: COMPLETE Column stats: NONE
                Union
                  Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1
                          columns.types string:bigint
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
          TableScan
            GatherStats: false
            Union
              Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                Statistics: Num rows: 262 Data size: 1257 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      columns _col0,_col1
                      columns.types string:bigint
                      escape.delim \
                      hive.serialization.extend.additional.nesting.levels true
                      serialization.escape.crlf true
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10005
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1
              columns.types string,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1
                columns.types string,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: src2
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src2
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src2 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src2
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src2 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src2
            name: default.src2
#### A masked pattern was here ####
          Partition
            base file name: src3
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
              bucket_count -1
              column.name.delimiter ,
              columns key,count
              columns.comments 
              columns.types string:bigint
#### A masked pattern was here ####
              name default.src3
              numFiles 1
              numRows 309
              rawDataSize 1482
              serialization.ddl struct src3 { string key, i64 count}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 1791
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns key,count
                columns.comments 
                columns.types string:bigint
#### A masked pattern was here ####
                name default.src3
                numFiles 1
                numRows 309
                rawDataSize 1482
                serialization.ddl struct src3 { string key, i64 count}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 1791
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src3
            name: default.src3
      Truncated Path -> Alias:
        /src2 [null-subquery1-subquery1:$hdt$_1-subquery1-subquery1:src2]
        /src3 [null-subquery1-subquery2:$hdt$_1-subquery1-subquery2:src3]
#### A masked pattern was here ####

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5
#### A masked pattern was here ####
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5
#### A masked pattern was here ####
0	1
0	3
0	3
2	1
2	1
2	1
4	1
4	1
4	1
5	1
5	3
5	3
8	1
8	1
8	1
9	1
9	1
9	1
