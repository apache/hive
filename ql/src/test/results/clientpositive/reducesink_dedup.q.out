PREHOOK: query: select p_name
from (select p_name from part distribute by 1 sort by 1) p 
distribute by 1 sort by 1
PREHOOK: type: QUERY
PREHOOK: Input: default@part
#### A masked pattern was here ####
POSTHOOK: query: select p_name
from (select p_name from part distribute by 1 sort by 1) p 
distribute by 1 sort by 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part
#### A masked pattern was here ####
almond antique burnished rose metallic
almond antique burnished rose metallic
almond antique chartreuse lavender yellow
almond antique salmon chartreuse burlywood
almond aquamarine burnished black steel
almond aquamarine pink moccasin thistle
almond antique violet chocolate turquoise
almond antique violet turquoise frosted
almond aquamarine midnight light salmon
almond aquamarine rose maroon antique
almond aquamarine sandy cyan gainsboro
almond antique chartreuse khaki white
almond antique forest lavender goldenrod
almond antique metallic orange dim
almond antique misty red olive
almond antique olive coral navajo
almond antique gainsboro frosted violet
almond antique violet mint lemon
almond aquamarine floral ivory bisque
almond aquamarine yellow dodger mint
almond azure aquamarine papaya violet
almond antique blue firebrick mint
almond antique medium spring khaki
almond antique sky peru orange
almond aquamarine dodger light gainsboro
almond azure blanched chiffon midnight
PREHOOK: query: create temporary table d1 (key int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@d1
POSTHOOK: query: create temporary table d1 (key int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@d1
PREHOOK: query: create temporary table d2 (key int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@d2
POSTHOOK: query: create temporary table d2 (key int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@d2
PREHOOK: query: explain from (select key from src cluster by key) a
  insert overwrite table d1 select a.key
  insert overwrite table d2 select a.key cluster by a.key
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@d1
PREHOOK: Output: default@d2
POSTHOOK: query: explain from (select key from src cluster by key) a
  insert overwrite table d1 select a.key
  insert overwrite table d2 select a.key cluster by a.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@d1
POSTHOOK: Output: default@d2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0, Stage-4, Stage-6
  Stage-4 depends on stages: Stage-2
  Stage-5 depends on stages: Stage-1, Stage-4, Stage-6
  Stage-1 depends on stages: Stage-2
  Stage-6 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: key (type: string)
              outputColumnNames: _col0
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                key expressions: _col0 (type: string)
                sort order: +
                Map-reduce partition columns: _col0 (type: string)
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
      Execution mode: vectorized
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string)
          outputColumnNames: _col0
          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: UDFToInteger(_col0) (type: int)
            outputColumnNames: _col0
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  name: default.d1
            Select Operator
              expressions: _col0 (type: int)
              outputColumnNames: key
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: compute_stats(key, 'hll')
                mode: hash
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 424 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          Select Operator
            expressions: UDFToInteger(_col0) (type: int)
            outputColumnNames: _col0
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  name: default.d2
            Select Operator
              expressions: _col0 (type: int)
              outputColumnNames: key
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: compute_stats(key, 'hll')
                mode: hash
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 424 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.d1

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key
          Column Types: int
          Table: default.d1

  Stage: Stage-4
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              sort order: 
              Statistics: Num rows: 1 Data size: 424 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>)
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          aggregations: compute_stats(VALUE._col0)
          mode: mergepartial
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 440 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 440 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key
          Column Types: int
          Table: default.d2

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.d2

  Stage: Stage-6
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              sort order: 
              Statistics: Num rows: 1 Data size: 424 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>)
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          aggregations: compute_stats(VALUE._col0)
          mode: mergepartial
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 440 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 440 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

