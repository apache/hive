PREHOOK: query: create table a_n4 as SELECT 100 as key, a_n4.value as value FROM src LATERAL VIEW explode(array(40, 50, 60)) a_n4 as value limit 3
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@a_n4
POSTHOOK: query: create table a_n4 as SELECT 100 as key, a_n4.value as value FROM src LATERAL VIEW explode(array(40, 50, 60)) a_n4 as value limit 3
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@a_n4
POSTHOOK: Lineage: a_n4.key SIMPLE []
POSTHOOK: Lineage: a_n4.value SCRIPT []
PREHOOK: query: explain extended select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: explain extended select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT `t`.`key`, `t`.`value`, `t1`.`key` AS `key1`, `t1`.`value` AS `value1`, `t3`.`key` AS `key2`, `t3`.`value` AS `value2`
FROM (SELECT `key`, `value`, `value` = 60 AS `=`, `value` = 50 AS `=3`
FROM `default`.`a_n4`) AS `t`
LEFT JOIN (SELECT `key`, CAST(50 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 50 AND `key` IS NOT NULL) AS `t1` ON `t`.`key` = `t1`.`key` AND `t`.`=3`
LEFT JOIN (SELECT `key`, CAST(60 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 60 AND `key` IS NOT NULL) AS `t3` ON `t`.`key` = `t3`.`key` AND `t`.`=`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a_n4
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 60) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 0
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
                auto parallelism: false
          TableScan
            alias: b
            filterExpr: ((value = 50) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 50) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 50 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_0:a_n4, $hdt$_1:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col2}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col4, _col5
          Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col4,_col5
                  columns.types int,int,boolean,int,int
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col1 (type: int), _col2 (type: boolean), _col4 (type: int), _col5 (type: int)
              auto parallelism: false
          TableScan
            alias: c
            filterExpr: ((value = 60) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 60) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 60 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col4,_col5
              columns.types int,int,boolean,int,int
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col4,_col5
                columns.types int,int,boolean,int,int
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_2:c]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col1}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col4, _col5, _col6, _col7
          Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: int), _col4 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: int)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3,_col4,_col5
                    columns.types int:int:int:int:int:int
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	40	NULL	NULL	NULL	NULL
100	50	100	50	NULL	NULL
100	60	NULL	NULL	100	60
PREHOOK: query: select /*+ MAPJOIN(b,c)*/ * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select /*+ MAPJOIN(b,c)*/ * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	40	NULL	NULL	NULL	NULL
100	50	100	50	NULL	NULL
100	60	NULL	NULL	100	60
PREHOOK: query: explain extended select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: explain extended select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT `t0`.`key`, `t0`.`value`, `t1`.`key` AS `key1`, `t1`.`value` AS `value1`, `t3`.`key` AS `key2`, `t3`.`value` AS `value2`
FROM (SELECT `key`, CAST(50 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 50 AND `key` IS NOT NULL) AS `t0`
RIGHT JOIN (SELECT `key`, `value`, `value` = 60 AS `=`, `value` = 50 AS `=3`
FROM `default`.`a_n4`) AS `t1` ON `t0`.`key` = `t1`.`key` AND `t1`.`=3`
LEFT JOIN (SELECT `key`, CAST(60 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 60 AND `key` IS NOT NULL) AS `t3` ON `t1`.`key` = `t3`.`key` AND `t1`.`=`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a_n4
            filterExpr: ((value = 50) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 50) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 50 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 0
                  value expressions: _col1 (type: int)
                  auto parallelism: false
          TableScan
            alias: b
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 60) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 1
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_0:a_n4, $hdt$_1:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Right Outer Join 0 to 1
          filter mappings:
            1 [0, 1]
          filter predicates:
            0 
            1 {VALUE._col2}
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3,_col4
                  columns.types int,int,int,int,boolean
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col2 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col2 (type: int)
              Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col0 (type: int), _col1 (type: int), _col3 (type: int), _col4 (type: boolean)
              auto parallelism: false
          TableScan
            alias: c
            filterExpr: ((value = 60) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 60) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 60 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4
              columns.types int,int,int,int,boolean
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4
                columns.types int,int,int,int,boolean
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_2:c]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col3}
            1 
          keys:
            0 _col2 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col6, _col7
          Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col6 (type: int), _col7 (type: int)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3,_col4,_col5
                    columns.types int:int:int:int:int:int
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	50	100	50	NULL	NULL
NULL	NULL	100	40	NULL	NULL
NULL	NULL	100	60	100	60
PREHOOK: query: select /*+ MAPJOIN(a_n4,c)*/ * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select /*+ MAPJOIN(a_n4,c)*/ * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	50	100	50	NULL	NULL
NULL	NULL	100	40	NULL	NULL
NULL	NULL	100	60	100	60
PREHOOK: query: explain extended select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: explain extended select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT `t0`.`key`, `t0`.`value`, `t1`.`key` AS `key1`, `t1`.`value` AS `value1`, `t3`.`key` AS `key2`, `t3`.`value` AS `value2`
FROM (SELECT `key`, CAST(50 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 50 AND `key` IS NOT NULL) AS `t0`
RIGHT JOIN (SELECT `key`, `value`, `value` = 60 AS `=`, `value` = 50 AS `=3`
FROM `default`.`a_n4`) AS `t1` ON `t0`.`key` = `t1`.`key` AND `t1`.`=3`
LEFT JOIN (SELECT `key`, CAST(60 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 60 AND `key` IS NOT NULL) AS `t3` ON `t1`.`key` = `t3`.`key` AND `t1`.`=`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a_n4
            filterExpr: ((value = 50) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 50) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 50 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 0
                  value expressions: _col1 (type: int)
                  auto parallelism: false
          TableScan
            alias: b
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 60) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 1
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_0:a_n4, $hdt$_1:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Right Outer Join 0 to 1
          filter mappings:
            1 [0, 1]
          filter predicates:
            0 
            1 {VALUE._col2}
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3,_col4
                  columns.types int,int,int,int,boolean
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col2 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col2 (type: int)
              Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col0 (type: int), _col1 (type: int), _col3 (type: int), _col4 (type: boolean)
              auto parallelism: false
          TableScan
            alias: c
            filterExpr: ((value = 60) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 60) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 60 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col4
              columns.types int,int,int,int,boolean
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col4
                columns.types int,int,int,int,boolean
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_2:c]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col3}
            1 
          keys:
            0 _col2 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col6, _col7
          Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: int), _col6 (type: int), _col7 (type: int)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3,_col4,_col5
                    columns.types int:int:int:int:int:int
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	50	100	50	NULL	NULL
NULL	NULL	100	40	NULL	NULL
NULL	NULL	100	60	100	60
PREHOOK: query: select /*+ MAPJOIN(a_n4,c)*/ * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select /*+ MAPJOIN(a_n4,c)*/ * from a_n4 right outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50 AND b.value>10) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	50	100	50	NULL	NULL
NULL	NULL	100	40	NULL	NULL
NULL	NULL	100	60	100	60
PREHOOK: query: explain extended select * from a_n4 full outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: explain extended select * from a_n4 full outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT `t`.`key`, `t`.`value`, `t0`.`key` AS `key1`, `t0`.`value` AS `value1`, `t2`.`key` AS `key2`, `t2`.`value` AS `value2`, `t4`.`key` AS `key3`, `t4`.`value` AS `value3`
FROM (SELECT `key`, `value`, `value` = 40 AS `=`, `value` = 50 AS `=3`
FROM `default`.`a_n4`) AS `t`
FULL JOIN (SELECT `key`, `value`, `value` = 60 AS `=`, `value` = 50 AS `=3`
FROM `default`.`a_n4`) AS `t0` ON `t`.`key` = `t0`.`key` AND `t`.`=3` AND `t0`.`=3`
LEFT JOIN (SELECT `key`, CAST(60 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 60 AND `key` IS NOT NULL) AS `t2` ON `t0`.`key` = `t2`.`key` AND `t0`.`=`
LEFT JOIN (SELECT `key`, CAST(40 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 40 AND `key` IS NOT NULL) AS `t4` ON `t`.`key` = `t4`.`key` AND `t`.`=`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a_n4
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 40) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 0
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
                auto parallelism: false
          TableScan
            alias: b
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 60) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 1
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_0:a_n4, $hdt$_1:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Full Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
            1 [0, 1]
          filter predicates:
            0 {VALUE._col2}
            1 {VALUE._col2}
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col6
          Statistics: Num rows: 9 Data size: 216 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col4,_col5,_col6
                  columns.types int,int,boolean,int,int,boolean
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col4 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col4 (type: int)
              Statistics: Num rows: 9 Data size: 216 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: boolean), _col5 (type: int), _col6 (type: boolean)
              auto parallelism: false
          TableScan
            alias: c
            filterExpr: ((value = 60) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 60) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 60 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col4,_col5,_col6
              columns.types int,int,boolean,int,int,boolean
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col4,_col5,_col6
                columns.types int,int,boolean,int,int,boolean
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_2:c]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col5}
            1 
          keys:
            0 _col4 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col4, _col5, _col8, _col9
          Statistics: Num rows: 9 Data size: 252 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col4,_col5,_col8,_col9
                  columns.types int,int,boolean,int,int,int,int
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 9 Data size: 252 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col1 (type: int), _col2 (type: boolean), _col4 (type: int), _col5 (type: int), _col8 (type: int), _col9 (type: int)
              auto parallelism: false
          TableScan
            alias: d
            filterExpr: ((value = 40) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 40) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 40 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10005
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col4,_col5,_col8,_col9
              columns.types int,int,boolean,int,int,int,int
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col4,_col5,_col8,_col9
                columns.types int,int,boolean,int,int,int,int
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_3:d]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col1}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col4, _col5, _col8, _col9, _col10, _col11
          Statistics: Num rows: 9 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: int), _col4 (type: int), _col5 (type: int), _col8 (type: int), _col9 (type: int), _col10 (type: int), _col11 (type: int)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
            Statistics: Num rows: 9 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 9 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7
                    columns.types int:int:int:int:int:int:int:int
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select * from a_n4 full outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select * from a_n4 full outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	40	NULL	NULL	NULL	NULL	100	40
100	50	100	50	NULL	NULL	NULL	NULL
100	60	NULL	NULL	NULL	NULL	NULL	NULL
NULL	NULL	100	40	NULL	NULL	NULL	NULL
NULL	NULL	100	60	100	60	NULL	NULL
PREHOOK: query: explain extended select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: explain extended select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT `t`.`key`, `t`.`value`, `t1`.`key` AS `key1`, `t1`.`value` AS `value1`, `t3`.`key` AS `key2`, `t3`.`value` AS `value2`, `t5`.`key` AS `key3`, `t5`.`value` AS `value3`
FROM (SELECT `key`, `value`, `value` = 40 AS `=`, `value` = 60 AS `=3`, `value` = 50 AS `=4`
FROM `default`.`a_n4`) AS `t`
LEFT JOIN (SELECT `key`, CAST(50 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 50 AND `key` IS NOT NULL) AS `t1` ON `t`.`key` = `t1`.`key` AND `t`.`=4`
LEFT JOIN (SELECT `key`, CAST(60 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 60 AND `key` IS NOT NULL) AS `t3` ON `t`.`key` = `t3`.`key` AND `t`.`=3`
LEFT JOIN (SELECT `key`, CAST(40 AS INTEGER) AS `value`
FROM `default`.`a_n4`
WHERE `value` = 40 AND `key` IS NOT NULL) AS `t5` ON `t`.`key` = `t5`.`key` AND `t`.`=`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: a_n4
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: key (type: int), value (type: int), (value = 40) (type: boolean), (value = 60) (type: boolean), (value = 50) (type: boolean)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4
              Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
              Reduce Output Operator
                key expressions: _col0 (type: int)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col0 (type: int)
                Statistics: Num rows: 3 Data size: 60 Basic stats: COMPLETE Column stats: COMPLETE
                tag: 0
                value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean), _col4 (type: boolean)
                auto parallelism: false
          TableScan
            alias: b
            filterExpr: ((value = 50) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 50) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 50 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_0:a_n4, $hdt$_1:b]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col3}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col5, _col6
          Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col3,_col5,_col6
                  columns.types int,int,boolean,boolean,int,int
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 3 Data size: 72 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col1 (type: int), _col2 (type: boolean), _col3 (type: boolean), _col5 (type: int), _col6 (type: int)
              auto parallelism: false
          TableScan
            alias: c
            filterExpr: ((value = 60) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 60) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 60 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10004
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3,_col5,_col6
              columns.types int,int,boolean,boolean,int,int
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3,_col5,_col6
                columns.types int,int,boolean,boolean,int,int
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_2:c]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col2}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col5, _col6, _col7, _col8
          Statistics: Num rows: 3 Data size: 84 Basic stats: COMPLETE Column stats: COMPLETE
          File Output Operator
            compressed: false
            GlobalTableId: 0
#### A masked pattern was here ####
            NumFilesPerFileSink: 1
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                properties:
                  column.name.delimiter ,
                  columns _col0,_col1,_col2,_col5,_col6,_col7,_col8
                  columns.types int,int,boolean,int,int,int,int
                  escape.delim \
                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            TotalFiles: 1
            GatherStats: false
            MultiFileSpray: false

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int)
              null sort order: a
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 3 Data size: 84 Basic stats: COMPLETE Column stats: COMPLETE
              tag: 0
              value expressions: _col1 (type: int), _col2 (type: boolean), _col5 (type: int), _col6 (type: int), _col7 (type: int), _col8 (type: int)
              auto parallelism: false
          TableScan
            alias: d
            filterExpr: ((value = 40) and key is not null) (type: boolean)
            Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: ((value = 40) and key is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
              Select Operator
                expressions: key (type: int), 40 (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  tag: 1
                  value expressions: _col1 (type: int)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10005
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col5,_col6,_col7,_col8
              columns.types int,int,boolean,int,int,int,int
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col5,_col6,_col7,_col8
                columns.types int,int,boolean,int,int,int,int
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
#### A masked pattern was here ####
          Partition
            base file name: a_n4
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types int:int
#### A masked pattern was here ####
              name default.a_n4
              numFiles 1
              numRows 3
              rawDataSize 18
              serialization.ddl struct a_n4 { i32 key, i32 value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 21
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types int:int
#### A masked pattern was here ####
                name default.a_n4
                numFiles 1
                numRows 3
                rawDataSize 18
                serialization.ddl struct a_n4 { i32 key, i32 value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 21
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.a_n4
            name: default.a_n4
      Truncated Path -> Alias:
        /a_n4 [$hdt$_3:d]
#### A masked pattern was here ####
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join 0 to 1
          filter mappings:
            0 [1, 1]
          filter predicates:
            0 {VALUE._col1}
            1 
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col1, _col5, _col6, _col7, _col8, _col9, _col10
          Statistics: Num rows: 3 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: int), _col8 (type: int), _col9 (type: int), _col10 (type: int)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
            Statistics: Num rows: 3 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 3 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7
                    columns.types int:int:int:int:int:int:int:int
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	40	NULL	NULL	NULL	NULL	100	40
100	50	100	50	NULL	NULL	NULL	NULL
100	60	NULL	NULL	100	60	NULL	NULL
PREHOOK: query: select /*+ MAPJOIN(b,c, d)*/ * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
PREHOOK: type: QUERY
PREHOOK: Input: default@a_n4
#### A masked pattern was here ####
POSTHOOK: query: select /*+ MAPJOIN(b,c, d)*/ * from a_n4 left outer join a_n4 b on (a_n4.key=b.key AND a_n4.value=50 AND b.value=50) left outer join a_n4 c on (a_n4.key=c.key AND a_n4.value=60 AND c.value=60) left outer join a_n4 d on (a_n4.key=d.key AND a_n4.value=40 AND d.value=40)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@a_n4
#### A masked pattern was here ####
100	40	NULL	NULL	NULL	NULL	100	40
100	50	100	50	NULL	NULL	NULL	NULL
100	60	NULL	NULL	100	60	NULL	NULL
