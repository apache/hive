PREHOOK: query: DESCRIBE FUNCTION explode
PREHOOK: type: DESCFUNCTION
POSTHOOK: query: DESCRIBE FUNCTION explode
POSTHOOK: type: DESCFUNCTION
explode(a) - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns 
PREHOOK: query: DESCRIBE FUNCTION EXTENDED explode
PREHOOK: type: DESCFUNCTION
POSTHOOK: query: DESCRIBE FUNCTION EXTENDED explode
POSTHOOK: type: DESCFUNCTION
explode(a) - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns 
PREHOOK: query: EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_TABREF
         TOK_TABNAME
            src
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               explode
               TOK_FUNCTION
                  array
                  1
                  2
                  3
            myCol
      TOK_LIMIT
         3


STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 3
      Processor Tree:
        TableScan
          alias: src
          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
          GatherStats: false
          Select Operator
            expressions: array(1,2,3) (type: array<int>)
            outputColumnNames: _col0
            Statistics: Num rows: 500 Data size: 28000 Basic stats: COMPLETE Column stats: COMPLETE
            UDTF Operator
              Statistics: Num rows: 500 Data size: 28000 Basic stats: COMPLETE Column stats: COMPLETE
              function name: explode
              Limit
                Number of rows: 3
                Statistics: Num rows: 3 Data size: 168 Basic stats: COMPLETE Column stats: COMPLETE
                ListSink

PREHOOK: query: EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_SUBQUERY
         TOK_QUERY
            TOK_FROM
               TOK_TABREF
                  TOK_TABNAME
                     src
            TOK_INSERT
               TOK_DESTINATION
                  TOK_DIR
                     TOK_TMP_FILE
               TOK_SELECT
                  TOK_SELEXPR
                     TOK_FUNCTION
                        explode
                        TOK_FUNCTION
                           array
                           1
                           2
                           3
                     myCol
               TOK_LIMIT
                  3
         a
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            .
               TOK_TABLE_OR_COL
                  a
               myCol
         TOK_SELEXPR
            TOK_FUNCTION
               count
               1
      TOK_GROUPBY
         .
            TOK_TABLE_OR_COL
               a
            myCol


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: array(1,2,3) (type: array<int>)
              outputColumnNames: _col0
              Statistics: Num rows: 500 Data size: 28000 Basic stats: COMPLETE Column stats: COMPLETE
              UDTF Operator
                Statistics: Num rows: 500 Data size: 28000 Basic stats: COMPLETE Column stats: COMPLETE
                function name: explode
                Limit
                  Number of rows: 3
                  Statistics: Num rows: 3 Data size: 168 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    sort order: 
                    Statistics: Num rows: 3 Data size: 168 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    value expressions: col (type: int)
                    auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 'default','default'
              columns.types string:string
#### A masked pattern was here ####
              name default.src
              numFiles 1
              numRows 500
              rawDataSize 5312
              serialization.ddl struct src { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 5812
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 'default','default'
                columns.types string:string
#### A masked pattern was here ####
                name default.src
                numFiles 1
                numRows 500
                rawDataSize 5312
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 5812
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src
            name: default.src
      Truncated Path -> Alias:
        /src [a:src]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: int)
          outputColumnNames: _col0
          Statistics: Num rows: 3 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
          Limit
            Number of rows: 3
            Statistics: Num rows: 3 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
            Group By Operator
              aggregations: count(1)
              keys: _col0 (type: int)
              mode: hash
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      columns _col0,_col1
                      columns.types int,bigint
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
              tag: -1
              value expressions: _col1 (type: bigint)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1
              columns.types int,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1
                columns.types int,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: int)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: bigint)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types int:bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
1
2
3
PREHOOK: query: SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
1
2
3
PREHOOK: query: SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
1	1
2	1
3	1
PREHOOK: query: EXPLAIN SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: 3
      Processor Tree:
        TableScan
          alias: src
          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: map(1:'one',2:'two',3:'three') (type: map<int,string>)
            outputColumnNames: _col0
            Statistics: Num rows: 500 Data size: 259500 Basic stats: COMPLETE Column stats: COMPLETE
            UDTF Operator
              Statistics: Num rows: 500 Data size: 259500 Basic stats: COMPLETE Column stats: COMPLETE
              function name: explode
              Limit
                Number of rows: 3
                Statistics: Num rows: 3 Data size: 1557 Basic stats: COMPLETE Column stats: COMPLETE
                ListSink

PREHOOK: query: EXPLAIN EXTENDED SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  
TOK_QUERY
   TOK_FROM
      TOK_SUBQUERY
         TOK_QUERY
            TOK_FROM
               TOK_TABREF
                  TOK_TABNAME
                     src
            TOK_INSERT
               TOK_DESTINATION
                  TOK_DIR
                     TOK_TMP_FILE
               TOK_SELECT
                  TOK_SELEXPR
                     TOK_FUNCTION
                        explode
                        TOK_FUNCTION
                           map
                           1
                           'one'
                           2
                           'two'
                           3
                           'three'
                     myKey
                     myVal
               TOK_LIMIT
                  3
         a
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            .
               TOK_TABLE_OR_COL
                  a
               myKey
         TOK_SELEXPR
            .
               TOK_TABLE_OR_COL
                  a
               myVal
         TOK_SELEXPR
            TOK_FUNCTION
               count
               1
      TOK_GROUPBY
         .
            TOK_TABLE_OR_COL
               a
            myKey
         .
            TOK_TABLE_OR_COL
               a
            myVal


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
            GatherStats: false
            Select Operator
              expressions: map(1:'one',2:'two',3:'three') (type: map<int,string>)
              outputColumnNames: _col0
              Statistics: Num rows: 500 Data size: 259500 Basic stats: COMPLETE Column stats: COMPLETE
              UDTF Operator
                Statistics: Num rows: 500 Data size: 259500 Basic stats: COMPLETE Column stats: COMPLETE
                function name: explode
                Limit
                  Number of rows: 3
                  Statistics: Num rows: 3 Data size: 1557 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    sort order: 
                    Statistics: Num rows: 3 Data size: 1557 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    value expressions: key (type: int), value (type: string)
                    auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: src
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.comments 'default','default'
              columns.types string:string
#### A masked pattern was here ####
              name default.src
              numFiles 1
              numRows 500
              rawDataSize 5312
              serialization.ddl struct src { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 5812
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE true
                bucket_count -1
                columns key,value
                columns.comments 'default','default'
                columns.types string:string
#### A masked pattern was here ####
                name default.src
                numFiles 1
                numRows 500
                rawDataSize 5312
                serialization.ddl struct src { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 5812
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.src
            name: default.src
      Truncated Path -> Alias:
        /src [a:src]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: int), VALUE._col1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 3 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
          Limit
            Number of rows: 3
            Statistics: Num rows: 3 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
            Group By Operator
              aggregations: count(1)
              keys: _col0 (type: int), _col1 (type: string)
              mode: hash
              outputColumnNames: _col0, _col1, _col2
              Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    properties:
                      columns _col0,_col1,_col2
                      columns.types int,string,bigint
                      escape.delim \
                      serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: _col0 (type: int), _col1 (type: string)
              sort order: ++
              Map-reduce partition columns: _col0 (type: int), _col1 (type: string)
              Statistics: Num rows: 3 Data size: 24 Basic stats: COMPLETE Column stats: COMPLETE
              tag: -1
              value expressions: _col2 (type: bigint)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              columns _col0,_col1,_col2
              columns.types int,string,bigint
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                columns _col0,_col1,_col2
                columns.types int,string,bigint
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: int), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
          Select Operator
            expressions: _col0 (type: int), _col1 (type: string), _col2 (type: bigint)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0,_col1,_col2
                    columns.types int:string:bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
1	one
2	two
3	three
PREHOOK: query: SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
1	one	1
2	two	1
3	three	1
PREHOOK: query: SELECT src.key, myCol FROM src lateral view explode(array(1,2,3)) x AS myCol LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT src.key, myCol FROM src lateral view explode(array(1,2,3)) x AS myCol LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
238	1
238	2
238	3
PREHOOK: query: SELECT src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
238	1	one
238	2	two
238	3	three
PREHOOK: query: -- HIVE-4295
SELECT BLOCK__OFFSET__INSIDE__FILE, src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3
PREHOOK: type: QUERY
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: -- HIVE-4295
SELECT BLOCK__OFFSET__INSIDE__FILE, src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
#### A masked pattern was here ####
0	238	1	one
0	238	2	two
0	238	3	three
