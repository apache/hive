PREHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_1
POSTHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_1
PREHOOK: query: DESCRIBE FORMATTED druid_table_1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_table_1
POSTHOOK: query: DESCRIBE FORMATTED druid_table_1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_table_1
# col_name            	data_type           	comment             
	 	 
__time              	timestamp           	from deserializer   
robot               	string              	from deserializer   
namespace           	string              	from deserializer   
anonymous           	string              	from deserializer   
unpatrolled         	string              	from deserializer   
page                	string              	from deserializer   
language            	string              	from deserializer   
newpage             	string              	from deserializer   
user                	string              	from deserializer   
count               	float               	from deserializer   
added               	float               	from deserializer   
delta               	float               	from deserializer   
variation           	float               	from deserializer   
deleted             	float               	from deserializer   
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"__time\":\"true\",\"added\":\"true\",\"anonymous\":\"true\",\"count\":\"true\",\"deleted\":\"true\",\"delta\":\"true\",\"language\":\"true\",\"namespace\":\"true\",\"newpage\":\"true\",\"page\":\"true\",\"robot\":\"true\",\"unpatrolled\":\"true\",\"user\":\"true\",\"variation\":\"true\"}}
	EXTERNAL            	TRUE                
	druid.datasource    	wikipedia           
	numFiles            	0                   
	numRows             	0                   
	rawDataSize         	0                   
	storage_handler     	org.apache.hadoop.hive.druid.QTestDruidStorageHandler
	totalSize           	0                   
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.druid.QTestDruidSerDe	 
InputFormat:        	null                	 
OutputFormat:       	null                	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["robot"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
            druid.query.type select
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":[],"metrics":["delta"],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
            druid.query.type select
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: delta (type: float)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"filter":{"type":"selector","dimension":"language","value":"en"},"dimensions":["robot"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
            druid.query.type select
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot"}],"limitSpec":{"type":"default"},"filter":{"type":"selector","dimension":"language","value":"en"},"aggregations":[{"type":"longSum","name":"dummy_agg","fieldName":"dummy_agg"}],"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1) a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1) a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            filterExpr: language is not null (type: boolean)
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["robot","language"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: language is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: robot (type: string), language (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col1 (type: string)
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  tag: 0
                  value expressions: _col0 (type: string)
                  auto parallelism: false
          TableScan
            alias: druid_table_1
            filterExpr: language is not null (type: boolean)
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["language"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: language is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: language (type: string)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: a
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  tag: 1
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["robot","language"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
                druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["language"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                druid.query.type select
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [$hdt$_0:druid_table_1, $hdt$_1:druid_table_1]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col1 (type: string)
            1 _col0 (type: string)
          outputColumnNames: _col0, _col2
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col2 (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Warning: Shuffle Join JOIN[5][tables = [druid_table_1, $hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product
PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"filter":{"type":"selector","dimension":"language","value":"en"},"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
            GatherStats: false
            Select Operator
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
              Reduce Output Operator
                null sort order: 
                sort order: 
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                tag: 1
                auto parallelism: false
          TableScan
            alias: druid_table_1
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"filter":{"type":"selector","dimension":"language","value":"en"},"dimensions":["robot"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Reduce Output Operator
              null sort order: 
              sort order: 
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              tag: 0
              value expressions: robot (type: string)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"filter":{"type":"selector","dimension":"language","value":"en"},"dimensions":[],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
                druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"filter":{"type":"selector","dimension":"language","value":"en"},"dimensions":["robot"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
                druid.query.type select
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [$hdt$_0:druid_table_1, druid_table_1]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 
            1 
          outputColumnNames: _col1
          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: string), 'en' (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            properties:
              druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot"},{"type":"default","dimension":"language"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":"day","timeZone":"UTC","locale":"en-US"}}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleMax","name":"$f3","fieldName":"added"},{"type":"doubleSum","name":"$f4","fieldName":"delta"}],"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"]}
              druid.query.type groupBy
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: robot (type: string), floor_day (type: timestamp), $f3 (type: float), $f4 (type: float), UDFToInteger(robot) (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col5
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Reduce Output Operator
                key expressions: _col5 (type: int), _col2 (type: float)
                null sort order: az
                sort order: +-
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                tag: -1
                TopN: 10
                TopN Hash Memory Usage: 0.1
                value expressions: _col0 (type: string), _col1 (type: timestamp), _col3 (type: float)
                auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
              druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot"},{"type":"default","dimension":"language"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":"day","timeZone":"UTC","locale":"en-US"}}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleMax","name":"$f3","fieldName":"added"},{"type":"doubleSum","name":"$f4","fieldName":"delta"}],"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"]}
              druid.query.type groupBy
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
                druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot"},{"type":"default","dimension":"language"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":"day","timeZone":"UTC","locale":"en-US"}}],"limitSpec":{"type":"default"},"aggregations":[{"type":"doubleMax","name":"$f3","fieldName":"added"},{"type":"doubleSum","name":"$f4","fieldName":"delta"}],"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"]}
                druid.query.type groupBy
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [$hdt$_0:druid_table_1]
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: timestamp), KEY.reducesinkkey1 (type: float), VALUE._col2 (type: float)
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Limit
            Number of rows: 10
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3
                    columns.types string:timestamp:float:float
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["namespace"],"metrics":["deleted"],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: substring(namespace, UDFToInteger(deleted), 4) (type: string)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              File Output Operator
                compressed: false
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            filterExpr: floor_day(__time) BETWEEN 1999-11-01 00:00:00.0 AND 1999-11-10 00:00:00.0 (type: boolean)
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"],"dimensions":["robot"],"metrics":[],"granularity":"all","pagingSpec":{"threshold":16384,"fromNext":true},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: floor_day(__time) BETWEEN 1999-11-01 00:00:00.0 AND 1999-11-10 00:00:00.0 (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: robot (type: string), floor_day(__time) (type: timestamp)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: timestamp)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: timestamp)
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: timestamp)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                    TopN Hash Memory Usage: 0.1
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: string), KEY._col1 (type: timestamp)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: string)
              sort order: +
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              TopN Hash Memory Usage: 0.1
              value expressions: _col1 (type: timestamp)
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: timestamp)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Limit
            Number of rows: 10
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            filterExpr: floor_day(extract) BETWEEN 1999-11-01 00:00:00.0 AND 1999-11-10 00:00:00.0 (type: boolean)
            properties:
              druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"UTC"}},{"type":"default","dimension":"robot"}],"limitSpec":{"type":"default"},"aggregations":[{"type":"longSum","name":"dummy_agg","fieldName":"dummy_agg"}],"intervals":["1900-01-01T00:00:00.000/3000-01-01T00:00:00.000"]}
              druid.query.type groupBy
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: floor_day(extract) BETWEEN 1999-11-01 00:00:00.0 AND 1999-11-10 00:00:00.0 (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: robot (type: string), extract (type: timestamp)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  TopN Hash Memory Usage: 0.1
                  value expressions: _col1 (type: timestamp)
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: timestamp)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Limit
            Number of rows: 10
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":"day","timeZone":"UTC","locale":"en-US"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"alphanumeric"}]},"aggregations":[{"type":"longSum","name":"dummy_agg","fieldName":"dummy_agg"}],"intervals":["1999-11-01T00:00:00.000/1999-11-10T00:00:00.001"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: __time (type: timestamp), robot (type: string), language (type: string), added (type: float), delta (type: float)
              outputColumnNames: __time, robot, language, added, delta
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Group By Operator
                aggregations: max(added), sum(delta)
                keys: robot (type: string), language (type: string), floor_day(__time) (type: timestamp)
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3, _col4
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp)
                  null sort order: aaa
                  sort order: +++
                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp)
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  tag: -1
                  value expressions: _col3 (type: float), _col4 (type: double)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [druid_table_1]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), sum(VALUE._col1)
          keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: timestamp)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col2 (type: timestamp), _col3 (type: float), _col4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    column.name.delimiter ,
                    columns _col0,_col1,_col2,_col3
                    columns.types string,timestamp,float,double
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: UDFToInteger(_col0) (type: int), _col2 (type: float)
              null sort order: az
              sort order: +-
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              tag: -1
              TopN: 10
              TopN Hash Memory Usage: 0.1
              value expressions: _col0 (type: string), _col1 (type: timestamp), _col3 (type: double)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types string,timestamp,float,double
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types string,timestamp,float,double
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: timestamp), KEY.reducesinkkey1 (type: float), VALUE._col2 (type: double)
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Limit
            Number of rows: 10
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3
                    columns.types string:timestamp:float:double
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

