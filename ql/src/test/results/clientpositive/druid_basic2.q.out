PREHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_1
POSTHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_1
PREHOOK: query: DESCRIBE FORMATTED druid_table_1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_table_1
POSTHOOK: query: DESCRIBE FORMATTED druid_table_1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_table_1
# col_name            	data_type           	comment             
__time              	timestamp with local time zone	from deserializer   
robot               	string              	from deserializer   
namespace           	string              	from deserializer   
anonymous           	string              	from deserializer   
unpatrolled         	string              	from deserializer   
page                	string              	from deserializer   
language            	string              	from deserializer   
newpage             	string              	from deserializer   
user                	string              	from deserializer   
count               	float               	from deserializer   
added               	float               	from deserializer   
delta               	float               	from deserializer   
variation           	float               	from deserializer   
deleted             	float               	from deserializer   
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"__time\":\"true\",\"added\":\"true\",\"anonymous\":\"true\",\"count\":\"true\",\"deleted\":\"true\",\"delta\":\"true\",\"language\":\"true\",\"namespace\":\"true\",\"newpage\":\"true\",\"page\":\"true\",\"robot\":\"true\",\"unpatrolled\":\"true\",\"user\":\"true\",\"variation\":\"true\"}}
	EXTERNAL            	TRUE                
	bucketing_version   	2                   
	druid.datasource    	wikipedia           
	numFiles            	0                   
	numRows             	0                   
	rawDataSize         	0                   
	storage_handler     	org.apache.hadoop.hive.druid.QTestDruidStorageHandler
	totalSize           	0                   
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.druid.QTestDruidSerDe	 
InputFormat:        	null                	 
OutputFormat:       	null                	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["robot"],"resultFormat":"compactedList"}
            druid.query.type scan
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT delta FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames delta
            druid.fieldTypes float
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"columns":["delta"],"resultFormat":"compactedList"}
            druid.query.type scan
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: delta (type: float)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot
FROM druid_table_1
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
            druid.query.type scan
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1
WHERE language = 'en'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT DISTINCT robot
FROM druid_table_1
WHERE language = 'en'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot
            druid.fieldTypes string
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"}],"limitSpec":{"type":"default"},"filter":{"type":"selector","dimension":"language","value":"en"},"aggregations":[],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1) a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1) a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            properties:
              druid.fieldNames robot,language
              druid.fieldTypes string,string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["robot","language"],"resultFormat":"compactedList"}
              druid.query.type scan
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: robot (type: string), language (type: string)
              outputColumnNames: _col0, _col1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Reduce Output Operator
                key expressions: _col1 (type: string)
                null sort order: a
                sort order: +
                Map-reduce partition columns: _col1 (type: string)
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                tag: 0
                value expressions: _col0 (type: string)
                auto parallelism: false
          TableScan
            alias: druid_table_1
            properties:
              druid.fieldNames language
              druid.fieldTypes string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["language"],"resultFormat":"compactedList"}
              druid.query.type scan
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Reduce Output Operator
              key expressions: language (type: string)
              null sort order: a
              sort order: +
              Map-reduce partition columns: language (type: string)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              tag: 1
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
              druid.fieldNames robot,language
              druid.fieldTypes string,string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["robot","language"],"resultFormat":"compactedList"}
              druid.query.type scan
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
                druid.fieldNames language
                druid.fieldTypes string
                druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"not","field":{"type":"selector","dimension":"language","value":null}},"columns":["language"],"resultFormat":"compactedList"}
                druid.query.type scan
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [$hdt$_0:druid_table_1, druid_table_1]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col1 (type: string)
            1 language (type: string)
          outputColumnNames: _col0, _col2
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col2 (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Warning: Shuffle Join JOIN[5][tables = [$hdt$_0, druid_table_1]] in Stage 'Stage-1:MAPRED' is a cross product
PREHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT a.robot, b.language
FROM
(
  (SELECT robot, language
  FROM druid_table_1
  WHERE language = 'en') a
  JOIN
  (SELECT language
  FROM druid_table_1) b
  ON a.language = b.language
)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            properties:
              druid.fieldNames robot
              druid.fieldTypes string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
              druid.query.type scan
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: robot (type: string)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Reduce Output Operator
                null sort order: 
                sort order: 
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                tag: 0
                value expressions: _col0 (type: string)
                auto parallelism: false
          TableScan
            alias: druid_table_1
            properties:
              druid.fieldNames vc
              druid.fieldTypes string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"virtualColumns":[{"type":"expression","name":"vc","expression":"'en'","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
              druid.query.type scan
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
            GatherStats: false
            Reduce Output Operator
              null sort order: 
              sort order: 
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
              tag: 1
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
              druid.fieldNames robot
              druid.fieldTypes string
              druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"columns":["robot"],"resultFormat":"compactedList"}
              druid.query.type scan
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
                druid.fieldNames vc
                druid.fieldTypes string
                druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"filter":{"type":"selector","dimension":"language","value":"en"},"virtualColumns":[{"type":"expression","name":"vc","expression":"'en'","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
                druid.query.type scan
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [$hdt$_0:druid_table_1, druid_table_1]
      Needs Tagging: true
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 
            1 
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 1 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), 'en' (type: string)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 1 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 1 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1
                    columns.types string:string
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot,floor_day,$f3,$f4,(tok_function tok_int (tok_table_or_col robot))
            druid.fieldTypes string,timestamp with local time zone,float,double,int
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"default","dimension":"language","outputName":"language","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"US/Pacific","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"(tok_function tok_int (tok_table_or_col robot))","direction":"ascending","dimensionOrder":"numeric"},{"dimension":"$f3","direction":"descending","dimensionOrder":"numeric"}]},"aggregations":[{"type":"doubleMax","name":"$f3","fieldName":"added"},{"type":"doubleSum","name":"$f4","fieldName":"delta"}],"postAggregations":[{"type":"expression","name":"(tok_function tok_int (tok_table_or_col robot))","expression":"CAST(\"robot\", 'LONG')"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          GatherStats: false
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone), $f3 (type: float), $f4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT substring(namespace, CAST(deleted AS INT), 4)
FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames vc
            druid.fieldTypes string
            druid.query.json {"queryType":"scan","dataSource":"wikipedia","intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"virtualColumns":[{"type":"expression","name":"vc","expression":"substring(\"namespace\", (CAST(\"deleted\", 'LONG') - 1), 4)","outputType":"STRING"}],"columns":["vc"],"resultFormat":"compactedList"}
            druid.query.type scan
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: vc (type: string)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot,floor_day
            druid.fieldTypes string,timestamp with local time zone
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"US/Pacific","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, `__time`
FROM druid_table_1
WHERE floor_day(`__time`) BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, `__time`
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames extract,robot
            druid.fieldTypes timestamp with local time zone,string
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"extraction","dimension":"__time","outputName":"extract","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","timeZone":"US/Pacific"}},{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: robot (type: string), extract (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT robot, floor_day(`__time`)
FROM druid_table_1
WHERE `__time` BETWEEN '1999-11-01 00:00:00' AND '1999-11-10 00:00:00'
GROUP BY robot, floor_day(`__time`)
ORDER BY robot
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.fieldNames robot,floor_day
            druid.fieldTypes string,timestamp with local time zone
            druid.query.json {"queryType":"groupBy","dataSource":"wikipedia","granularity":"all","dimensions":[{"type":"default","dimension":"robot","outputName":"robot","outputType":"STRING"},{"type":"extraction","dimension":"__time","outputName":"floor_day","extractionFn":{"type":"timeFormat","format":"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","granularity":{"type":"period","period":"P1D","timeZone":"US/Pacific"},"timeZone":"US/Pacific","locale":"und"}}],"limitSpec":{"type":"default","limit":10,"columns":[{"dimension":"robot","direction":"ascending","dimensionOrder":"lexicographic"}]},"aggregations":[],"intervals":["1999-11-01T08:00:00.000Z/1999-11-10T08:00:00.001Z"]}
            druid.query.type groupBy
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: robot (type: string), floor_day (type: timestamp with local time zone)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED
SELECT robot, floor_day(`__time`), max(added) as m, sum(delta) as s
FROM druid_table_1
GROUP BY robot, language, floor_day(`__time`)
ORDER BY CAST(robot AS INTEGER) ASC, m DESC
LIMIT 10
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            GatherStats: false
            Select Operator
              expressions: __time (type: timestamp with local time zone), robot (type: string), language (type: string), added (type: float), delta (type: float)
              outputColumnNames: __time, robot, language, added, delta
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Group By Operator
                aggregations: max(added), sum(delta)
                keys: robot (type: string), language (type: string), floor_day(__time) (type: timestamp with local time zone)
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3, _col4
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp with local time zone)
                  null sort order: aaa
                  sort order: +++
                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: timestamp with local time zone)
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  tag: -1
                  value expressions: _col3 (type: float), _col4 (type: double)
                  auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: druid_table_1
            input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
            output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
            properties:
              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
              EXTERNAL TRUE
              bucket_count -1
              bucketing_version 2
              column.name.delimiter ,
              columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
              columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
              columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
              druid.datasource wikipedia
#### A masked pattern was here ####
              name default.druid_table_1
              numFiles 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
              storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
              totalSize 0
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
          
              input format: org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat
              output format: org.apache.hadoop.hive.druid.io.DruidOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"__time":"true","added":"true","anonymous":"true","count":"true","deleted":"true","delta":"true","language":"true","namespace":"true","newpage":"true","page":"true","robot":"true","unpatrolled":"true","user":"true","variation":"true"}}
                EXTERNAL TRUE
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns __time,robot,namespace,anonymous,unpatrolled,page,language,newpage,user,count,added,delta,variation,deleted
                columns.comments 'from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer','from deserializer'
                columns.types timestamp with local time zone:string:string:string:string:string:string:string:string:float:float:float:float:float
                druid.datasource wikipedia
#### A masked pattern was here ####
                name default.druid_table_1
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct druid_table_1 { timestamp with local time zone __time, string robot, string namespace, string anonymous, string unpatrolled, string page, string language, string newpage, string user, float count, float added, float delta, float variation, float deleted}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.druid.QTestDruidSerDe
                storage_handler org.apache.hadoop.hive.druid.QTestDruidStorageHandler
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.druid.QTestDruidSerDe
              name: default.druid_table_1
            name: default.druid_table_1
      Truncated Path -> Alias:
        /druid_table_1 [druid_table_1]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), sum(VALUE._col1)
          keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: timestamp with local time zone)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col2 (type: timestamp with local time zone), _col3 (type: float), _col4 (type: double)
            outputColumnNames: _col0, _col1, _col2, _col3
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    column.name.delimiter ,
                    columns _col0,_col1,_col2,_col3
                    columns.types string,timestamp with local time zone,float,double
                    escape.delim \
                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            GatherStats: false
            Reduce Output Operator
              key expressions: UDFToInteger(_col0) (type: int), _col2 (type: float)
              null sort order: az
              sort order: +-
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              tag: -1
              TopN: 10
              TopN Hash Memory Usage: 0.1
              value expressions: _col0 (type: string), _col1 (type: timestamp with local time zone), _col3 (type: double)
              auto parallelism: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: -mr-10003
            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
            properties:
              column.name.delimiter ,
              columns _col0,_col1,_col2,_col3
              columns.types string,timestamp with local time zone,float,double
              escape.delim \
              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          
              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
              properties:
                column.name.delimiter ,
                columns _col0,_col1,_col2,_col3
                columns.types string,timestamp with local time zone,float,double
                escape.delim \
                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
      Truncated Path -> Alias:
#### A masked pattern was here ####
      Needs Tagging: false
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), VALUE._col1 (type: timestamp with local time zone), KEY.reducesinkkey1 (type: float), VALUE._col2 (type: double)
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Limit
            Number of rows: 10
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  properties:
                    columns _col0,_col1,_col2,_col3
                    columns.types string:timestamp with local time zone:float:double
                    escape.delim \
                    hive.serialization.extend.additional.nesting.levels true
                    serialization.escape.crlf true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

