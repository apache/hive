PREHOOK: query: drop table if exists customer_address
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists customer_address
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table customer_address
(
    ca_address_sk             int,
    ca_address_id             string,
    ca_street_number          string,
    ca_street_name            string,
    ca_street_type            string,
    ca_suite_number           string,
    ca_city                   string,
    ca_county                 string,
    ca_state                  string,
    ca_zip                    string,
    ca_country                string,
    ca_gmt_offset             decimal(5,2),
    ca_location_type          string
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@customer_address
POSTHOOK: query: create table customer_address
(
    ca_address_sk             int,
    ca_address_id             string,
    ca_street_number          string,
    ca_street_name            string,
    ca_street_type            string,
    ca_suite_number           string,
    ca_city                   string,
    ca_county                 string,
    ca_state                  string,
    ca_zip                    string,
    ca_country                string,
    ca_gmt_offset             decimal(5,2),
    ca_location_type          string
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@customer_address
PREHOOK: query: drop table if exists date_dim
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists date_dim
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table date_dim
(
    d_date_sk                 int,
    d_date_id                 string,
    d_date                    string,
    d_month_seq               int,
    d_week_seq                int,
    d_quarter_seq             int,
    d_year                    int,
    d_dow                     int,
    d_moy                     int,
    d_dom                     int,
    d_qoy                     int,
    d_fy_year                 int,
    d_fy_quarter_seq          int,
    d_fy_week_seq             int,
    d_day_name                string,
    d_quarter_name            string,
    d_holiday                 string,
    d_weekend                 string,
    d_following_holiday       string,
    d_first_dom               int,
    d_last_dom                int,
    d_same_day_ly             int,
    d_same_day_lq             int,
    d_current_day             string,
    d_current_week            string,
    d_current_month           string,
    d_current_quarter         string,
    d_current_year            string
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@date_dim
POSTHOOK: query: create table date_dim
(
    d_date_sk                 int,
    d_date_id                 string,
    d_date                    string,
    d_month_seq               int,
    d_week_seq                int,
    d_quarter_seq             int,
    d_year                    int,
    d_dow                     int,
    d_moy                     int,
    d_dom                     int,
    d_qoy                     int,
    d_fy_year                 int,
    d_fy_quarter_seq          int,
    d_fy_week_seq             int,
    d_day_name                string,
    d_quarter_name            string,
    d_holiday                 string,
    d_weekend                 string,
    d_following_holiday       string,
    d_first_dom               int,
    d_last_dom                int,
    d_same_day_ly             int,
    d_same_day_lq             int,
    d_current_day             string,
    d_current_week            string,
    d_current_month           string,
    d_current_quarter         string,
    d_current_year            string
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@date_dim
PREHOOK: query: drop table if exists web_returns
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists web_returns
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table web_returns
(
    wr_returned_date_sk        int,
    wr_returned_time_sk        int,
    wr_item_sk                 int,
    wr_refunded_customer_sk    int,
    wr_refunded_cdemo_sk       int,
    wr_refunded_hdemo_sk       int,
    wr_refunded_addr_sk        int,
    wr_returning_customer_sk   int,
    wr_returning_cdemo_sk      int,
    wr_returning_hdemo_sk      int,
    wr_returning_addr_sk       int,
    wr_web_page_sk             int,
    wr_reason_sk               int,
    wr_order_number            int,
    wr_return_quantity         int,
    wr_return_amt              decimal(7,2),
    wr_return_tax              decimal(7,2),
    wr_return_amt_inc_tax      decimal(7,2),
    wr_fee                     decimal(7,2),
    wr_return_ship_cost        decimal(7,2),
    wr_refunded_cash           decimal(7,2),
    wr_reversed_charge         decimal(7,2),
    wr_account_credit          decimal(7,2),
    wr_net_loss                decimal(7,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@web_returns
POSTHOOK: query: create table web_returns
(
    wr_returned_date_sk        int,
    wr_returned_time_sk        int,
    wr_item_sk                 int,
    wr_refunded_customer_sk    int,
    wr_refunded_cdemo_sk       int,
    wr_refunded_hdemo_sk       int,
    wr_refunded_addr_sk        int,
    wr_returning_customer_sk   int,
    wr_returning_cdemo_sk      int,
    wr_returning_hdemo_sk      int,
    wr_returning_addr_sk       int,
    wr_web_page_sk             int,
    wr_reason_sk               int,
    wr_order_number            int,
    wr_return_quantity         int,
    wr_return_amt              decimal(7,2),
    wr_return_tax              decimal(7,2),
    wr_return_amt_inc_tax      decimal(7,2),
    wr_fee                     decimal(7,2),
    wr_return_ship_cost        decimal(7,2),
    wr_refunded_cash           decimal(7,2),
    wr_reversed_charge         decimal(7,2),
    wr_account_credit          decimal(7,2),
    wr_net_loss                decimal(7,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@web_returns
PREHOOK: query: drop table if exists web_sales
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists web_sales
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table web_sales
(
    ws_sold_date_sk           int,
    ws_sold_time_sk           int,
    ws_ship_date_sk           int,
    ws_item_sk                int,
    ws_bill_customer_sk       int,
    ws_bill_cdemo_sk          int,
    ws_bill_hdemo_sk          int,
    ws_bill_addr_sk           int,
    ws_ship_customer_sk       int,
    ws_ship_cdemo_sk          int,
    ws_ship_hdemo_sk          int,
    ws_ship_addr_sk           int,
    ws_web_page_sk            int,
    ws_web_site_sk            int,
    ws_ship_mode_sk           int,
    ws_warehouse_sk           int,
    ws_promo_sk               int,
    ws_order_number           int,
    ws_quantity               int,
    ws_wholesale_cost         decimal(7,2),
    ws_list_price             decimal(7,2),
    ws_sales_price            decimal(7,2),
    ws_ext_discount_amt       decimal(7,2),
    ws_ext_sales_price        decimal(7,2),
    ws_ext_wholesale_cost     decimal(7,2),
    ws_ext_list_price         decimal(7,2),
    ws_ext_tax                decimal(7,2),
    ws_coupon_amt             decimal(7,2),
    ws_ext_ship_cost          decimal(7,2),
    ws_net_paid               decimal(7,2),
    ws_net_paid_inc_tax       decimal(7,2),
    ws_net_paid_inc_ship      decimal(7,2),
    ws_net_paid_inc_ship_tax  decimal(7,2),
    ws_net_profit             decimal(7,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@web_sales
POSTHOOK: query: create table web_sales
(
    ws_sold_date_sk           int,
    ws_sold_time_sk           int,
    ws_ship_date_sk           int,
    ws_item_sk                int,
    ws_bill_customer_sk       int,
    ws_bill_cdemo_sk          int,
    ws_bill_hdemo_sk          int,
    ws_bill_addr_sk           int,
    ws_ship_customer_sk       int,
    ws_ship_cdemo_sk          int,
    ws_ship_hdemo_sk          int,
    ws_ship_addr_sk           int,
    ws_web_page_sk            int,
    ws_web_site_sk            int,
    ws_ship_mode_sk           int,
    ws_warehouse_sk           int,
    ws_promo_sk               int,
    ws_order_number           int,
    ws_quantity               int,
    ws_wholesale_cost         decimal(7,2),
    ws_list_price             decimal(7,2),
    ws_sales_price            decimal(7,2),
    ws_ext_discount_amt       decimal(7,2),
    ws_ext_sales_price        decimal(7,2),
    ws_ext_wholesale_cost     decimal(7,2),
    ws_ext_list_price         decimal(7,2),
    ws_ext_tax                decimal(7,2),
    ws_coupon_amt             decimal(7,2),
    ws_ext_ship_cost          decimal(7,2),
    ws_net_paid               decimal(7,2),
    ws_net_paid_inc_tax       decimal(7,2),
    ws_net_paid_inc_ship      decimal(7,2),
    ws_net_paid_inc_ship_tax  decimal(7,2),
    ws_net_profit             decimal(7,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@web_sales
PREHOOK: query: drop table if exists web_site
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists web_site
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table web_site
(
    web_site_sk             int,
    web_site_id             string,
    web_rec_start_date      string,
    web_rec_end_date        string,
    web_name                string,
    web_open_date_sk        int,
    web_close_date_sk       int,
    web_class               string,
    web_manager             string,
    web_mkt_id              int,
    web_mkt_class           string,
    web_mkt_desc            string,
    web_market_manager      string,
    web_company_id          int,
    web_company_name        string,
    web_street_number       string,
    web_street_name         string,
    web_street_type         string,
    web_suite_number        string,
    web_city                string,
    web_county              string,
    web_state               string,
    web_zip                 string,
    web_country             string,
    web_gmt_offset          decimal(5,2),
    web_tax_percentage      decimal(5,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@web_site
POSTHOOK: query: create table web_site
(
    web_site_sk             int,
    web_site_id             string,
    web_rec_start_date      string,
    web_rec_end_date        string,
    web_name                string,
    web_open_date_sk        int,
    web_close_date_sk       int,
    web_class               string,
    web_manager             string,
    web_mkt_id              int,
    web_mkt_class           string,
    web_mkt_desc            string,
    web_market_manager      string,
    web_company_id          int,
    web_company_name        string,
    web_street_number       string,
    web_street_name         string,
    web_street_type         string,
    web_suite_number        string,
    web_city                string,
    web_county              string,
    web_state               string,
    web_zip                 string,
    web_country             string,
    web_gmt_offset          decimal(5,2),
    web_tax_percentage      decimal(5,2)
)
row format delimited fields terminated by '\t'
STORED AS ORC tblproperties ("transactional"="true", "orc.compress"="ZLIB")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@web_site
PREHOOK: query: explain
vectorization detail
with ws_wh as
(select ws1.ws_order_number,ws1.ws_warehouse_sk wh1,ws2.ws_warehouse_sk wh2
 from web_sales ws1,web_sales ws2
 where ws1.ws_order_number = ws2.ws_order_number
   and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)
 select
   count(distinct ws_order_number) as `order count`
  ,sum(ws_ext_ship_cost) as `total shipping cost`
  ,sum(ws_net_profit) as `total net profit`
from
   web_sales ws1
  ,date_dim
  ,customer_address
  ,web_site
where
    d_date between '1999-5-01' and
           (cast('1999-5-01' as date) + 60 days)
and ws1.ws_ship_date_sk = d_date_sk
and ws1.ws_ship_addr_sk = ca_address_sk
and ca_state = 'TX'
and ws1.ws_web_site_sk = web_site_sk
and web_company_name = 'pri'
and ws1.ws_order_number in (select ws_order_number
                            from ws_wh)
and ws1.ws_order_number in (select wr_order_number
                            from web_returns,ws_wh
                            where wr_order_number = ws_wh.ws_order_number)
order by count(distinct ws_order_number)
limit 100
PREHOOK: type: QUERY
PREHOOK: Input: default@customer_address
PREHOOK: Input: default@date_dim
PREHOOK: Input: default@web_returns
PREHOOK: Input: default@web_sales
PREHOOK: Input: default@web_site
#### A masked pattern was here ####
POSTHOOK: query: explain
vectorization detail
with ws_wh as
(select ws1.ws_order_number,ws1.ws_warehouse_sk wh1,ws2.ws_warehouse_sk wh2
 from web_sales ws1,web_sales ws2
 where ws1.ws_order_number = ws2.ws_order_number
   and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)
 select
   count(distinct ws_order_number) as `order count`
  ,sum(ws_ext_ship_cost) as `total shipping cost`
  ,sum(ws_net_profit) as `total net profit`
from
   web_sales ws1
  ,date_dim
  ,customer_address
  ,web_site
where
    d_date between '1999-5-01' and
           (cast('1999-5-01' as date) + 60 days)
and ws1.ws_ship_date_sk = d_date_sk
and ws1.ws_ship_addr_sk = ca_address_sk
and ca_state = 'TX'
and ws1.ws_web_site_sk = web_site_sk
and web_company_name = 'pri'
and ws1.ws_order_number in (select ws_order_number
                            from ws_wh)
and ws1.ws_order_number in (select wr_order_number
                            from web_returns,ws_wh
                            where wr_order_number = ws_wh.ws_order_number)
order by count(distinct ws_order_number)
limit 100
POSTHOOK: type: QUERY
POSTHOOK: Input: default@customer_address
POSTHOOK: Input: default@date_dim
POSTHOOK: Input: default@web_returns
POSTHOOK: Input: default@web_sales
POSTHOOK: Input: default@web_site
#### A masked pattern was here ####
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-2, Stage-11
  Stage-4 depends on stages: Stage-3, Stage-14
  Stage-5 depends on stages: Stage-4
  Stage-6 depends on stages: Stage-5
  Stage-7 depends on stages: Stage-6
  Stage-10 is a root stage
  Stage-11 depends on stages: Stage-10
  Stage-15 is a root stage
  Stage-13 depends on stages: Stage-15
  Stage-14 depends on stages: Stage-13
  Stage-0 depends on stages: Stage-7

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: ws1
            filterExpr: (ws_order_number is not null and ws_ship_date_sk is not null and ws_ship_addr_sk is not null and ws_web_site_sk is not null) (type: boolean)
            Statistics: Num rows: 1 Data size: 240 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (ws_order_number is not null and ws_ship_addr_sk is not null and ws_ship_date_sk is not null and ws_web_site_sk is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 240 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ws_ship_date_sk (type: int), ws_ship_addr_sk (type: int), ws_web_site_sk (type: int), ws_order_number (type: int), ws_ext_ship_cost (type: decimal(7,2)), ws_net_profit (type: decimal(7,2))
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 240 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 1 Data size: 240 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col2 (type: int), _col3 (type: int), _col4 (type: decimal(7,2)), _col5 (type: decimal(7,2))
          TableScan
            alias: customer_address
            filterExpr: ((ca_state = 'TX') and ca_address_sk is not null) (type: boolean)
            Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ((ca_state = 'TX') and ca_address_sk is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ca_address_sk (type: int)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col1 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col2, _col3, _col4, _col5
          Statistics: Num rows: 1 Data size: 264 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col2 (type: int)
              sort order: +
              Map-reduce partition columns: _col2 (type: int)
              Statistics: Num rows: 1 Data size: 264 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: int), _col3 (type: int), _col4 (type: decimal(7,2)), _col5 (type: decimal(7,2))
          TableScan
            alias: web_site
            filterExpr: ((web_company_name = 'pri') and web_site_sk is not null) (type: boolean)
            Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ((web_company_name = 'pri') and web_site_sk is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: web_site_sk (type: int)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col2 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col3, _col4, _col5
          Statistics: Num rows: 1 Data size: 290 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col3 (type: int)
              sort order: +
              Map-reduce partition columns: _col3 (type: int)
              Statistics: Num rows: 1 Data size: 290 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: int), _col4 (type: decimal(7,2)), _col5 (type: decimal(7,2))
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col3 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col3, _col4, _col5
          Statistics: Num rows: 1 Data size: 319 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-4
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col3 (type: int)
              sort order: +
              Map-reduce partition columns: _col3 (type: int)
              Statistics: Num rows: 1 Data size: 319 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: int), _col4 (type: decimal(7,2)), _col5 (type: decimal(7,2))
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col3 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col0, _col3, _col4, _col5
          Statistics: Num rows: 1 Data size: 350 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 1 Data size: 350 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col3 (type: int), _col4 (type: decimal(7,2)), _col5 (type: decimal(7,2))
          TableScan
            alias: date_dim
            filterExpr: (CAST( d_date AS TIMESTAMP) BETWEEN TIMESTAMP'1999-05-01 00:00:00' AND TIMESTAMP'1999-06-30 00:00:00' and d_date_sk is not null) (type: boolean)
            Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (CAST( d_date AS TIMESTAMP) BETWEEN TIMESTAMP'1999-05-01 00:00:00' AND TIMESTAMP'1999-06-30 00:00:00' and d_date_sk is not null) (type: boolean)
              Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: d_date_sk (type: int)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col3, _col4, _col5
          Statistics: Num rows: 1 Data size: 385 Basic stats: COMPLETE Column stats: NONE
          Group By Operator
            aggregations: sum(_col4), sum(_col5)
            keys: _col3 (type: int)
            mode: hash
            outputColumnNames: _col0, _col2, _col3
            Statistics: Num rows: 1 Data size: 385 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-6
    Map Reduce
      Map Operator Tree:
          TableScan
            TableScan Vectorization:
                native: true
                vectorizationSchemaColumns: [0:_col0:int, 1:_col2:decimal(17,2), 2:_col3:decimal(17,2)]
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Reduce Sink Vectorization:
                  className: VectorReduceSinkOperator
                  native: false
                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
              Statistics: Num rows: 1 Data size: 385 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col2 (type: decimal(17,2)), _col3 (type: decimal(17,2))
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 3
              includeColumns: [0, 1, 2]
              dataColumns: _col0:int, _col2:decimal(17,2), _col3:decimal(17,2)
              partitionColumnCount: 0
              scratchColumnTypeNames: []
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: sum(VALUE._col0), sum(VALUE._col1)
          keys: KEY._col0 (type: int)
          mode: partial2
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 385 Basic stats: COMPLETE Column stats: NONE
          Group By Operator
            aggregations: count(_col0), sum(_col1), sum(_col2)
            mode: partial2
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 344 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-7
    Map Reduce
      Map Operator Tree:
          TableScan
            TableScan Vectorization:
                native: true
                vectorizationSchemaColumns: [0:_col0:bigint, 1:_col1:decimal(17,2), 2:_col2:decimal(17,2)]
            Reduce Output Operator
              sort order: 
              Reduce Sink Vectorization:
                  className: VectorReduceSinkOperator
                  native: false
                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
              Statistics: Num rows: 1 Data size: 344 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: bigint), _col1 (type: decimal(17,2)), _col2 (type: decimal(17,2))
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 3
              includeColumns: [0, 1, 2]
              dataColumns: _col0:bigint, _col1:decimal(17,2), _col2:decimal(17,2)
              partitionColumnCount: 0
              scratchColumnTypeNames: []
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0), sum(VALUE._col1), sum(VALUE._col2)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 344 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 344 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-10
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: ws1
            filterExpr: ws_order_number is not null (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ws_order_number is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ws_warehouse_sk (type: int), ws_order_number (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int)
          TableScan
            alias: ws2
            filterExpr: ws_order_number is not null (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ws_order_number is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ws_warehouse_sk (type: int), ws_order_number (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int)
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col1 (type: int)
            1 _col1 (type: int)
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
          Filter Operator
            predicate: (_col0 <> _col2) (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: _col1 (type: int)
              outputColumnNames: _col1
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                keys: _col1 (type: int)
                mode: hash
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-11
    Map Reduce
      Map Operator Tree:
          TableScan
            TableScan Vectorization:
                native: true
                vectorizationSchemaColumns: [0:_col0:int]
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Reduce Sink Vectorization:
                  className: VectorReduceSinkOperator
                  native: false
                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 1
              includeColumns: [0]
              dataColumns: _col0:int
              partitionColumnCount: 0
              scratchColumnTypeNames: []
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: int)
          mode: mergepartial
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-15
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: ws1
            filterExpr: ws_order_number is not null (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ws_order_number is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ws_warehouse_sk (type: int), ws_order_number (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int)
          TableScan
            alias: ws2
            filterExpr: ws_order_number is not null (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ws_order_number is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: ws_warehouse_sk (type: int), ws_order_number (type: int)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col1 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col1 (type: int)
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int)
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col1 (type: int)
            1 _col1 (type: int)
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
          Filter Operator
            predicate: (_col0 <> _col2) (type: boolean)
            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: _col1 (type: int)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-13
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: web_returns
            filterExpr: wr_order_number is not null (type: boolean)
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: wr_order_number is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: wr_order_number (type: int)
                outputColumnNames: _col13
                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col13 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col13 (type: int)
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
      Map Vectorization:
          enabled: false
          enabledConditionsNotMet: Vectorized map work only works with 1 TableScanOperator IS false
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col13 (type: int)
            1 _col0 (type: int)
          outputColumnNames: _col13
          Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
          Group By Operator
            keys: _col13 (type: int)
            mode: hash
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-14
    Map Reduce
      Map Operator Tree:
          TableScan
            TableScan Vectorization:
                native: true
                vectorizationSchemaColumns: [0:_col0:int]
            Reduce Output Operator
              key expressions: _col0 (type: int)
              sort order: +
              Map-reduce partition columns: _col0 (type: int)
              Reduce Sink Vectorization:
                  className: VectorReduceSinkOperator
                  native: false
                  nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                  nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
          inputFormatFeatureSupport: []
          featureSupportInUse: []
          inputFileFormats: org.apache.hadoop.mapred.SequenceFileInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 1
              includeColumns: [0]
              dataColumns: _col0:int
              partitionColumnCount: 0
              scratchColumnTypeNames: []
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: int)
          mode: mergepartial
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

