PREHOOK: query: create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@vectortab2k
POSTHOOK: query: create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@vectortab2k
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@vectortab2k
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@vectortab2k
PREHOOK: query: create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@vectortab2korc
POSTHOOK: query: create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@vectortab2korc
PREHOOK: query: INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2k
PREHOOK: Output: default@vectortab2korc
POSTHOOK: query: INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2k
POSTHOOK: Output: default@vectortab2korc
POSTHOOK: Lineage: vectortab2korc.b SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:b, type:bigint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.bo SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:bo, type:boolean, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.d SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:d, type:double, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.dc SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:dc, type:decimal(38,18), comment:null), ]
POSTHOOK: Lineage: vectortab2korc.dt SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:dt, type:date, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.f SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:f, type:float, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.i SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:i, type:int, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.s SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:s, type:string, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.s2 SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:s2, type:string, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.si SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:si, type:smallint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.t SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:t, type:tinyint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.ts SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts, type:timestamp, comment:null), ]
POSTHOOK: Lineage: vectortab2korc.ts2 SIMPLE [(vectortab2k)vectortab2k.FieldSchema(name:ts2, type:timestamp, comment:null), ]
PREHOOK: query: explain vectorization detail
select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: vectortab2korc
            Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
            Select Operator
              expressions: dc (type: decimal(38,18))
              outputColumnNames: dc
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumns: [6]
              Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: min(dc), max(dc), sum(dc), avg(dc)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 6) -> decimal(38,18), VectorUDAFMaxDecimal(col 6) -> decimal(38,18), VectorUDAFSumDecimal(col 6) -> decimal(38,18), VectorUDAFAvgDecimal(col 6) -> struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>
                    className: VectorGroupByOperator
                    groupByMode: HASH
                    vectorOutput: true
                    native: false
                    vectorProcessingMode: HASH
                    projectedOutputColumns: [0, 1, 2, 3]
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Reduce Sink Vectorization:
                      className: VectorReduceSinkOperator
                      native: false
                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                  Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: decimal(38,18)), _col1 (type: decimal(38,18)), _col2 (type: decimal(38,18)), _col3 (type: struct<count:bigint,sum:decimal(38,18),input:decimal(38,18)>)
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          groupByVectorOutput: true
          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 13
              includeColumns: [6]
              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
              partitionColumnCount: 0
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
          Group By Vectorization:
              groupByMode: MERGEPARTIAL
              vectorOutput: false
              native: false
              vectorProcessingMode: NONE
              projectedOutputColumns: null
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 624 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
POSTHOOK: query: select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
-4997414117561.546875000000000000	4994550248722.298828000000000000	-10252745435816.024410000000000000	-5399023399.587163986308583465
PREHOOK: query: explain vectorization detail
select min(d), max(d), sum(d), avg(d) from vectortab2korc
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(d), max(d), sum(d), avg(d) from vectortab2korc
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: vectortab2korc
            Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
            Select Operator
              expressions: d (type: double)
              outputColumnNames: d
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumns: [5]
              Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: min(d), max(d), sum(d), avg(d)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDouble(col 5) -> double, VectorUDAFMaxDouble(col 5) -> double, VectorUDAFSumDouble(col 5) -> double, VectorUDAFAvgDouble(col 5) -> struct<count:bigint,sum:double,input:double>
                    className: VectorGroupByOperator
                    groupByMode: HASH
                    vectorOutput: true
                    native: false
                    vectorProcessingMode: HASH
                    projectedOutputColumns: [0, 1, 2, 3]
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Reduce Sink Vectorization:
                      className: VectorReduceSinkOperator
                      native: false
                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: double), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:double>)
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          groupByVectorOutput: true
          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 13
              includeColumns: [5]
              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
              partitionColumnCount: 0
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
          Group By Vectorization:
              groupByMode: MERGEPARTIAL
              vectorOutput: false
              native: false
              vectorProcessingMode: NONE
              projectedOutputColumns: null
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
POSTHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
-4999829.07	4997627.14	-1.7516847286999977E8	-92193.93308947356
PREHOOK: query: explain vectorization detail
select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: vectortab2korc
            Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
            TableScan Vectorization:
                native: true
                projectedOutputColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
            Select Operator
              expressions: ts (type: timestamp)
              outputColumnNames: ts
              Select Vectorization:
                  className: VectorSelectOperator
                  native: true
                  projectedOutputColumns: [10]
              Statistics: Num rows: 2000 Data size: 918712 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: min(ts), max(ts), sum(ts), avg(ts)
                Group By Vectorization:
                    aggregators: VectorUDAFMinTimestamp(col 10) -> timestamp, VectorUDAFMaxTimestamp(col 10) -> timestamp, VectorUDAFSumTimestamp(col 10) -> double, VectorUDAFAvgTimestamp(col 10) -> struct<count:bigint,sum:double,input:timestamp>
                    className: VectorGroupByOperator
                    groupByMode: HASH
                    vectorOutput: true
                    native: false
                    vectorProcessingMode: HASH
                    projectedOutputColumns: [0, 1, 2, 3]
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Reduce Sink Vectorization:
                      className: VectorReduceSinkOperator
                      native: false
                      nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                      nativeConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: timestamp), _col1 (type: timestamp), _col2 (type: double), _col3 (type: struct<count:bigint,sum:double,input:timestamp>)
      Execution mode: vectorized
      Map Vectorization:
          enabled: true
          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
          groupByVectorOutput: true
          inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
          allNative: false
          usesVectorUDFAdaptor: false
          vectorized: true
          rowBatchContext:
              dataColumnCount: 13
              includeColumns: [10]
              dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
              partitionColumnCount: 0
      Reduce Vectorization:
          enabled: false
          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true
          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false
      Reduce Operator Tree:
        Group By Operator
          aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), avg(VALUE._col3)
          Group By Vectorization:
              groupByMode: MERGEPARTIAL
              vectorOutput: false
              native: false
              vectorProcessingMode: NONE
              projectedOutputColumns: null
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3
          Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
POSTHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc
#### A masked pattern was here ####
2013-02-18 21:06:48	2081-02-22 01:21:53	4.591384881081E12	2.4254542425150557E9
