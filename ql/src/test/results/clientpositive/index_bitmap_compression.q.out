PREHOOK: query: CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD
PREHOOK: type: CREATEINDEX
PREHOOK: Input: default@src
POSTHOOK: query: CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD
POSTHOOK: type: CREATEINDEX
POSTHOOK: Input: default@src
POSTHOOK: Output: default@default__src_src_index__
PREHOOK: query: ALTER INDEX src_index ON src REBUILD
PREHOOK: type: ALTERINDEX_REBUILD
PREHOOK: Input: default@src
PREHOOK: Output: default@default__src_src_index__
POSTHOOK: query: ALTER INDEX src_index ON src REBUILD
POSTHOOK: type: ALTERINDEX_REBUILD
POSTHOOK: Input: default@src
POSTHOOK: Output: default@default__src_src_index__
POSTHOOK: Lineage: default__src_src_index__._bitmaps EXPRESSION [(src)src.FieldSchema(name:ROW__OFFSET__INSIDE__BLOCK, type:bigint, comment:), ]
POSTHOOK: Lineage: default__src_src_index__._bucketname SIMPLE [(src)src.FieldSchema(name:INPUT__FILE__NAME, type:string, comment:), ]
POSTHOOK: Lineage: default__src_src_index__._offset SIMPLE [(src)src.FieldSchema(name:BLOCK__OFFSET__INSIDE__FILE, type:bigint, comment:), ]
POSTHOOK: Lineage: default__src_src_index__.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-2 depends on stages: Stage-3
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: default__src_src_index__
            filterExpr: ((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
            Filter Operator
              predicate: ((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
              Select Operator
                expressions: _bucketname (type: string), _offset (type: bigint)
                outputColumnNames: _bucketname, _offset
                Group By Operator
                  aggregations: collect_set(_offset)
                  keys: _bucketname (type: string)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Reduce Output Operator
                    key expressions: _col0 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col0 (type: string)
                    value expressions: _col1 (type: array<bigint>)
      Reduce Operator Tree:
        Group By Operator
          aggregations: collect_set(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            filterExpr: ((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) (type: boolean)
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) (type: boolean)
              Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: key (type: string), value (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: true
                  Statistics: Num rows: 55 Data size: 584 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, value FROM src WHERE key > 80 AND key < 100
PREHOOK: type: QUERY
PREHOOK: Input: default@default__src_src_index__
PREHOOK: Input: default@src
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM src WHERE key > 80 AND key < 100
POSTHOOK: type: QUERY
POSTHOOK: Input: default@default__src_src_index__
POSTHOOK: Input: default@src
#### A masked pattern was here ####
82	val_82
83	val_83
83	val_83
84	val_84
84	val_84
85	val_85
86	val_86
87	val_87
90	val_90
90	val_90
90	val_90
92	val_92
95	val_95
95	val_95
96	val_96
97	val_97
97	val_97
98	val_98
98	val_98
PREHOOK: query: DROP INDEX src_index on src
PREHOOK: type: DROPINDEX
PREHOOK: Input: default@src
POSTHOOK: query: DROP INDEX src_index on src
POSTHOOK: type: DROPINDEX
POSTHOOK: Input: default@src
