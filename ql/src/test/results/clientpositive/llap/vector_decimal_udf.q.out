PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_n0
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_n0
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE DECIMAL_UDF_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF_txt
POSTHOOK: query: CREATE TABLE DECIMAL_UDF_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF_txt
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_udf_txt
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_udf_txt
PREHOOK: query: CREATE TABLE DECIMAL_UDF_n0 (key decimal(20,10), value int)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF_n0
POSTHOOK: query: CREATE TABLE DECIMAL_UDF_n0 (key decimal(20,10), value int)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF_n0
PREHOOK: query: INSERT OVERWRITE TABLE DECIMAL_UDF_n0 SELECT * FROM DECIMAL_UDF_txt
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt
PREHOOK: Output: default@decimal_udf_n0
POSTHOOK: query: INSERT OVERWRITE TABLE DECIMAL_UDF_n0 SELECT * FROM DECIMAL_UDF_txt
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt
POSTHOOK: Output: default@decimal_udf_n0
POSTHOOK: Lineage: decimal_udf_n0.key SIMPLE [(decimal_udf_txt)decimal_udf_txt.FieldSchema(name:key, type:decimal(20,10), comment:null), ]
POSTHOOK: Lineage: decimal_udf_n0.value SIMPLE [(decimal_udf_txt)decimal_udf_txt.FieldSchema(name:value, type:int, comment:null), ]
PREHOOK: query: insert into DECIMAL_UDF_n0 values (NULL, NULL)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@decimal_udf_n0
POSTHOOK: query: insert into DECIMAL_UDF_n0 values (NULL, NULL)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@decimal_udf_n0
POSTHOOK: Lineage: decimal_udf_n0.key EXPRESSION []
POSTHOOK: Lineage: decimal_udf_n0.value EXPRESSION []
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + key) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(21,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.6000000000
-0.6600000000
-0.6660000000
-2.2400000000
-2.2400000000
-2.2440000000
-2469135780.2469135780
-2510.9800000000
-8800.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0200000000
0.0400000000
0.2000000000
0.4000000000
0.6000000000
0.6600000000
0.6660000000
2.0000000000
2.0000000000
2.0000000000
2.2400000000
2.2440000000
20.0000000000
200.0000000000
2469135780.2469135600
248.0000000000
250.4000000000
4.0000000000
4.0000000000
40.0000000000
400.0000000000
6.2800000000
6.2800000000
6.2800000000
6.2800000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + CAST( value AS decimal(10,0))) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(21,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.3000000000
-0.3300000000
-0.3330000000
-12.1220000000
-2.1200000000
-2.1200000000
-2469135780.1234567890
-2510.4900000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0100000000
0.0200000000
0.1000000000
0.2000000000
0.3000000000
0.3300000000
0.3330000000
2.0000000000
2.0000000000
2.0000000000
2.1200000000
2.1220000000
20.0000000000
200.0000000000
2469135780.1234567800
248.0000000000
250.2000000000
4.0000000000
4.0000000000
40.0000000000
400.0000000000
6.1400000000
6.1400000000
6.1400000000
7.1400000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.3
-0.33
-0.333
-1.62
-1.62
-1.8518518351234567E9
-1882.99
-2200.0
-6.622
0.0
0.0
0.0
0.01
0.02
0.1
0.2
0.3
0.33
0.333
1.5
1.5
1.5
1.62
1.622
1.8518518351234567E9
15.0
150.0
186.0
187.7
3.0
3.0
30.0
300.0
4.640000000000001
4.640000000000001
4.640000000000001
5.140000000000001
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + 1.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.1200000000000001
-0.1200000000000001
-0.12200000000000011
-1.2345678891234567E9
-1254.49
-4399.0
0.667
0.6699999999999999
0.7
1.0
1.0
1.0
1.01
1.02
1.1
1.2
1.2345678911234567E9
1.3
1.33
1.333
101.0
11.0
125.0
126.2
2.0
2.0
2.0
2.12
2.122
201.0
21.0
3.0
3.0
4.140000000000001
4.140000000000001
4.140000000000001
4.140000000000001
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - key) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(21,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - CAST( value AS decimal(10,0))) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(21,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.1200000000
-0.1200000000
-0.1234567890
-0.3000000000
-0.3300000000
-0.3330000000
-0.4900000000
-0.8600000000
-8800.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0100000000
0.0200000000
0.1000000000
0.1200000000
0.1220000000
0.1234567800
0.1400000000
0.1400000000
0.1400000000
0.2000000000
0.2000000000
0.3000000000
0.3300000000
0.3330000000
9.8780000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.3
-0.33
-0.333
-0.6200000000000001
-0.6200000000000001
-6.172839451234567E8
-627.99
-6600.0
0.0
0.0
0.0
0.01
0.02
0.1
0.2
0.3
0.33
0.333
0.5
0.5
0.5
0.6200000000000001
0.6220000000000001
1.0
1.0
1.1400000000000001
1.6400000000000001
1.6400000000000001
1.6400000000000001
10.0
100.0
4.378
5.0
50.0
6.172839451234567E8
62.0
62.7
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - 1.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.667
-0.6699999999999999
-0.7
-0.8
-0.9
-0.98
-0.99
-1.0
-1.0
-1.0
-1.2345678911234567E9
-1.3
-1.33
-1.333
-1256.49
-2.12
-2.12
-2.122
-4401.0
0.0
0.0
0.0
0.1200000000000001
0.12200000000000011
1.0
1.0
1.2345678891234567E9
123.0
124.2
19.0
199.0
2.14
2.14
2.14
2.14
9.0
99.0
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * key) (type: decimal(38,17))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(38,17)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(38,17)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
0.00000000000000000
0.00000000000000000
0.00000000000000000
0.00010000000000000
0.00040000000000000
0.01000000000000000
0.04000000000000000
0.09000000000000000
0.09000000000000000
0.10890000000000000
0.10890000000000000
0.11088900000000000
0.11088900000000000
1.00000000000000000
1.00000000000000000
1.00000000000000000
1.25440000000000000
1.25440000000000000
1.25440000000000000
1.25888400000000000
1.25888400000000000
100.00000000000000000
10000.00000000000000000
1524157875323883652.79682997652796840
1524157875323883675.01905199875019052
15376.00000000000000000
15675.04000000000000000
1576255.14010000000000000
19360000.00000000000000000
4.00000000000000000
4.00000000000000000
400.00000000000000000
40000.00000000000000000
9.85960000000000000
9.85960000000000000
9.85960000000000000
9.85960000000000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_n0 where key * value > 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_n0 where key * value > 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColGreaterDecimalScalar(col 4:decimal(31,10), val 0)(children: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,10))
                    predicate: ((key * CAST( value AS decimal(10,0))) > 0) (type: boolean)
                    Statistics: Num rows: 13 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: decimal(20,10)), value (type: int)
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 13 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 13 Data size: 1508 Basic stats: COMPLETE Column stats: COMPLETE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, value FROM DECIMAL_UDF_n0 where key * value > 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM DECIMAL_UDF_n0 where key * value > 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1.1200000000	-1
-1.1200000000	-1
-1.1220000000	-11
-1234567890.1234567890	-1234567890
-1255.4900000000	-1255
1.0000000000	1
1.0000000000	1
1.0000000000	1
1.1200000000	1
1.1220000000	1
10.0000000000	10
100.0000000000	100
1234567890.1234567800	1234567890
124.0000000000	124
125.2000000000	125
2.0000000000	2
2.0000000000	2
20.0000000000	20
200.0000000000	200
3.1400000000	3
3.1400000000	3
3.1400000000	3
3.1400000000	4
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * CAST( value AS decimal(10,0))) (type: decimal(31,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * value FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * value FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-19360000.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
1.0000000000
1.0000000000
1.0000000000
1.1200000000
1.1200000000
1.1200000000
1.1220000000
100.0000000000
10000.0000000000
12.3420000000
12.5600000000
1524157875171467876.3907942000
1524157875171467887.5019052100
15376.0000000000
15650.0000000000
1575639.9500000000
4.0000000000
4.0000000000
400.0000000000
40000.0000000000
9.4200000000
9.4200000000
9.4200000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.0
-0.0
-0.0
-9680000.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.5
0.5
0.5
0.56
0.56
0.56
0.561
2.0
2.0
200.0
20000.0
4.71
4.71
4.71
50.0
5000.0
6.171
6.28
7.6207893758573389E17
7.6207893758573389E17
7688.0
7825.0
787819.975
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * 2.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.6
-0.66
-0.666
-2.24
-2.24
-2.244
-2.4691357802469134E9
-2510.98
-8800.0
0.0
0.0
0.0
0.02
0.04
0.2
0.4
0.6
0.66
0.666
2.0
2.0
2.0
2.24
2.244
2.4691357802469134E9
20.0
200.0
248.0
250.4
4.0
4.0
40.0
400.0
6.28
6.28
6.28
6.28
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key / 0) (type: decimal(22,12))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColDivideDecimalScalar(col 0:decimal(20,10), val 0) -> 3:decimal(22,12)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(22,12)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_n0 WHERE key is not null and key <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_n0 WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColNotEqualDecimalScalar(col 0:decimal(20,10), val 0)
                    predicate: (key <> 0) (type: boolean)
                    Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: (key / key) (type: decimal(38,18))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [3]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(38,18)
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(38,18)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / key FROM DECIMAL_UDF_n0 WHERE key is not null and key <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / key FROM DECIMAL_UDF_n0 WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_n0 WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_n0 WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: (key / CAST( value AS decimal(10,0))) (type: decimal(31,21))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,21)
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,21)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / value FROM DECIMAL_UDF_n0 WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / value FROM DECIMAL_UDF_n0 WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1.000000000000000000000
0.102000000000000000000
0.785000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000099999992710
1.000000000100000000000
1.000390438247011952191
1.001600000000000000000
1.046666666666666666667
1.046666666666666666667
1.046666666666666666667
1.120000000000000000000
1.120000000000000000000
1.120000000000000000000
1.122000000000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_n0  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_n0  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: (UDFToDouble(key) / (UDFToDouble(value) / 2.0D)) (type: double)
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DoubleColDivideDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_n0  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_n0  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-2.0
0.20400000000000001
1.57
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0000000002
2.0000000002
2.000780876494024
2.0032
2.0933333333333333
2.0933333333333333
2.0933333333333333
2.24
2.24
2.24
2.244
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (1.0D + (UDFToDouble(key) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DoubleScalarAddDoubleColumn(val 1.0, col 4:double)(children: DoubleColDivideDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double) -> 3:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-2199.0
-6.172839440617284E8
-626.745
0.43899999999999995
0.43999999999999995
0.43999999999999995
0.8335
0.835
0.85
1.0
1.0
1.0
1.005
1.01
1.05
1.1
1.15
1.165
1.1665
1.5
1.5
1.5
1.56
1.561
101.0
11.0
2.0
2.0
2.5700000000000003
2.5700000000000003
2.5700000000000003
2.5700000000000003
51.0
6.0
6.172839460617284E8
63.0
63.6
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: abs(key) (type: decimal(20,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncAbsDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(20,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(20,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
0.0000000000
0.0000000000
0.0000000000
0.0100000000
0.0200000000
0.1000000000
0.2000000000
0.3000000000
0.3000000000
0.3300000000
0.3300000000
0.3330000000
0.3330000000
1.0000000000
1.0000000000
1.0000000000
1.1200000000
1.1200000000
1.1200000000
1.1220000000
1.1220000000
10.0000000000
100.0000000000
1234567890.1234567800
1234567890.1234567890
124.0000000000
125.2000000000
1255.4900000000
2.0000000000
2.0000000000
20.0000000000
200.0000000000
3.1400000000
3.1400000000
3.1400000000
3.1400000000
4400.0000000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_n0 GROUP BY value ORDER BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_n0 GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: sum(key), count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDecimal(col 0:decimal(20,10)) -> decimal(30,10), VectorUDAFCount(col 0:decimal(20,10)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 18 Data size: 2232 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 18 Data size: 2232 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: decimal(30,10)), _col2 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:decimal(30,10), VALUE._col1:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), count(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDecimal(col 1:decimal(30,10)) -> decimal(30,10), VectorUDAFCountMerge(col 2:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 17 Data size: 2108 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: int), (_col1 / CAST( _col2 AS decimal(19,0))) (type: decimal(38,18)), (CAST( _col1 AS decimal(24,14)) / _col2) (type: decimal(38,28)), _col1 (type: decimal(30,10))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 6, 1]
                      selectExpressions: DecimalColDivideDecimalColumn(col 1:decimal(30,10), col 3:decimal(19,0))(children: CastLongToDecimal(col 2:bigint) -> 3:decimal(19,0)) -> 4:decimal(38,18), DecimalColDivideDecimalColumn(col 5:decimal(24,14), col 3:decimal(19,0))(children: CastDecimalToDecimal(col 1:decimal(30,10)) -> 5:decimal(24,14), CastLongToDecimal(col 2:bigint) -> 3:decimal(19,0)) -> 6:decimal(38,28)
                  Statistics: Num rows: 17 Data size: 5780 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        keyColumnNums: [0]
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        valueColumnNums: [4, 6, 1]
                    Statistics: Num rows: 17 Data size: 5780 Basic stats: COMPLETE Column stats: COMPLETE
                    value expressions: _col1 (type: decimal(38,18)), _col2 (type: decimal(38,28)), _col3 (type: decimal(30,10))
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY.reducesinkkey0:int, VALUE._col0:decimal(38,18), VALUE._col1:decimal(38,28), VALUE._col2:decimal(30,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: decimal(38,18)), VALUE._col1 (type: decimal(38,28)), VALUE._col2 (type: decimal(30,10))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 17 Data size: 5780 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 17 Data size: 5780 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_n0 GROUP BY value ORDER BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_n0 GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1	-1.120000000000000000	-1.1200000000000000000000000000	-2.2400000000
-11	-1.122000000000000000	-1.1220000000000000000000000000	-1.1220000000
-1234567890	-1234567890.123456789000000000	-1234567890.1234567890000000000000000000	-1234567890.1234567890
-1255	-1255.490000000000000000	-1255.4900000000000000000000000000	-1255.4900000000
0	0.025384615384615385	0.0253846153846153846153846154	0.3300000000
1	1.048400000000000000	1.0484000000000000000000000000	5.2420000000
10	10.000000000000000000	10.0000000000000000000000000000	10.0000000000
100	100.000000000000000000	100.0000000000000000000000000000	100.0000000000
1234567890	1234567890.123456780000000000	1234567890.1234567800000000000000000000	1234567890.1234567800
124	124.000000000000000000	124.0000000000000000000000000000	124.0000000000
125	125.200000000000000000	125.2000000000000000000000000000	125.2000000000
2	2.000000000000000000	2.0000000000000000000000000000	4.0000000000
20	20.000000000000000000	20.0000000000000000000000000000	20.0000000000
200	200.000000000000000000	200.0000000000000000000000000000	200.0000000000
3	3.140000000000000000	3.1400000000000000000000000000	9.4200000000
4	3.140000000000000000	3.1400000000000000000000000000	3.1400000000
4400	-4400.000000000000000000	-4400.0000000000000000000000000000	-4400.0000000000
NULL	NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (- key) (type: decimal(20,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncNegateDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(20,10)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(20,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT -key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT -key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.0100000000
-0.0200000000
-0.1000000000
-0.2000000000
-0.3000000000
-0.3300000000
-0.3330000000
-1.0000000000
-1.0000000000
-1.0000000000
-1.1200000000
-1.1220000000
-10.0000000000
-100.0000000000
-1234567890.1234567800
-124.0000000000
-125.2000000000
-2.0000000000
-2.0000000000
-20.0000000000
-200.0000000000
-3.1400000000
-3.1400000000
-3.1400000000
-3.1400000000
0.0000000000
0.0000000000
0.0000000000
0.3000000000
0.3300000000
0.3330000000
1.1200000000
1.1200000000
1.1220000000
1234567890.1234567890
1255.4900000000
4400.0000000000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimal_udf_n0
          Select Operator
            expressions: key (type: decimal(20,10))
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT +key FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT +key FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.3000000000
-0.3300000000
-0.3330000000
-1.1200000000
-1.1200000000
-1.1220000000
-1234567890.1234567890
-1255.4900000000
-4400.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0100000000
0.0200000000
0.1000000000
0.2000000000
0.3000000000
0.3300000000
0.3330000000
1.0000000000
1.0000000000
1.0000000000
1.1200000000
1.1220000000
10.0000000000
100.0000000000
1234567890.1234567800
124.0000000000
125.2000000000
2.0000000000
2.0000000000
20.0000000000
200.0000000000
3.1400000000
3.1400000000
3.1400000000
3.1400000000
NULL
NULL
PREHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: ceil(key) (type: decimal(11,0))
                    outputColumnNames: _col0
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1
-1
-1
-1234567890
-1255
-4400
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
10
100
1234567891
124
126
2
2
2
2
20
200
4
4
4
4
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: floor(key) (type: decimal(11,0))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncFloorDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(11,0)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(11,0)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1
-1
-1
-1234567891
-1256
-2
-2
-2
-4400
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
10
100
1234567890
124
125
2
2
20
200
3
3
3
3
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: round(key, 2) (type: decimal(13,2))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncRoundWithNumDigitsDecimalToDecimal(col 0:decimal(20,10), decimalPlaces 2) -> 3:decimal(13,2)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(13,2)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.30
-0.33
-0.33
-1.12
-1.12
-1.12
-1234567890.12
-1255.49
-4400.00
0.00
0.00
0.00
0.01
0.02
0.10
0.20
0.30
0.33
0.33
1.00
1.00
1.00
1.12
1.12
10.00
100.00
1234567890.12
124.00
125.20
2.00
2.00
20.00
200.00
3.14
3.14
3.14
3.14
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: power(key, 2) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: VectorUDFAdaptor(power(key, 2)) -> 3:double
                    Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
0.0
0.0
0.0
0.010000000000000002
0.04000000000000001
0.09
0.09
0.10890000000000001
0.10890000000000001
0.11088900000000002
0.11088900000000002
1.0
1.0
1.0
1.0E-4
1.2544000000000002
1.2544000000000002
1.2544000000000002
1.2588840000000003
1.2588840000000003
1.52415787532388352E18
1.52415787532388352E18
1.936E7
100.0
10000.0
15376.0
15675.04
1576255.1401
4.0
4.0
4.0E-4
400.0
40000.0
9.8596
9.8596
9.8596
9.8596
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: ((key + 1) % (key / 2)) (type: decimal(22,12))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [5]
                        selectExpressions: DecimalColModuloDecimalColumn(col 3:decimal(21,10), col 4:decimal(22,12))(children: DecimalColAddDecimalScalar(col 0:decimal(20,10), val 1) -> 3:decimal(21,10), DecimalColDivideDecimalScalar(col 0:decimal(20,10), val 2) -> 4:decimal(22,12)) -> 5:decimal(22,12)
                    Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 39 Data size: 4368 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10), decimal(22,12), decimal(22,12)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-0.120000000000
-0.120000000000
-0.122000000000
-2199.000000000000
-617283944.061728394500
-626.745000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.000000000000
0.001000000000
0.001000000000
0.010000000000
0.010000000000
0.100000000000
0.100000000000
0.439000000000
0.440000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
1.000000000000
NULL
NULL
NULL
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_n0 GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_n0 GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: value (type: int), key (type: decimal(20,10)), UDFToDouble(key) (type: double), (UDFToDouble(key) * UDFToDouble(key)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [1, 0, 3, 6]
                        selectExpressions: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColMultiplyDoubleColumn(col 4:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 4:double, CastDecimalToDouble(col 0:decimal(20,10)) -> 5:double) -> 6:double
                    Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: sum(_col3), sum(_col2), count(_col1)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDouble(col 6:double) -> double, VectorUDAFSumDouble(col 3:double) -> double, VectorUDAFCount(col 0:decimal(20,10)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 18 Data size: 504 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 18 Data size: 504 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double, double]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:double, VALUE._col1:double, VALUE._col2:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), count(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDouble(col 1:double) -> double, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 17 Data size: 476 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: int), power(((_col1 - ((_col2 * _col2) / _col3)) / _col3), 0.5) (type: double), ((_col1 - ((_col2 * _col2) / _col3)) / _col3) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 6]
                      selectExpressions: FuncPowerDoubleToDouble(col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 4:double) -> 5:double) -> 4:double) -> 5:double) -> 4:double, DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 6:double)(children: DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 5:double) -> 6:double) -> 5:double) -> 6:double
                  Statistics: Num rows: 17 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 17 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_n0 GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_n0 GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1	0.0	0.0
-11	0.0	0.0
-1234567890	0.0	0.0
-1255	0.0	0.0
0	0.22561046704494161	0.05090008284023669
1	0.05928102563215448	0.003514240000000157
10	0.0	0.0
100	0.0	0.0
1234567890	0.0	0.0
124	0.0	0.0
125	0.0	0.0
2	0.0	0.0
20	0.0	0.0
200	0.0	0.0
3	0.0	0.0
4	0.0	0.0
4400	0.0	0.0
NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_n0 GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_n0 GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: value (type: int), key (type: decimal(20,10)), UDFToDouble(key) (type: double), (UDFToDouble(key) * UDFToDouble(key)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [1, 0, 3, 6]
                        selectExpressions: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColMultiplyDoubleColumn(col 4:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 4:double, CastDecimalToDouble(col 0:decimal(20,10)) -> 5:double) -> 6:double
                    Statistics: Num rows: 39 Data size: 4412 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: sum(_col3), sum(_col2), count(_col1)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDouble(col 6:double) -> double, VectorUDAFSumDouble(col 3:double) -> double, VectorUDAFCount(col 0:decimal(20,10)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 18 Data size: 504 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 18 Data size: 504 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double, double]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:double, VALUE._col1:double, VALUE._col2:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), count(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDouble(col 1:double) -> double, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 17 Data size: 476 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: int), power(((_col1 - ((_col2 * _col2) / _col3)) / CASE WHEN ((_col3 = 1L)) THEN (null) ELSE ((_col3 - 1)) END), 0.5) (type: double), ((_col1 - ((_col2 * _col2) / _col3)) / CASE WHEN ((_col3 = 1L)) THEN (null) ELSE ((_col3 - 1)) END) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 9]
                      selectExpressions: FuncPowerDoubleToDouble(col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 8:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 4:double) -> 5:double) -> 4:double, IfExprNullCondExpr(col 6:boolean, null, col 7:bigint)(children: LongColEqualLongScalar(col 3:bigint, val 1) -> 6:boolean, LongColSubtractLongScalar(col 3:bigint, val 1) -> 7:bigint) -> 8:bigint) -> 5:double) -> 4:double, DoubleColDivideLongColumn(col 5:double, col 11:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 9:double)(children: DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 5:double) -> 9:double) -> 5:double, IfExprNullCondExpr(col 8:boolean, null, col 10:bigint)(children: LongColEqualLongScalar(col 3:bigint, val 1) -> 8:boolean, LongColSubtractLongScalar(col 3:bigint, val 1) -> 10:bigint) -> 11:bigint) -> 9:double
                  Statistics: Num rows: 17 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 17 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_n0 GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_n0 GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1	0.0	0.0
-11	NULL	NULL
-1234567890	NULL	NULL
-1255	NULL	NULL
0	0.23482281918556472	0.05514175641025642
1	0.06627820154470243	0.0043928000000001965
10	NULL	NULL
100	NULL	NULL
1234567890	NULL	NULL
124	NULL	NULL
125	NULL	NULL
2	0.0	0.0
20	NULL	NULL
200	NULL	NULL
3	0.0	0.0
4	NULL	NULL
4400	NULL	NULL
NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: _col0
                    Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: histogram_numeric(_col0, 3)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 160 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 160 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: array<double>)
            Execution mode: llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
        Reducer 2 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: histogram_numeric(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 720 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 720 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
[{"x":-1.2345678901234567E9,"y":1.0},{"x":-144.50057142857142,"y":35.0},{"x":1.2345678901234567E9,"y":1.0}]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: min(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: decimal(20,10))
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(20,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
-1234567890.1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMaxDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: decimal(20,10))
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(20,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMaxDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
1234567890.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_n0
                  Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 39 Data size: 4256 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFCount(col 0:decimal(20,10)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFCountMerge(col 0:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_n0
#### A masked pattern was here ####
37
PREHOOK: query: CREATE TABLE DECIMAL_UDF_txt_small (key decimal(15,3), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF_txt_small
POSTHOOK: query: CREATE TABLE DECIMAL_UDF_txt_small (key decimal(15,3), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF_txt_small
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt_small
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_udf_txt_small
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt_small
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_udf_txt_small
PREHOOK: query: insert into DECIMAL_UDF_txt_small values (NULL, NULL)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@decimal_udf_txt_small
POSTHOOK: query: insert into DECIMAL_UDF_txt_small values (NULL, NULL)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@decimal_udf_txt_small
POSTHOOK: Lineage: decimal_udf_txt_small.key EXPRESSION []
POSTHOOK: Lineage: decimal_udf_txt_small.value EXPRESSION []
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + key) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.600
-0.660
-0.666
-2.240
-2.240
-2.244
-2469135780.246
-2510.980
-8800.000
0.000
0.000
0.000
0.020
0.040
0.200
0.400
0.600
0.660
0.666
2.000
2.000
2.000
2.240
2.244
20.000
200.000
2469135780.246
248.000
250.400
4.000
4.000
40.000
400.000
6.280
6.280
6.280
6.280
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + CAST( value AS decimal(10,0))) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.300
-0.330
-0.333
-12.122
-2.120
-2.120
-2469135780.123
-2510.490
0.000
0.000
0.000
0.000
0.010
0.020
0.100
0.200
0.300
0.330
0.333
2.000
2.000
2.000
2.120
2.122
20.000
200.000
2469135780.123
248.000
250.200
4.000
4.000
40.000
400.000
6.140
6.140
6.140
7.140
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.3
-0.33
-0.333
-1.62
-1.62
-1.851851835123E9
-1882.99
-2200.0
-6.622
0.0
0.0
0.0
0.01
0.02
0.1
0.2
0.3
0.33
0.333
1.5
1.5
1.5
1.62
1.622
1.851851835123E9
15.0
150.0
186.0
187.7
3.0
3.0
30.0
300.0
4.640000000000001
4.640000000000001
4.640000000000001
5.140000000000001
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + 1.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.1200000000000001
-0.1200000000000001
-0.12200000000000011
-1.234567889123E9
-1254.49
-4399.0
0.667
0.6699999999999999
0.7
1.0
1.0
1.0
1.01
1.02
1.1
1.2
1.234567891123E9
1.3
1.33
1.333
101.0
11.0
125.0
126.2
2.0
2.0
2.0
2.12
2.122
201.0
21.0
3.0
3.0
4.140000000000001
4.140000000000001
4.140000000000001
4.140000000000001
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - key) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - CAST( value AS decimal(10,0))) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.120
-0.120
-0.123
-0.300
-0.330
-0.333
-0.490
-0.860
-8800.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.010
0.020
0.100
0.120
0.122
0.123
0.140
0.140
0.140
0.200
0.200
0.300
0.330
0.333
9.878
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.3
-0.33
-0.333
-0.6200000000000001
-0.6200000000000001
-6.172839451229999E8
-627.99
-6600.0
0.0
0.0
0.0
0.01
0.02
0.1
0.2
0.3
0.33
0.333
0.5
0.5
0.5
0.6200000000000001
0.6220000000000001
1.0
1.0
1.1400000000000001
1.6400000000000001
1.6400000000000001
1.6400000000000001
10.0
100.0
4.378
5.0
50.0
6.172839451229999E8
62.0
62.7
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - 1.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.667
-0.6699999999999999
-0.7
-0.8
-0.9
-0.98
-0.99
-1.0
-1.0
-1.0
-1.234567891123E9
-1.3
-1.33
-1.333
-1256.49
-2.12
-2.12
-2.122
-4401.0
0.0
0.0
0.0
0.1200000000000001
0.12200000000000011
1.0
1.0
1.234567889123E9
123.0
124.2
19.0
199.0
2.14
2.14
2.14
2.14
9.0
99.0
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * key) (type: decimal(31,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(31,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(31,6)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.000000
0.000000
0.000000
0.000100
0.000400
0.010000
0.040000
0.090000
0.090000
0.108900
0.108900
0.110889
0.110889
1.000000
1.000000
1.000000
1.254400
1.254400
1.254400
1.258884
1.258884
100.000000
10000.000000
1524157875322755800.955129
1524157875322755800.955129
15376.000000
15675.040000
1576255.140100
19360000.000000
4.000000
4.000000
400.000000
40000.000000
9.859600
9.859600
9.859600
9.859600
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColGreaterDecimalScalar(col 4:decimal(26,3), val 0)(children: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,3))
                    predicate: ((key * CAST( value AS decimal(10,0))) > 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: decimal(15,3)), value (type: int)
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1.120	-1
-1.120	-1
-1.122	-11
-1234567890.123	-1234567890
-1255.490	-1255
1.000	1
1.000	1
1.000	1
1.120	1
1.122	1
10.000	10
100.000	100
1234567890.123	1234567890
124.000	124
125.200	125
2.000	2
2.000	2
20.000	20
200.000	200
3.140	3
3.140	3
3.140	3
3.140	4
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * CAST( value AS decimal(10,0))) (type: decimal(26,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-19360000.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
1.000
1.000
1.000
1.120
1.120
1.120
1.122
100.000
10000.000
12.342
12.560
1524157875170903950.470
1524157875170903950.470
15376.000
15650.000
1575639.950
4.000
4.000
400.000
40000.000
9.420
9.420
9.420
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * (UDFToDouble(value) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.0
-0.0
-0.0
-9680000.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.5
0.5
0.5
0.56
0.56
0.56
0.561
2.0
2.0
200.0
20000.0
4.71
4.71
4.71
50.0
5000.0
6.171
6.28
7.620789375854519E17
7.620789375854519E17
7688.0
7825.0
787819.975
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * 2.0D) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.6
-0.66
-0.666
-2.24
-2.24
-2.244
-2.469135780246E9
-2510.98
-8800.0
0.0
0.0
0.0
0.02
0.04
0.2
0.4
0.6
0.66
0.666
2.0
2.0
2.0
2.24
2.244
2.469135780246E9
20.0
200.0
248.0
250.4
4.0
4.0
40.0
400.0
6.28
6.28
6.28
6.28
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key / 0) (type: decimal(18,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColDivideDecimalScalar(col 0:decimal(15,3), val 0) -> 3:decimal(18,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(18,6)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColNotEqualDecimalScalar(col 0:decimal(15,3), val 0)
                    predicate: (key <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / key) (type: decimal(34,19))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [3]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(34,19)
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(34,19)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / CAST( value AS decimal(10,0))) (type: decimal(26,14))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,14)
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,14)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1.00000000000000
0.10200000000000
0.78500000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000009963
1.00000000009963
1.00039043824701
1.00160000000000
1.04666666666667
1.04666666666667
1.04666666666667
1.12000000000000
1.12000000000000
1.12000000000000
1.12200000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (UDFToDouble(key) / (UDFToDouble(value) / 2.0D)) (type: double)
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DoubleColDivideDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2.0
0.20400000000000001
1.57
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0000000001992597
2.0000000001992597
2.000780876494024
2.0032
2.0933333333333333
2.0933333333333333
2.0933333333333333
2.24
2.24
2.24
2.244
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (1.0D + (UDFToDouble(key) / 2.0D)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DoubleScalarAddDoubleColumn(val 1.0, col 4:double)(children: DoubleColDivideDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double) -> 3:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2199.0
-6.172839440615E8
-626.745
0.43899999999999995
0.43999999999999995
0.43999999999999995
0.8335
0.835
0.85
1.0
1.0
1.0
1.005
1.01
1.05
1.1
1.15
1.165
1.1665
1.5
1.5
1.5
1.56
1.561
101.0
11.0
2.0
2.0
2.5700000000000003
2.5700000000000003
2.5700000000000003
2.5700000000000003
51.0
6.0
6.172839460615E8
63.0
63.6
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: abs(key) (type: decimal(15,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncAbsDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(15,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.000
0.000
0.000
0.010
0.020
0.100
0.200
0.300
0.300
0.330
0.330
0.333
0.333
1.000
1.000
1.000
1.120
1.120
1.120
1.122
1.122
10.000
100.000
1234567890.123
1234567890.123
124.000
125.200
1255.490
2.000
2.000
20.000
200.000
3.140
3.140
3.140
3.140
4400.000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(key), count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDecimal(col 0:decimal(15,3)) -> decimal(25,3), VectorUDAFCount(col 0:decimal(15,3)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: decimal(25,3)), _col2 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:decimal(25,3), VALUE._col1:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), count(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDecimal(col 1:decimal(25,3)) -> decimal(25,3), VectorUDAFCountMerge(col 2:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), (_col1 / CAST( _col2 AS decimal(19,0))) (type: decimal(38,16)), (CAST( _col1 AS decimal(19,7)) / _col2) (type: decimal(38,26)), _col1 (type: decimal(25,3))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 6, 1]
                      selectExpressions: DecimalColDivideDecimalColumn(col 1:decimal(25,3), col 3:decimal(19,0))(children: CastLongToDecimal(col 2:bigint) -> 3:decimal(19,0)) -> 4:decimal(38,16), DecimalColDivideDecimalColumn(col 5:decimal(19,7), col 3:decimal(19,0))(children: CastDecimalToDecimal(col 1:decimal(25,3)) -> 5:decimal(19,7), CastLongToDecimal(col 2:bigint) -> 3:decimal(19,0)) -> 6:decimal(38,26)
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        keyColumnNums: [0]
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        valueColumnNums: [4, 6, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col1 (type: decimal(38,16)), _col2 (type: decimal(38,26)), _col3 (type: decimal(25,3))
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY.reducesinkkey0:int, VALUE._col0:decimal(38,16), VALUE._col1:decimal(38,26), VALUE._col2:decimal(25,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: decimal(38,16)), VALUE._col1 (type: decimal(38,26)), VALUE._col2 (type: decimal(25,3))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1	-1.1200000000000000	-1.12000000000000000000000000	-2.240
-11	-1.1220000000000000	-1.12200000000000000000000000	-1.122
-1234567890	-1234567890.1230000000000000	-1234567890.12300000000000000000000000	-1234567890.123
-1255	-1255.4900000000000000	-1255.49000000000000000000000000	-1255.490
0	0.0253846153846154	0.02538461538461538461538462	0.330
1	1.0484000000000000	1.04840000000000000000000000	5.242
10	10.0000000000000000	10.00000000000000000000000000	10.000
100	100.0000000000000000	100.00000000000000000000000000	100.000
1234567890	1234567890.1230000000000000	1234567890.12300000000000000000000000	1234567890.123
124	124.0000000000000000	124.00000000000000000000000000	124.000
125	125.2000000000000000	125.20000000000000000000000000	125.200
2	2.0000000000000000	2.00000000000000000000000000	4.000
20	20.0000000000000000	20.00000000000000000000000000	20.000
200	200.0000000000000000	200.00000000000000000000000000	200.000
3	3.1400000000000000	3.14000000000000000000000000	9.420
4	3.1400000000000000	3.14000000000000000000000000	3.140
4400	-4400.0000000000000000	-4400.00000000000000000000000000	-4400.000
NULL	NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (- key) (type: decimal(15,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncNegateDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(15,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT -key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT -key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.010
-0.020
-0.100
-0.200
-0.300
-0.330
-0.333
-1.000
-1.000
-1.000
-1.120
-1.122
-10.000
-100.000
-1234567890.123
-124.000
-125.200
-2.000
-2.000
-20.000
-200.000
-3.140
-3.140
-3.140
-3.140
0.000
0.000
0.000
0.300
0.330
0.333
1.120
1.120
1.122
1234567890.123
1255.490
4400.000
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimal_udf_txt_small
          Select Operator
            expressions: key (type: decimal(15,3))
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT +key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT +key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.300
-0.330
-0.333
-1.120
-1.120
-1.122
-1234567890.123
-1255.490
-4400.000
0.000
0.000
0.000
0.010
0.020
0.100
0.200
0.300
0.330
0.333
1.000
1.000
1.000
1.120
1.122
10.000
100.000
1234567890.123
124.000
125.200
2.000
2.000
20.000
200.000
3.140
3.140
3.140
3.140
NULL
NULL
PREHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ceil(key) (type: decimal(13,0))
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1
-1
-1
-1234567890
-1255
-4400
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
10
100
1234567891
124
126
2
2
2
2
20
200
4
4
4
4
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: floor(key) (type: decimal(13,0))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncFloorDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(13,0)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(13,0)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1
-1
-1
-1234567891
-1256
-2
-2
-2
-4400
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
10
100
1234567890
124
125
2
2
20
200
3
3
3
3
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: round(key, 2) (type: decimal(15,2))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncRoundWithNumDigitsDecimalToDecimal(col 0:decimal(15,3), decimalPlaces 2) -> 3:decimal(15,2)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,2)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.30
-0.33
-0.33
-1.12
-1.12
-1.12
-1234567890.12
-1255.49
-4400.00
0.00
0.00
0.00
0.01
0.02
0.10
0.20
0.30
0.33
0.33
1.00
1.00
1.00
1.12
1.12
10.00
100.00
1234567890.12
124.00
125.20
2.00
2.00
20.00
200.00
3.14
3.14
3.14
3.14
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: power(key, 2) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: VectorUDFAdaptor(power(key, 2)) -> 3:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.0
0.0
0.0
0.010000000000000002
0.04000000000000001
0.09
0.09
0.10890000000000001
0.10890000000000001
0.11088900000000002
0.11088900000000002
1.0
1.0
1.0
1.0E-4
1.2544000000000002
1.2544000000000002
1.2544000000000002
1.2588840000000003
1.2588840000000003
1.52415787532275558E18
1.52415787532275558E18
1.936E7
100.0
10000.0
15376.0
15675.04
1576255.1401
4.0
4.0
4.0E-4
400.0
40000.0
9.8596
9.8596
9.8596
9.8596
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: ((key + 1) % (key / 2)) (type: decimal(18,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [5]
                        selectExpressions: DecimalColModuloDecimalColumn(col 3:decimal(16,3), col 4:decimal(18,6))(children: DecimalColAddDecimalScalar(col 0:decimal(15,3), val 1) -> 3:decimal(16,3), DecimalColDivideDecimalScalar(col 0:decimal(15,3), val 2) -> 4:decimal(18,6)) -> 5:decimal(18,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3), decimal(18,6), decimal(18,6)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-0.120000
-0.120000
-0.122000
-2199.000000
-617283944.061500
-626.745000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.001000
0.001000
0.010000
0.010000
0.100000
0.100000
0.439000
0.440000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
NULL
NULL
NULL
NULL
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: value (type: int), key (type: decimal(15,3)), UDFToDouble(key) (type: double), (UDFToDouble(key) * UDFToDouble(key)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [1, 0, 3, 6]
                        selectExpressions: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColMultiplyDoubleColumn(col 4:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 4:double, CastDecimalToDouble(col 0:decimal(15,3)) -> 5:double) -> 6:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(_col3), sum(_col2), count(_col1)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDouble(col 6:double) -> double, VectorUDAFSumDouble(col 3:double) -> double, VectorUDAFCount(col 0:decimal(15,3)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double, double]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:double, VALUE._col1:double, VALUE._col2:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), count(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDouble(col 1:double) -> double, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), power(((_col1 - ((_col2 * _col2) / _col3)) / _col3), 0.5) (type: double), ((_col1 - ((_col2 * _col2) / _col3)) / _col3) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 6]
                      selectExpressions: FuncPowerDoubleToDouble(col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 4:double) -> 5:double) -> 4:double) -> 5:double) -> 4:double, DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 6:double)(children: DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 5:double) -> 6:double) -> 5:double) -> 6:double
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1	0.0	0.0
-11	0.0	0.0
-1234567890	0.0	0.0
-1255	0.0	0.0
0	0.22561046704494161	0.05090008284023669
1	0.05928102563215448	0.003514240000000157
10	0.0	0.0
100	0.0	0.0
1234567890	0.0	0.0
124	0.0	0.0
125	0.0	0.0
2	0.0	0.0
20	0.0	0.0
200	0.0	0.0
3	0.0	0.0
4	0.0	0.0
4400	0.0	0.0
NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: value (type: int), key (type: decimal(15,3)), UDFToDouble(key) (type: double), (UDFToDouble(key) * UDFToDouble(key)) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [1, 0, 3, 6]
                        selectExpressions: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColMultiplyDoubleColumn(col 4:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 4:double, CastDecimalToDouble(col 0:decimal(15,3)) -> 5:double) -> 6:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(_col3), sum(_col2), count(_col1)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDouble(col 6:double) -> double, VectorUDAFSumDouble(col 3:double) -> double, VectorUDAFCount(col 0:decimal(15,3)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: _col0 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: double), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double, double]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:double, VALUE._col1:double, VALUE._col2:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1), count(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDouble(col 1:double) -> double, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), power(((_col1 - ((_col2 * _col2) / _col3)) / CASE WHEN ((_col3 = 1L)) THEN (null) ELSE ((_col3 - 1)) END), 0.5) (type: double), ((_col1 - ((_col2 * _col2) / _col3)) / CASE WHEN ((_col3 = 1L)) THEN (null) ELSE ((_col3 - 1)) END) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 4, 9]
                      selectExpressions: FuncPowerDoubleToDouble(col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 8:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 5:double)(children: DoubleColDivideLongColumn(col 4:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 4:double) -> 5:double) -> 4:double, IfExprNullCondExpr(col 6:boolean, null, col 7:bigint)(children: LongColEqualLongScalar(col 3:bigint, val 1) -> 6:boolean, LongColSubtractLongScalar(col 3:bigint, val 1) -> 7:bigint) -> 8:bigint) -> 5:double) -> 4:double, DoubleColDivideLongColumn(col 5:double, col 11:bigint)(children: DoubleColSubtractDoubleColumn(col 1:double, col 9:double)(children: DoubleColDivideLongColumn(col 5:double, col 3:bigint)(children: DoubleColMultiplyDoubleColumn(col 2:double, col 2:double) -> 5:double) -> 9:double) -> 5:double, IfExprNullCondExpr(col 8:boolean, null, col 10:bigint)(children: LongColEqualLongScalar(col 3:bigint, val 1) -> 8:boolean, LongColSubtractLongScalar(col 3:bigint, val 1) -> 10:bigint) -> 11:bigint) -> 9:double
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1	0.0	0.0
-11	NULL	NULL
-1234567890	NULL	NULL
-1255	NULL	NULL
0	0.23482281918556472	0.05514175641025642
1	0.06627820154470243	0.0043928000000001965
10	NULL	NULL
100	NULL	NULL
1234567890	NULL	NULL
124	NULL	NULL
125	NULL	NULL
2	0.0	0.0
20	NULL	NULL
200	NULL	NULL
3	0.0	0.0
4	NULL	NULL
4400	NULL	NULL
NULL	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: histogram_numeric(_col0, 3)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: array<double>)
            Execution mode: llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
        Reducer 2 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: histogram_numeric(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
[{"x":-1.234567890123E9,"y":1.0},{"x":-144.50057142857142,"y":35.0},{"x":1.234567890123E9,"y":1.0}]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(15,3))
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(15,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: max(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMaxDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(15,3))
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(15,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMaxDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFCount(col 0:decimal(15,3)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFCountMerge(col 0:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
37
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@decimal_udf_txt
PREHOOK: Output: default@decimal_udf_txt
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@decimal_udf_txt
POSTHOOK: Output: default@decimal_udf_txt
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_n0
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@decimal_udf_n0
PREHOOK: Output: default@decimal_udf_n0
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_n0
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@decimal_udf_n0
POSTHOOK: Output: default@decimal_udf_n0
