PREHOOK: query: CREATE TABLE dest1_n0(key INT, val1 INT, val2 INT)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@dest1_n0
POSTHOOK: query: CREATE TABLE dest1_n0(key INT, val1 INT, val2 INT)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dest1_n0
PREHOOK: query: CREATE TABLE dest2(key INT, val1 INT, val2 INT)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@dest2
POSTHOOK: query: CREATE TABLE dest2(key INT, val1 INT, val2 INT)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dest2
PREHOOK: query: CREATE TABLE INPUT(key INT, value STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@INPUT
POSTHOOK: query: CREATE TABLE INPUT(key INT, value STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@INPUT
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv5.txt' INTO TABLE INPUT
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@input
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv5.txt' INTO TABLE INPUT
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@input
PREHOOK: query: EXPLAIN
FROM INPUT 
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: EXPLAIN
FROM INPUT 
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Map 1 (SIMPLE_EDGE)
        Reducer 7 <- Reducer 6 (SIMPLE_EDGE)
        Reducer 8 <- Reducer 7 (CUSTOM_SIMPLE_EDGE)
        Reducer 9 <- Reducer 8 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: input
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: int), substr(value, 5) (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: key (type: int)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: int), substr(value, 5) (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: key (type: int)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(KEY._col1:0._col0), count(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: int)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: z
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint), _col2 (type: bigint)
        Reducer 3 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), count(VALUE._col1)
                keys: KEY._col0 (type: int)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest1_n0
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Map-reduce partition columns: rand() (type: double)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: int), val1 (type: int), val2 (type: int)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  null sort order: 
                  sort order: 
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: int), _col11 (type: bigint), _col12 (type: binary)
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: final
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(KEY._col1:0._col0), sum(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: int)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: z
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: double), _col2 (type: double)
        Reducer 7 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: int)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest2
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Map-reduce partition columns: rand() (type: double)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: int), val1 (type: int), val2 (type: int)
        Reducer 8 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  null sort order: 
                  sort order: 
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: int), _col11 (type: bigint), _col12 (type: binary)
        Reducer 9 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: final
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest1_n0

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest1_n0

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest2

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest2

PREHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
POSTHOOK: Lineage: dest1_n0.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: SELECT * from dest1_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest1_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
128	1	1
150	1	1
165	1	1
193	1	1
213	3	2
224	1	1
238	3	3
255	1	1
265	1	1
27	1	1
273	1	1
278	1	1
311	1	1
369	1	1
401	1	1
409	1	1
484	1	1
66	1	1
86	1	1
98	1	1
PREHOOK: query: SELECT * from dest2
PREHOOK: type: QUERY
PREHOOK: Input: default@dest2
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2
#### A masked pattern was here ####
128	128	128
150	150	150
165	165	165
193	193	193
213	640	427
224	224	224
238	717	717
255	255	255
265	265	265
27	27	27
273	273	273
278	278	278
311	311	311
369	369	369
401	401	401
409	409	409
484	484	484
66	66	66
86	86	86
98	98	98
PREHOOK: query: EXPLAIN
FROM INPUT 
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: EXPLAIN
FROM INPUT 
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Map 1 (SIMPLE_EDGE)
        Reducer 7 <- Reducer 6 (SIMPLE_EDGE)
        Reducer 8 <- Reducer 7 (CUSTOM_SIMPLE_EDGE)
        Reducer 9 <- Reducer 8 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: input
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: int), substr(value, 5) (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: key (type: int)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: int), substr(value, 5) (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: key (type: int)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(KEY._col1:0._col0), count(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: int)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: z
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint), _col2 (type: bigint)
        Reducer 3 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0), count(VALUE._col1)
                keys: KEY._col0 (type: int)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest1_n0
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Map-reduce partition columns: rand() (type: double)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: int), val1 (type: int), val2 (type: int)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  null sort order: 
                  sort order: 
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: int), _col11 (type: bigint), _col12 (type: binary)
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: final
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(KEY._col1:0._col0), sum(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: int)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  null sort order: z
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: double), _col2 (type: double)
        Reducer 7 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1)
                keys: KEY._col0 (type: int)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest2
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Map-reduce partition columns: rand() (type: double)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: int), val1 (type: int), val2 (type: int)
        Reducer 8 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: partial1
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  null sort order: 
                  sort order: 
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: int), _col11 (type: bigint), _col12 (type: binary)
        Reducer 9 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: final
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest1_n0

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest1_n0

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest2

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest2

PREHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
POSTHOOK: Lineage: dest1_n0.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: SELECT * from dest1_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest1_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
128	1	1
150	1	1
165	1	1
193	1	1
213	3	2
224	1	1
238	3	3
255	1	1
265	1	1
27	1	1
273	1	1
278	1	1
311	1	1
369	1	1
401	1	1
409	1	1
484	1	1
66	1	1
86	1	1
98	1	1
PREHOOK: query: SELECT * from dest2
PREHOOK: type: QUERY
PREHOOK: Input: default@dest2
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2
#### A masked pattern was here ####
128	128	128
150	150	150
165	165	165
193	193	193
213	640	427
224	224	224
238	717	717
255	255	255
265	265	265
27	27	27
273	273	273
278	278	278
311	311	311
369	369	369
401	401	401
409	409	409
484	484	484
66	66	66
86	86	86
98	98	98
PREHOOK: query: EXPLAIN
FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: EXPLAIN
FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
        Reducer 4 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: input
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: int), substr(value, 5) (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: key (type: int)
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Forward
                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                Group By Operator
                  aggregations: sum(DISTINCT KEY._col1:0._col0), count(DISTINCT KEY._col1:1._col0)
                  keys: KEY._col0 (type: int)
                  mode: complete
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.dest1_n0
                    Select Operator
                      expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                      outputColumnNames: key, val1, val2
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                        value expressions: key (type: int), val1 (type: int), val2 (type: int)
                Group By Operator
                  aggregations: sum(DISTINCT KEY._col1:0._col0), avg(DISTINCT KEY._col1:1._col0)
                  keys: KEY._col0 (type: int)
                  mode: complete
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: int), UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.dest2
                    Select Operator
                      expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int)
                      outputColumnNames: key, val1, val2
                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: NONE
                        value expressions: key (type: int), val1 (type: int), val2 (type: int)
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: complete
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col0), count(1), count(VALUE._col0), compute_bit_vector_hll(VALUE._col0), min(VALUE._col2), max(VALUE._col2), count(VALUE._col2), compute_bit_vector_hll(VALUE._col2), min(VALUE._col3), max(VALUE._col3), count(VALUE._col3), compute_bit_vector_hll(VALUE._col3)
                mode: complete
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'LONG' (type: string), UDFToLong(_col9) (type: bigint), UDFToLong(_col10) (type: bigint), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 680 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest1_n0

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest1_n0

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest2

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: int, int, int
          Table: default.dest2

PREHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
PREHOOK: type: QUERY
PREHOOK: Input: default@input
PREHOOK: Output: default@dest1_n0
PREHOOK: Output: default@dest2
POSTHOOK: query: FROM INPUT
INSERT OVERWRITE TABLE dest1_n0 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@input
POSTHOOK: Output: default@dest1_n0
POSTHOOK: Output: default@dest2
POSTHOOK: Lineage: dest1_n0.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest1_n0.val2 EXPRESSION [(input)input.null, ]
POSTHOOK: Lineage: dest2.key SIMPLE [(input)input.FieldSchema(name:key, type:int, comment:null), ]
POSTHOOK: Lineage: dest2.val1 EXPRESSION [(input)input.FieldSchema(name:value, type:string, comment:null), ]
POSTHOOK: Lineage: dest2.val2 EXPRESSION [(input)input.null, ]
PREHOOK: query: SELECT * from dest1_n0
PREHOOK: type: QUERY
PREHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest1_n0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest1_n0
#### A masked pattern was here ####
128	128	1
150	150	1
165	165	1
193	193	1
213	427	2
224	224	1
238	717	3
255	255	1
265	265	1
27	27	1
273	273	1
278	278	1
311	311	1
369	369	1
401	401	1
409	409	1
484	484	1
66	66	1
86	86	1
98	98	1
PREHOOK: query: SELECT * from dest2
PREHOOK: type: QUERY
PREHOOK: Input: default@dest2
#### A masked pattern was here ####
POSTHOOK: query: SELECT * from dest2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2
#### A masked pattern was here ####
128	128	128
150	150	150
165	165	165
193	193	193
213	427	213
224	224	224
238	717	239
255	255	255
265	265	265
27	27	27
273	273	273
278	278	278
311	311	311
369	369	369
401	401	401
409	409	409
484	484	484
66	66	66
86	86	86
98	98	98
