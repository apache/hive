PREHOOK: query: CREATE TABLE dest1_n50(dummy STRING, key INT, value DOUBLE) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@dest1_n50
POSTHOOK: query: CREATE TABLE dest1_n50(dummy STRING, key INT, value DOUBLE) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dest1_n50
PREHOOK: query: EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1_n50 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest1_n50
POSTHOOK: query: EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1_n50 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest1_n50
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  filterExpr: (UDFToDouble(key) < 100.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) < 100.0D) (type: boolean)
                    Statistics: Num rows: 166 Data size: 14442 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 166 Data size: 15770 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 166 Data size: 15770 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 166 Data size: 15770 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: '1234' (type: string), UDFToInteger(_col0) (type: int), UDFToDouble(_col1) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 166 Data size: 16600 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 166 Data size: 16600 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest1_n50
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: int), _col2 (type: double)
                    outputColumnNames: dummy, key, value
                    Statistics: Num rows: 166 Data size: 16600 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(dummy)), avg(COALESCE(length(dummy),0)), count(1), count(dummy), compute_bit_vector_hll(dummy), min(key), max(key), count(key), compute_bit_vector_hll(key), min(value), max(value), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                      Statistics: Num rows: 1 Data size: 568 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 568 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary), _col9 (type: double), _col10 (type: double), _col11 (type: bigint), _col12 (type: binary)
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), min(VALUE._col9), max(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 500 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'DOUBLE' (type: string), _col9 (type: double), _col10 (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 796 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 796 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest1_n50

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: dummy, key, value
          Column Types: string, int, double
          Table: default.dest1_n50

PREHOOK: query: FROM src
INSERT OVERWRITE TABLE dest1_n50 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest1_n50
POSTHOOK: query: FROM src
INSERT OVERWRITE TABLE dest1_n50 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest1_n50
POSTHOOK: Lineage: dest1_n50.dummy SIMPLE []
POSTHOOK: Lineage: dest1_n50.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest1_n50.value EXPRESSION [(src)src.null, ]
PREHOOK: query: SELECT dest1_n50.* FROM dest1_n50
PREHOOK: type: QUERY
PREHOOK: Input: default@dest1_n50
#### A masked pattern was here ####
POSTHOOK: query: SELECT dest1_n50.* FROM dest1_n50
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest1_n50
#### A masked pattern was here ####
1234	0	3.0
1234	10	1.0
1234	11	1.0
1234	12	2.0
1234	15	2.0
1234	17	1.0
1234	18	2.0
1234	19	1.0
1234	2	1.0
1234	20	1.0
1234	24	2.0
1234	26	2.0
1234	27	1.0
1234	28	1.0
1234	30	1.0
1234	33	1.0
1234	34	1.0
1234	35	3.0
1234	37	2.0
1234	4	1.0
1234	41	1.0
1234	42	2.0
1234	43	1.0
1234	44	1.0
1234	47	1.0
1234	5	3.0
1234	51	2.0
1234	53	1.0
1234	54	1.0
1234	57	1.0
1234	58	2.0
1234	64	1.0
1234	65	1.0
1234	66	1.0
1234	67	2.0
1234	69	1.0
1234	70	3.0
1234	72	2.0
1234	74	1.0
1234	76	2.0
1234	77	1.0
1234	78	1.0
1234	8	1.0
1234	80	1.0
1234	82	1.0
1234	83	2.0
1234	84	2.0
1234	85	1.0
1234	86	1.0
1234	87	1.0
1234	9	1.0
1234	90	3.0
1234	92	1.0
1234	95	2.0
1234	96	1.0
1234	97	2.0
1234	98	2.0
