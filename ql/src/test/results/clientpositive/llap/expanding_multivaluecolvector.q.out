PREHOOK: query: create external table source (id int, str string) row format delimited fields terminated by ',' stored as textfile location '../../data/files/splittablecolumn/'
PREHOOK: type: CREATETABLE
#### A masked pattern was here ####
PREHOOK: Output: database:default
PREHOOK: Output: default@source
POSTHOOK: query: create external table source (id int, str string) row format delimited fields terminated by ',' stored as textfile location '../../data/files/splittablecolumn/'
POSTHOOK: type: CREATETABLE
#### A masked pattern was here ####
POSTHOOK: Output: database:default
POSTHOOK: Output: default@source
PREHOOK: query: create table table_with_list (id int, val array<string>) stored as orc
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table_with_list
POSTHOOK: query: create table table_with_list (id int, val array<string>) stored as orc
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table_with_list
PREHOOK: query: explain vectorization detail
insert overwrite table table_with_list select id, split(str, '#') from source
PREHOOK: type: QUERY
PREHOOK: Input: default@source
PREHOOK: Output: default@table_with_list
POSTHOOK: query: explain vectorization detail
insert overwrite table table_with_list select id, split(str, '#') from source
POSTHOOK: type: QUERY
POSTHOOK: Input: default@source
POSTHOOK: Output: default@table_with_list
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: source
                  Statistics: Num rows: 6088 Data size: 984180 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:id:int, 1:str:string, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: id (type: int), split(str, '#') (type: array<string>)
                    outputColumnNames: _col0, _col1
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 3]
                        selectExpressions: VectorUDFAdaptor(split(str, '#')) -> 3:array<string>
                    Statistics: Num rows: 6088 Data size: 984180 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 6088 Data size: 984180 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                          name: default.table_with_list
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: id:int, str:string
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [array<string>]

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.table_with_list

PREHOOK: query: insert overwrite table table_with_list select id, split(str, '#') from source
PREHOOK: type: QUERY
PREHOOK: Input: default@source
PREHOOK: Output: default@table_with_list
POSTHOOK: query: insert overwrite table table_with_list select id, split(str, '#') from source
POSTHOOK: type: QUERY
POSTHOOK: Input: default@source
POSTHOOK: Output: default@table_with_list
POSTHOOK: Lineage: table_with_list.id SIMPLE [(source)source.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: table_with_list.val EXPRESSION [(source)source.FieldSchema(name:str, type:string, comment:null), ]
PREHOOK: query: select * from table_with_list order by id desc limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@table_with_list
#### A masked pattern was here ####
POSTHOOK: query: select * from table_with_list order by id desc limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table_with_list
#### A masked pattern was here ####
9000	["start","mid","9000"]
8999	["start","mid","8999"]
8998	["start","mid","8998"]
8997	["start","mid","8997"]
8996	["start","mid","8996"]
8995	["start","mid","8995"]
8994	["start","mid","8994"]
8993	["start","mid","8993"]
8992	["start","mid","8992"]
8991	["start","mid","8991"]
PREHOOK: query: create table source_partitioned (id int, str string) partitioned by (part string)
row format delimited fields terminated by ',' stored as textfile
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@source_partitioned
POSTHOOK: query: create table source_partitioned (id int, str string) partitioned by (part string)
row format delimited fields terminated by ',' stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@source_partitioned
PREHOOK: query: load data local inpath '../../data/files/splittablecolumn/lines1to5000.txt'
overwrite into table source_partitioned partition (part='a')
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@source_partitioned
POSTHOOK: query: load data local inpath '../../data/files/splittablecolumn/lines1to5000.txt'
overwrite into table source_partitioned partition (part='a')
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@source_partitioned
POSTHOOK: Output: default@source_partitioned@part=a
PREHOOK: query: load data local inpath '../../data/files/splittablecolumn/lines5001to9000.txt'
overwrite into table source_partitioned partition (part='b')
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@source_partitioned
POSTHOOK: query: load data local inpath '../../data/files/splittablecolumn/lines5001to9000.txt'
overwrite into table source_partitioned partition (part='b')
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@source_partitioned
POSTHOOK: Output: default@source_partitioned@part=b
PREHOOK: query: create table table_partitioned (id int, val int) partitioned by (part string) stored as orc
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@table_partitioned
POSTHOOK: query: create table table_partitioned (id int, val int) partitioned by (part string) stored as orc
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@table_partitioned
PREHOOK: query: explain vectorization detail
insert overwrite table table_partitioned select id, split(str, '#')[2], part from source_partitioned
PREHOOK: type: QUERY
PREHOOK: Input: default@source_partitioned
PREHOOK: Input: default@source_partitioned@part=a
PREHOOK: Input: default@source_partitioned@part=b
PREHOOK: Output: default@table_partitioned
POSTHOOK: query: explain vectorization detail
insert overwrite table table_partitioned select id, split(str, '#')[2], part from source_partitioned
POSTHOOK: type: QUERY
POSTHOOK: Input: default@source_partitioned
POSTHOOK: Input: default@source_partitioned@part=a
POSTHOOK: Input: default@source_partitioned@part=b
POSTHOOK: Output: default@table_partitioned
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: source_partitioned
                  Statistics: Num rows: 4534 Data size: 1526284 Basic stats: PARTIAL Column stats: PARTIAL
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:id:int, 1:str:string, 2:part:string, 3:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: id (type: int), UDFToInteger(split(str, '#')[2]) (type: int), part (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 6, 2]
                        selectExpressions: CastStringToLong(col 5:string)(children: ListIndexColScalar(col 4:array<string>, col 2:int)(children: VectorUDFAdaptor(split(str, '#')) -> 4:array<string>) -> 5:string) -> 6:int
                    Statistics: Num rows: 4534 Data size: 1526284 Basic stats: PARTIAL Column stats: PARTIAL
                    Reduce Output Operator
                      key expressions: _col2 (type: string)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col2 (type: string)
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkStringOperator
                          keyColumns: 2:string
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumns: 0:int, 6:int
                      Statistics: Num rows: 4534 Data size: 1526284 Basic stats: PARTIAL Column stats: PARTIAL
                      value expressions: _col0 (type: int), _col1 (type: int)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: true
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: id:int, str:string
                    partitionColumnCount: 1
                    partitionColumns: part:string
                    scratchColumnTypeNames: [array<string>, string, bigint]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col2:string, VALUE._col0:int, VALUE._col1:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), VALUE._col1 (type: int), KEY._col2 (type: string)
                outputColumnNames: _col0, _col1, _col2
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [1, 2, 0]
                File Output Operator
                  compressed: false
                  Dp Sort State: PARTITION_SORTED
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 4534 Data size: 1526284 Basic stats: PARTIAL Column stats: PARTIAL
                  table:
                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      name: default.table_partitioned

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            part 
          replace: true
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.table_partitioned

PREHOOK: query: insert overwrite table table_partitioned select id, split(str, '#')[2], part from source_partitioned
PREHOOK: type: QUERY
PREHOOK: Input: default@source_partitioned
PREHOOK: Input: default@source_partitioned@part=a
PREHOOK: Input: default@source_partitioned@part=b
PREHOOK: Output: default@table_partitioned
POSTHOOK: query: insert overwrite table table_partitioned select id, split(str, '#')[2], part from source_partitioned
POSTHOOK: type: QUERY
POSTHOOK: Input: default@source_partitioned
POSTHOOK: Input: default@source_partitioned@part=a
POSTHOOK: Input: default@source_partitioned@part=b
POSTHOOK: Output: default@table_partitioned
POSTHOOK: Output: default@table_partitioned@part=a
POSTHOOK: Output: default@table_partitioned@part=b
POSTHOOK: Lineage: table_partitioned PARTITION(part=a).id SIMPLE [(source_partitioned)source_partitioned.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: table_partitioned PARTITION(part=a).val EXPRESSION [(source_partitioned)source_partitioned.FieldSchema(name:str, type:string, comment:null), ]
POSTHOOK: Lineage: table_partitioned PARTITION(part=b).id SIMPLE [(source_partitioned)source_partitioned.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: table_partitioned PARTITION(part=b).val EXPRESSION [(source_partitioned)source_partitioned.FieldSchema(name:str, type:string, comment:null), ]
PREHOOK: query: select * from table_partitioned where part='a' order by id desc limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@table_partitioned
PREHOOK: Input: default@table_partitioned@part=a
#### A masked pattern was here ####
POSTHOOK: query: select * from table_partitioned where part='a' order by id desc limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table_partitioned
POSTHOOK: Input: default@table_partitioned@part=a
#### A masked pattern was here ####
5000	5000	a
4999	4999	a
4998	4998	a
4997	4997	a
4996	4996	a
4995	4995	a
4994	4994	a
4993	4993	a
4992	4992	a
4991	4991	a
PREHOOK: query: select * from table_partitioned where part='b' order by id desc limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@table_partitioned
PREHOOK: Input: default@table_partitioned@part=b
#### A masked pattern was here ####
POSTHOOK: query: select * from table_partitioned where part='b' order by id desc limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table_partitioned
POSTHOOK: Input: default@table_partitioned@part=b
#### A masked pattern was here ####
9000	9000	b
8999	8999	b
8998	8998	b
8997	8997	b
8996	8996	b
8995	8995	b
8994	8994	b
8993	8993	b
8992	8992	b
8991	8991	b
PREHOOK: query: select count(*) from table_partitioned where part='a'
PREHOOK: type: QUERY
PREHOOK: Input: default@table_partitioned
PREHOOK: Input: default@table_partitioned@part=a
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from table_partitioned where part='a'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table_partitioned
POSTHOOK: Input: default@table_partitioned@part=a
#### A masked pattern was here ####
5000
PREHOOK: query: select count(*) from table_partitioned where part='b'
PREHOOK: type: QUERY
PREHOOK: Input: default@table_partitioned
PREHOOK: Input: default@table_partitioned@part=b
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from table_partitioned where part='b'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@table_partitioned
POSTHOOK: Input: default@table_partitioned@part=b
#### A masked pattern was here ####
4000
