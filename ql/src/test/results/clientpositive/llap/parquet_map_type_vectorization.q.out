PREHOOK: query: DROP TABLE parquet_map_type_staging
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE parquet_map_type_staging
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE parquet_map_type
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE parquet_map_type
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE parquet_map_type_staging (
id int,
stringMap map<string, string>,
intMap map<int, int>,
doubleMap map<double, double>,
stringIndex string,
intIndex int,
doubleIndex double
) ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '|'
  COLLECTION ITEMS TERMINATED BY ','
  MAP KEYS TERMINATED BY ':'
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@parquet_map_type_staging
POSTHOOK: query: CREATE TABLE parquet_map_type_staging (
id int,
stringMap map<string, string>,
intMap map<int, int>,
doubleMap map<double, double>,
stringIndex string,
intIndex int,
doubleIndex double
) ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '|'
  COLLECTION ITEMS TERMINATED BY ','
  MAP KEYS TERMINATED BY ':'
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@parquet_map_type_staging
PREHOOK: query: CREATE TABLE parquet_map_type (
id int,
stringMap map<string, string>,
intMap map<int, int>,
doubleMap map<double, double>,
stringIndex string,
intIndex int,
doubleIndex double
) STORED AS PARQUET
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@parquet_map_type
POSTHOOK: query: CREATE TABLE parquet_map_type (
id int,
stringMap map<string, string>,
intMap map<int, int>,
doubleMap map<double, double>,
stringIndex string,
intIndex int,
doubleIndex double
) STORED AS PARQUET
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@parquet_map_type
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_vector_map_type.txt' OVERWRITE INTO TABLE parquet_map_type_staging
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@parquet_map_type_staging
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/parquet_vector_map_type.txt' OVERWRITE INTO TABLE parquet_map_type_staging
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@parquet_map_type_staging
PREHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging where id < 1024
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type_staging
PREHOOK: Output: default@parquet_map_type
POSTHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging where id < 1024
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type_staging
POSTHOOK: Output: default@parquet_map_type
POSTHOOK: Lineage: parquet_map_type.doubleindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doubleindex, type:double, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.doublemap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doublemap, type:map<double,double>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.id SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intindex, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intmap, type:map<int,int>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringindex, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringmap, type:map<string,string>, comment:null), ]
PREHOOK: query: select count(*) from parquet_map_type
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from parquet_map_type
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
1023
PREHOOK: query: explain vectorization expression select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456],
doubleMap[123.123], stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain vectorization expression select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456],
doubleMap[123.123], stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: parquet_map_type
                  Statistics: Num rows: 1023 Data size: 2183412 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                  Select Operator
                    expressions: stringmap (type: map<string,string>), intmap (type: map<int,int>), doublemap (type: map<double,double>), stringmap['k2'] (type: string), intmap[456] (type: int), doublemap[123.123] (type: double), stringmap[stringindex] (type: string), intmap[intindex] (type: int), doublemap[doubleindex] (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [1, 2, 3, 8, 9, 10, 11, 12, 13]
                        selectExpressions: VectorUDFMapIndexStringScalar(col 1:map<string,string>, key: k2) -> 8:string, VectorUDFMapIndexLongScalar(col 2:map<int,int>, key: 456) -> 9:int, VectorUDFMapIndexDoubleScalar(col 3:map<double,double>, key: 123.123) -> 10:double, VectorUDFMapIndexStringCol(col 1:map<string,string>, key: col 4:string) -> 11:string, VectorUDFMapIndexLongCol(col 2:map<int,int>, key: col 5:int) -> 12:int, VectorUDFMapIndexDoubleCol(col 3:map<double,double>, key: col 6:double) -> 13:double
                    Statistics: Num rows: 1023 Data size: 2183412 Basic stats: COMPLETE Column stats: NONE
                    Limit
                      Number of rows: 10
                      Limit Vectorization:
                          className: VectorLimitOperator
                          native: true
                      Statistics: Num rows: 10 Data size: 21340 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 10 Data size: 21340 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs (cache only)
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true

  Stage: Stage-0
    Fetch Operator
      limit: 10
      Processor Tree:
        ListSink

PREHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
{"k1":"v1","k2":"v1-2"}	{123:1,456:2}	{123.123:1.1,456.456:1.2}	v1-2	2	1.1	v1	1	1.2
{"k1":"v2","k2":"v2-2"}	{123:3,456:4}	{123.123:2.1,456.456:2.2}	v2-2	4	2.1	v2	3	2.2
{"k1":"v3","k2":"v3-2"}	{123:5,456:6}	{123.123:3.1,456.456:3.2}	v3-2	6	3.1	v3	5	3.2
{"k1":"v4","k2":"v4-2"}	{123:7,456:8}	{123.123:4.1,456.456:4.2}	v4-2	8	4.1	v4	7	4.2
{"k1":"v5","k2":"v5-2"}	{123:9,456:10}	{123.123:5.1,456.456:5.2}	v5-2	10	5.1	v5	9	5.2
{"k1":"v6","k2":"v6-2"}	{123:11,456:12}	{123.123:6.1,456.456:6.2}	v6-2	12	6.1	v6	11	6.2
{"k1":"v7","k2":"v7-2"}	{123:13,456:14}	{123.123:7.1,456.456:7.2}	v7-2	14	7.1	v7	13	7.2
{"k1":"v8","k2":"v8-2"}	{123:15,456:16}	{123.123:8.1,456.456:8.2}	v8-2	16	8.1	v8	15	8.2
{"k1":"v9","k2":"v9-2"}	{123:17,456:18}	{123.123:9.1,456.456:9.2}	v9-2	18	9.1	v9	17	9.2
{"k1":"v10","k2":"v10-2"}	{123:19,456:20}	{123.123:10.1,456.456:10.2}	v10-2	20	10.1	v10	19	10.2
PREHOOK: query: explain vectorization expression select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain vectorization expression select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: parquet_map_type
                  Statistics: Num rows: 1023 Data size: 1992704 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterStringColLikeStringScalar(col 8:string, pattern v100%)(children: VectorUDFMapIndexStringScalar(col 1:map<string,string>, key: k1) -> 8:string)
                    predicate: (stringmap['k1'] like 'v100%') (type: boolean)
                    Statistics: Num rows: 511 Data size: 995378 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: stringmap['k1'] (type: string), intmap[123] (type: int), doublemap[123.123] (type: double)
                      outputColumnNames: _col0, _col1, _col2
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [8, 9, 10]
                          selectExpressions: VectorUDFMapIndexStringScalar(col 1:map<string,string>, key: k1) -> 8:string, VectorUDFMapIndexLongScalar(col 2:map<int,int>, key: 123) -> 9:int, VectorUDFMapIndexDoubleScalar(col 3:map<double,double>, key: 123.123) -> 10:double
                      Statistics: Num rows: 511 Data size: 995378 Basic stats: COMPLETE Column stats: NONE
                      Top N Key Operator
                        sort order: +
                        keys: _col0 (type: string)
                        Statistics: Num rows: 511 Data size: 995378 Basic stats: COMPLETE Column stats: NONE
                        top n: 10
                        Top N Key Vectorization:
                            className: VectorTopNKeyOperator
                            keyExpressions: col 8:string
                            native: true
                        Group By Operator
                          aggregations: sum(_col1), sum(_col2)
                          Group By Vectorization:
                              aggregators: VectorUDAFSumLong(col 9:int) -> bigint, VectorUDAFSumDouble(col 10:double) -> double
                              className: VectorGroupByOperator
                              groupByMode: HASH
                              keyExpressions: col 8:string
                              native: false
                              vectorProcessingMode: HASH
                              projectedOutputColumnNums: [0, 1]
                          keys: _col0 (type: string)
                          mode: hash
                          outputColumnNames: _col0, _col1, _col2
                          Statistics: Num rows: 511 Data size: 995378 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: string)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: string)
                            Reduce Sink Vectorization:
                                className: VectorReduceSinkStringOperator
                                native: true
                                nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            Statistics: Num rows: 511 Data size: 995378 Basic stats: COMPLETE Column stats: NONE
                            TopN Hash Memory Usage: 0.1
                            value expressions: _col1 (type: bigint), _col2 (type: double)
            Execution mode: vectorized, llap
            LLAP IO: all inputs (cache only)
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), sum(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFSumLong(col 1:bigint) -> bigint, VectorUDAFSumDouble(col 2:double) -> double
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:string
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 255 Data size: 496715 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col1 (type: bigint), _col2 (type: double), _col0 (type: string)
                  outputColumnNames: _col0, _col1, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [1, 2, 0]
                  Statistics: Num rows: 255 Data size: 496715 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col3 (type: string)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                    Statistics: Num rows: 255 Data size: 496715 Basic stats: COMPLETE Column stats: NONE
                    TopN Hash Memory Usage: 0.1
                    value expressions: _col0 (type: bigint), _col1 (type: double)
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: bigint), VALUE._col1 (type: double), KEY.reducesinkkey0 (type: string)
                outputColumnNames: _col0, _col1, _col2
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [1, 2, 0]
                Statistics: Num rows: 255 Data size: 496715 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 10
                  Limit Vectorization:
                      className: VectorLimitOperator
                      native: true
                  Statistics: Num rows: 10 Data size: 19470 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 10 Data size: 19470 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
199	100.1	v100
1999	1000.1	v1000
2001	1001.1	v1001
2003	1002.1	v1002
2005	1003.1	v1003
2007	1004.1	v1004
2009	1005.1	v1005
2011	1006.1	v1006
2013	1007.1	v1007
2015	1008.1	v1008
PREHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging where id < 1025
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type_staging
PREHOOK: Output: default@parquet_map_type
POSTHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging where id < 1025
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type_staging
POSTHOOK: Output: default@parquet_map_type
POSTHOOK: Lineage: parquet_map_type.doubleindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doubleindex, type:double, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.doublemap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doublemap, type:map<double,double>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.id SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intindex, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intmap, type:map<int,int>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringindex, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringmap, type:map<string,string>, comment:null), ]
PREHOOK: query: select count(*) from parquet_map_type
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from parquet_map_type
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
1024
PREHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
{"k1":"v1","k2":"v1-2"}	{123:1,456:2}	{123.123:1.1,456.456:1.2}	v1-2	2	1.1	v1	1	1.2
{"k1":"v2","k2":"v2-2"}	{123:3,456:4}	{123.123:2.1,456.456:2.2}	v2-2	4	2.1	v2	3	2.2
{"k1":"v3","k2":"v3-2"}	{123:5,456:6}	{123.123:3.1,456.456:3.2}	v3-2	6	3.1	v3	5	3.2
{"k1":"v4","k2":"v4-2"}	{123:7,456:8}	{123.123:4.1,456.456:4.2}	v4-2	8	4.1	v4	7	4.2
{"k1":"v5","k2":"v5-2"}	{123:9,456:10}	{123.123:5.1,456.456:5.2}	v5-2	10	5.1	v5	9	5.2
{"k1":"v6","k2":"v6-2"}	{123:11,456:12}	{123.123:6.1,456.456:6.2}	v6-2	12	6.1	v6	11	6.2
{"k1":"v7","k2":"v7-2"}	{123:13,456:14}	{123.123:7.1,456.456:7.2}	v7-2	14	7.1	v7	13	7.2
{"k1":"v8","k2":"v8-2"}	{123:15,456:16}	{123.123:8.1,456.456:8.2}	v8-2	16	8.1	v8	15	8.2
{"k1":"v9","k2":"v9-2"}	{123:17,456:18}	{123.123:9.1,456.456:9.2}	v9-2	18	9.1	v9	17	9.2
{"k1":"v10","k2":"v10-2"}	{123:19,456:20}	{123.123:10.1,456.456:10.2}	v10-2	20	10.1	v10	19	10.2
PREHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
199	100.1	v100
1999	1000.1	v1000
2001	1001.1	v1001
2003	1002.1	v1002
2005	1003.1	v1003
2007	1004.1	v1004
2009	1005.1	v1005
2011	1006.1	v1006
2013	1007.1	v1007
2015	1008.1	v1008
PREHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type_staging
PREHOOK: Output: default@parquet_map_type
POSTHOOK: query: INSERT OVERWRITE TABLE parquet_map_type
SELECT id, stringMap, intMap, doubleMap, stringIndex, intIndex, doubleIndex FROM parquet_map_type_staging
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type_staging
POSTHOOK: Output: default@parquet_map_type
POSTHOOK: Lineage: parquet_map_type.doubleindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doubleindex, type:double, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.doublemap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:doublemap, type:map<double,double>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.id SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:id, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intindex, type:int, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.intmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:intmap, type:map<int,int>, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringindex SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringindex, type:string, comment:null), ]
POSTHOOK: Lineage: parquet_map_type.stringmap SIMPLE [(parquet_map_type_staging)parquet_map_type_staging.FieldSchema(name:stringmap, type:map<string,string>, comment:null), ]
PREHOOK: query: select count(*) from parquet_map_type
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select count(*) from parquet_map_type
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
1025
PREHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select stringMap, intMap, doubleMap, stringMap['k2'], intMap[456], doubleMap[123.123],
stringMap[stringIndex], intMap[intIndex], doubleMap[doubleIndex] from parquet_map_type limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
{"k1":"v1","k2":"v1-2"}	{123:1,456:2}	{123.123:1.1,456.456:1.2}	v1-2	2	1.1	v1	1	1.2
{"k1":"v2","k2":"v2-2"}	{123:3,456:4}	{123.123:2.1,456.456:2.2}	v2-2	4	2.1	v2	3	2.2
{"k1":"v3","k2":"v3-2"}	{123:5,456:6}	{123.123:3.1,456.456:3.2}	v3-2	6	3.1	v3	5	3.2
{"k1":"v4","k2":"v4-2"}	{123:7,456:8}	{123.123:4.1,456.456:4.2}	v4-2	8	4.1	v4	7	4.2
{"k1":"v5","k2":"v5-2"}	{123:9,456:10}	{123.123:5.1,456.456:5.2}	v5-2	10	5.1	v5	9	5.2
{"k1":"v6","k2":"v6-2"}	{123:11,456:12}	{123.123:6.1,456.456:6.2}	v6-2	12	6.1	v6	11	6.2
{"k1":"v7","k2":"v7-2"}	{123:13,456:14}	{123.123:7.1,456.456:7.2}	v7-2	14	7.1	v7	13	7.2
{"k1":"v8","k2":"v8-2"}	{123:15,456:16}	{123.123:8.1,456.456:8.2}	v8-2	16	8.1	v8	15	8.2
{"k1":"v9","k2":"v9-2"}	{123:17,456:18}	{123.123:9.1,456.456:9.2}	v9-2	18	9.1	v9	17	9.2
{"k1":"v10","k2":"v10-2"}	{123:19,456:20}	{123.123:10.1,456.456:10.2}	v10-2	20	10.1	v10	19	10.2
PREHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
PREHOOK: type: QUERY
PREHOOK: Input: default@parquet_map_type
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select sum(intMap[123]), sum(doubleMap[123.123]), stringMap['k1']
from parquet_map_type where stringMap['k1'] like 'v100%' group by stringMap['k1'] order by stringMap['k1'] limit 10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@parquet_map_type
POSTHOOK: Output: hdfs://### HDFS PATH ###
199	100.1	v100
1999	1000.1	v1000
2001	1001.1	v1001
2003	1002.1	v1002
2005	1003.1	v1003
2007	1004.1	v1004
2009	1005.1	v1005
2011	1006.1	v1006
2013	1007.1	v1007
2015	1008.1	v1008
