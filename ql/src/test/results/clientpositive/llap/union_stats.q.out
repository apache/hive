PREHOOK: query: explain extended create table t as select * from src union all select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: explain extended create table t as select * from src union all select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
OPTIMIZED SQL: SELECT `key`, `value`
FROM `default`.`src`
UNION ALL
SELECT `key`, `value`
FROM `default`.`src`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-4 depends on stages: Stage-2, Stage-0
  Stage-3 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 3 <- Union 2 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 1
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 1000 Data size: 178000 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            columns key,value
                            columns.types string:string
                            name default.t
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t
                      TotalFiles: 1
                      GatherStats: true
                      MultiFileSpray: false
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: src
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,value
                    columns.comments 'default','default'
                    columns.types string:string
#### A masked pattern was here ####
                    name default.src
                    numFiles 1
                    numRows 500
                    rawDataSize 5312
                    serialization.ddl struct src { string key, string value}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 5812
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,value
                      columns.comments 'default','default'
                      columns.types string:string
#### A masked pattern was here ####
                      name default.src
                      numFiles 1
                      numRows 500
                      rawDataSize 5312
                      serialization.ddl struct src { string key, string value}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 5812
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src
                  name: default.src
            Truncated Path -> Alias:
              /src [src]
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 1
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 1000 Data size: 178000 Basic stats: COMPLETE Column stats: COMPLETE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            columns key,value
                            columns.types string:string
                            name default.t
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t
                      TotalFiles: 1
                      GatherStats: true
                      MultiFileSpray: false
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: src
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,value
                    columns.comments 'default','default'
                    columns.types string:string
#### A masked pattern was here ####
                    name default.src
                    numFiles 1
                    numRows 500
                    rawDataSize 5312
                    serialization.ddl struct src { string key, string value}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    totalSize 5812
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}
                      bucket_count -1
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,value
                      columns.comments 'default','default'
                      columns.types string:string
#### A masked pattern was here ####
                      name default.src
                      numFiles 1
                      numRows 500
                      rawDataSize 5312
                      serialization.ddl struct src { string key, string value}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      totalSize 5812
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src
                  name: default.src
            Truncated Path -> Alias:
              /src [src]
        Union 2 
            Vertex: Union 2

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-4
      Create Table Operator:
        Create Table
          columns: key string, value string
          input format: org.apache.hadoop.mapred.TextInputFormat
          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          name: default.t

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
#### A masked pattern was here ####

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

PREHOOK: query: create table t as select * from src union all select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@t
POSTHOOK: query: create table t as select * from src union all select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t
POSTHOOK: Lineage: t.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select count(1) from t
PREHOOK: type: QUERY
PREHOOK: Input: default@t
#### A masked pattern was here ####
POSTHOOK: query: select count(1) from t
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t
#### A masked pattern was here ####
1000
PREHOOK: query: desc formatted t
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@t
POSTHOOK: query: desc formatted t
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@t
# col_name            	data_type           	comment             
key                 	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	bucketing_version   	2                   
	numFiles            	2                   
	numRows             	1000                
	rawDataSize         	10624               
	totalSize           	11624               
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: create table tt as select * from t union all select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Input: default@t
PREHOOK: Output: database:default
PREHOOK: Output: default@tt
POSTHOOK: query: create table tt as select * from t union all select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Input: default@t
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tt
POSTHOOK: Lineage: tt.key EXPRESSION [(t)t.FieldSchema(name:key, type:string, comment:null), (src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: tt.value EXPRESSION [(t)t.FieldSchema(name:value, type:string, comment:null), (src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: desc formatted tt
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@tt
POSTHOOK: query: desc formatted tt
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@tt
# col_name            	data_type           	comment             
key                 	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	bucketing_version   	2                   
	numFiles            	2                   
	numRows             	1500                
	rawDataSize         	15936               
	totalSize           	17436               
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: drop table tt
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@tt
PREHOOK: Output: default@tt
POSTHOOK: query: drop table tt
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@tt
POSTHOOK: Output: default@tt
PREHOOK: query: create table tt as select * from src union all select * from t
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Input: default@t
PREHOOK: Output: database:default
PREHOOK: Output: default@tt
POSTHOOK: query: create table tt as select * from src union all select * from t
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Input: default@t
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tt
POSTHOOK: Lineage: tt.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), (t)t.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: tt.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), (t)t.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: desc formatted tt
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@tt
POSTHOOK: query: desc formatted tt
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@tt
# col_name            	data_type           	comment             
key                 	string              	                    
value               	string              	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	bucketing_version   	2                   
	numFiles            	2                   
	numRows             	1500                
	rawDataSize         	15936               
	totalSize           	17436               
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: create table t1 like src
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1
POSTHOOK: query: create table t1 like src
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1
PREHOOK: query: create table t2 like src
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t2
POSTHOOK: query: create table t2 like src
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t2
PREHOOK: query: create table t3 like src
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t3
POSTHOOK: query: create table t3 like src
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t3
PREHOOK: query: explain from (select * from src union all select * from src)s
insert overwrite table t1 select *
insert overwrite table t2 select *
insert overwrite table t3 select *
PREHOOK: type: QUERY
POSTHOOK: query: explain from (select * from src union all select * from src)s
insert overwrite table t1 select *
insert overwrite table t2 select *
insert overwrite table t3 select *
POSTHOOK: type: QUERY
Plan optimized by CBO.

Vertex dependency in root stage
Map 1 <- Union 2 (CONTAINS)
Map 6 <- Union 2 (CONTAINS)
Reducer 3 <- Union 2 (CUSTOM_SIMPLE_EDGE)
Reducer 4 <- Union 2 (CUSTOM_SIMPLE_EDGE)
Reducer 5 <- Union 2 (CUSTOM_SIMPLE_EDGE)

Stage-5
  Stats Work{}
    Stage-0
      Move Operator
        table:{"name:":"default.t1"}
        Stage-4
          Dependency Collection{}
            Stage-3
              Reducer 3 llap
              File Output Operator [FS_14]
                Group By Operator [GBY_12] (rows=1 width=880)
                  Output:["_col0","_col1"],aggregations:["compute_stats(VALUE._col0)","compute_stats(VALUE._col1)"]
                <-Union 2 [CUSTOM_SIMPLE_EDGE]
                  <-Map 1 [CONTAINS] llap
                    File Output Operator [FS_36]
                      table:{"name:":"default.t1"}
                      Select Operator [SEL_34] (rows=500 width=178)
                        Output:["_col0","_col1"]
                        TableScan [TS_33] (rows=500 width=178)
                          Output:["key","value"]
                    Reduce Output Operator [RS_45]
                      Group By Operator [GBY_42] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_37] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_34]
                    File Output Operator [FS_38]
                      table:{"name:":"default.t2"}
                       Please refer to the previous Select Operator [SEL_34]
                    Reduce Output Operator [RS_46]
                      Group By Operator [GBY_43] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_39] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_34]
                    File Output Operator [FS_40]
                      table:{"name:":"default.t3"}
                       Please refer to the previous Select Operator [SEL_34]
                    Reduce Output Operator [RS_47]
                      Group By Operator [GBY_44] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_41] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_34]
                  <-Map 6 [CONTAINS] llap
                    File Output Operator [FS_51]
                      table:{"name:":"default.t1"}
                      Select Operator [SEL_49] (rows=500 width=178)
                        Output:["_col0","_col1"]
                        TableScan [TS_48] (rows=500 width=178)
                          Output:["key","value"]
                    Reduce Output Operator [RS_60]
                      Group By Operator [GBY_57] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_52] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_49]
                    File Output Operator [FS_53]
                      table:{"name:":"default.t2"}
                       Please refer to the previous Select Operator [SEL_49]
                    Reduce Output Operator [RS_61]
                      Group By Operator [GBY_58] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_54] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_49]
                    File Output Operator [FS_55]
                      table:{"name:":"default.t3"}
                       Please refer to the previous Select Operator [SEL_49]
                    Reduce Output Operator [RS_62]
                      Group By Operator [GBY_59] (rows=1 width=880)
                        Output:["_col0","_col1"],aggregations:["compute_stats(key, 'hll')","compute_stats(value, 'hll')"]
                        Select Operator [SEL_56] (rows=1000 width=178)
                          Output:["key","value"]
                           Please refer to the previous Select Operator [SEL_49]
              Reducer 4 llap
              File Output Operator [FS_23]
                Group By Operator [GBY_21] (rows=1 width=880)
                  Output:["_col0","_col1"],aggregations:["compute_stats(VALUE._col0)","compute_stats(VALUE._col1)"]
                <- Please refer to the previous Union 2 [CUSTOM_SIMPLE_EDGE]
              Reducer 5 llap
              File Output Operator [FS_32]
                Group By Operator [GBY_30] (rows=1 width=880)
                  Output:["_col0","_col1"],aggregations:["compute_stats(VALUE._col0)","compute_stats(VALUE._col1)"]
                <- Please refer to the previous Union 2 [CUSTOM_SIMPLE_EDGE]
Stage-6
  Stats Work{}
    Stage-1
      Move Operator
        table:{"name:":"default.t2"}
         Please refer to the previous Stage-4
Stage-7
  Stats Work{}
    Stage-2
      Move Operator
        table:{"name:":"default.t3"}
         Please refer to the previous Stage-4

PREHOOK: query: from (select * from src union all select * from src)s
insert overwrite table t1 select *
insert overwrite table t2 select *
insert overwrite table t3 select *
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@t1
PREHOOK: Output: default@t2
PREHOOK: Output: default@t3
POSTHOOK: query: from (select * from src union all select * from src)s
insert overwrite table t1 select *
insert overwrite table t2 select *
insert overwrite table t3 select *
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@t1
POSTHOOK: Output: default@t2
POSTHOOK: Output: default@t3
POSTHOOK: Lineage: t1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t1.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: t2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t2.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: t3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t3.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: desc formatted t1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@t1
POSTHOOK: query: desc formatted t1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@t1
# col_name            	data_type           	comment             
key                 	string              	default             
value               	string              	default             
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"key\":\"true\",\"value\":\"true\"}}
	numFiles            	2                   
	numRows             	1000                
	rawDataSize         	10624               
	totalSize           	11624               
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: desc formatted t2
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@t2
POSTHOOK: query: desc formatted t2
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@t2
# col_name            	data_type           	comment             
key                 	string              	default             
value               	string              	default             
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"key\":\"true\",\"value\":\"true\"}}
	numFiles            	2                   
	numRows             	1000                
	rawDataSize         	10624               
	totalSize           	11624               
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: select count(1) from t1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: select count(1) from t1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
1000
