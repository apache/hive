PREHOOK: query: CREATE TABLE lineitem_trs (
   l_orderkey BIGINT,
   l_partkey BIGINT,
   l_suppkey BIGINT,
   l_linenumber INT,
   l_quantity DECIMAL(12,2),
   l_extendedprice DECIMAL(12,2),
   l_discount DECIMAL(12,2),
   l_tax DECIMAL(12,2),
   l_returnflag STRING,
   l_linestatus STRING,
   l_shipdate STRING,
   l_commitdate STRING,
   l_receiptdate STRING,
   l_shipinstruct STRING,
   l_shipmode STRING,
   l_comment STRING
) STORED AS ORC
TBLPROPERTIES('transactional'='true')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@lineitem_trs
POSTHOOK: query: CREATE TABLE lineitem_trs (
   l_orderkey BIGINT,
   l_partkey BIGINT,
   l_suppkey BIGINT,
   l_linenumber INT,
   l_quantity DECIMAL(12,2),
   l_extendedprice DECIMAL(12,2),
   l_discount DECIMAL(12,2),
   l_tax DECIMAL(12,2),
   l_returnflag STRING,
   l_linestatus STRING,
   l_shipdate STRING,
   l_commitdate STRING,
   l_receiptdate STRING,
   l_shipinstruct STRING,
   l_shipmode STRING,
   l_comment STRING
) STORED AS ORC
TBLPROPERTIES('transactional'='true')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@lineitem_trs
PREHOOK: query: CREATE TABLE unique_lineitem_stage (
  l_orderkey STRING,
  l_partkey BIGINT,
  l_suppkey BIGINT,
  l_linenumber INT,
  l_quantity DECIMAL(12,2),
  l_extendedprice DECIMAL(12,2),
  l_discount DECIMAL(12,2),
  l_tax DECIMAL(12,2),
  l_returnflag STRING,
  l_linestatus STRING,
  l_shipdate STRING,
  l_commitdate STRING,
  l_receiptdate STRING,
  l_shipinstruct STRING,
  l_shipmode STRING,
  l_comment STRING
) STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@unique_lineitem_stage
POSTHOOK: query: CREATE TABLE unique_lineitem_stage (
  l_orderkey STRING,
  l_partkey BIGINT,
  l_suppkey BIGINT,
  l_linenumber INT,
  l_quantity DECIMAL(12,2),
  l_extendedprice DECIMAL(12,2),
  l_discount DECIMAL(12,2),
  l_tax DECIMAL(12,2),
  l_returnflag STRING,
  l_linestatus STRING,
  l_shipdate STRING,
  l_commitdate STRING,
  l_receiptdate STRING,
  l_shipinstruct STRING,
  l_shipmode STRING,
  l_comment STRING
) STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@unique_lineitem_stage
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
merge into lineitem_trs using (
  select * from lineitem_trs
  where L_ORDERKEY in(
    select L_ORDERKEY
    from unique_lineitem_stage
  )
) sub on sub.L_ORDERKEY = lineitem_trs.L_ORDERKEY
  when matched then update set l_linenumber=8
  when not matched then insert values (
    sub.L_ORDERKEY, sub.L_PARTKEY, sub.L_SUPPKEY, sub.L_LINENUMBER, sub.L_QUANTITY, sub.L_EXTENDEDPRICE,
    sub.L_DISCOUNT, sub.L_TAX, sub.L_RETURNFLAG, sub.L_LINESTATUS, sub.L_SHIPDATE, sub.L_COMMITDATE,
    sub.L_RECEIPTDATE, sub.L_SHIPINSTRUCT,sub.L_SHIPMODE, sub.L_COMMENT
  )
PREHOOK: type: QUERY
PREHOOK: Input: default@lineitem_trs
PREHOOK: Input: default@unique_lineitem_stage
PREHOOK: Output: default@lineitem_trs
PREHOOK: Output: default@lineitem_trs
PREHOOK: Output: default@merge_tmp_table
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
merge into lineitem_trs using (
  select * from lineitem_trs
  where L_ORDERKEY in(
    select L_ORDERKEY
    from unique_lineitem_stage
  )
) sub on sub.L_ORDERKEY = lineitem_trs.L_ORDERKEY
  when matched then update set l_linenumber=8
  when not matched then insert values (
    sub.L_ORDERKEY, sub.L_PARTKEY, sub.L_SUPPKEY, sub.L_LINENUMBER, sub.L_QUANTITY, sub.L_EXTENDEDPRICE,
    sub.L_DISCOUNT, sub.L_TAX, sub.L_RETURNFLAG, sub.L_LINESTATUS, sub.L_SHIPDATE, sub.L_COMMITDATE,
    sub.L_RECEIPTDATE, sub.L_SHIPINSTRUCT,sub.L_SHIPMODE, sub.L_COMMENT
  )
POSTHOOK: type: QUERY
POSTHOOK: Input: default@lineitem_trs
POSTHOOK: Input: default@unique_lineitem_stage
POSTHOOK: Output: default@lineitem_trs
POSTHOOK: Output: default@lineitem_trs
POSTHOOK: Output: default@merge_tmp_table
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-4 depends on stages: Stage-3
  Stage-0 depends on stages: Stage-4
  Stage-5 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-4
  Stage-6 depends on stages: Stage-1
  Stage-2 depends on stages: Stage-4
  Stage-7 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-3
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 3 <- Map 1 (BROADCAST_EDGE), Map 2 (SIMPLE_EDGE), Map 6 (BROADCAST_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: lineitem_trs
                  filterExpr: UDFToDouble(l_orderkey) is not null (type: boolean)
                  Statistics: Num rows: 1 Data size: 1948 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:l_orderkey:bigint, 1:l_partkey:bigint, 2:l_suppkey:bigint, 3:l_linenumber:int, 4:l_quantity:decimal(12,2)/DECIMAL_64, 5:l_extendedprice:decimal(12,2)/DECIMAL_64, 6:l_discount:decimal(12,2)/DECIMAL_64, 7:l_tax:decimal(12,2)/DECIMAL_64, 8:l_returnflag:string, 9:l_linestatus:string, 10:l_shipdate:string, 11:l_commitdate:string, 12:l_receiptdate:string, 13:l_shipinstruct:string, 14:l_shipmode:string, 15:l_comment:string, 16:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 17:double)(children: CastLongToDouble(col 0:bigint) -> 17:double)
                    predicate: UDFToDouble(l_orderkey) is not null (type: boolean)
                    Statistics: Num rows: 1 Data size: 1948 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: l_orderkey (type: bigint), l_partkey (type: bigint), l_suppkey (type: bigint), l_linenumber (type: int), l_quantity (type: decimal(12,2)), l_extendedprice (type: decimal(12,2)), l_discount (type: decimal(12,2)), l_tax (type: decimal(12,2)), l_returnflag (type: string), l_linestatus (type: string), l_shipdate (type: string), l_commitdate (type: string), l_receiptdate (type: string), l_shipinstruct (type: string), l_shipmode (type: string), l_comment (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                      Statistics: Num rows: 1 Data size: 1948 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: UDFToDouble(_col0) (type: double)
                        null sort order: a
                        sort order: +
                        Map-reduce partition columns: UDFToDouble(_col0) (type: double)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkMultiKeyOperator
                            keyColumns: 17:double
                            keyExpressions: CastLongToDouble(col 0:bigint) -> 17:double
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumns: 0:bigint, 1:bigint, 2:bigint, 3:int, 4:decimal(12,2), 5:decimal(12,2), 6:decimal(12,2), 7:decimal(12,2), 8:string, 9:string, 10:string, 11:string, 12:string, 13:string, 14:string, 15:string
                        Statistics: Num rows: 1 Data size: 1948 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: int), _col4 (type: decimal(12,2)), _col5 (type: decimal(12,2)), _col6 (type: decimal(12,2)), _col7 (type: decimal(12,2)), _col8 (type: string), _col9 (type: string), _col10 (type: string), _col11 (type: string), _col12 (type: string), _col13 (type: string), _col14 (type: string), _col15 (type: string)
            Execution mode: vectorized, llap
            LLAP IO: may be used (ACID table)
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 16
                    includeColumns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    dataColumns: l_orderkey:bigint, l_partkey:bigint, l_suppkey:bigint, l_linenumber:int, l_quantity:decimal(12,2)/DECIMAL_64, l_extendedprice:decimal(12,2)/DECIMAL_64, l_discount:decimal(12,2)/DECIMAL_64, l_tax:decimal(12,2)/DECIMAL_64, l_returnflag:string, l_linestatus:string, l_shipdate:string, l_commitdate:string, l_receiptdate:string, l_shipinstruct:string, l_shipmode:string, l_comment:string
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: unique_lineitem_stage
                  filterExpr: UDFToDouble(l_orderkey) is not null (type: boolean)
                  Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:l_orderkey:string, 1:l_partkey:bigint, 2:l_suppkey:bigint, 3:l_linenumber:int, 4:l_quantity:decimal(12,2)/DECIMAL_64, 5:l_extendedprice:decimal(12,2)/DECIMAL_64, 6:l_discount:decimal(12,2)/DECIMAL_64, 7:l_tax:decimal(12,2)/DECIMAL_64, 8:l_returnflag:string, 9:l_linestatus:string, 10:l_shipdate:string, 11:l_commitdate:string, 12:l_receiptdate:string, 13:l_shipinstruct:string, 14:l_shipmode:string, 15:l_comment:string, 16:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: SelectColumnIsNotNull(col 17:double)(children: CastStringToDouble(col 0:string) -> 17:double)
                    predicate: UDFToDouble(l_orderkey) is not null (type: boolean)
                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      Group By Vectorization:
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 0:string
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: []
                      keys: l_orderkey (type: string)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: a
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkStringOperator
                            keyColumns: 0:string
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 16
                    includeColumns: [0]
                    dataColumns: l_orderkey:string, l_partkey:bigint, l_suppkey:bigint, l_linenumber:int, l_quantity:decimal(12,2)/DECIMAL_64, l_extendedprice:decimal(12,2)/DECIMAL_64, l_discount:decimal(12,2)/DECIMAL_64, l_tax:decimal(12,2)/DECIMAL_64, l_returnflag:string, l_linestatus:string, l_shipdate:string, l_commitdate:string, l_receiptdate:string, l_shipinstruct:string, l_shipmode:string, l_comment:string
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: lineitem_trs
                  filterExpr: (l_orderkey is not null and UDFToDouble(l_orderkey) is not null) (type: boolean)
                  Statistics: Num rows: 1 Data size: 1944 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:l_orderkey:bigint, 1:l_partkey:bigint, 2:l_suppkey:bigint, 3:l_linenumber:int, 4:l_quantity:decimal(12,2)/DECIMAL_64, 5:l_extendedprice:decimal(12,2)/DECIMAL_64, 6:l_discount:decimal(12,2)/DECIMAL_64, 7:l_tax:decimal(12,2)/DECIMAL_64, 8:l_returnflag:string, 9:l_linestatus:string, 10:l_shipdate:string, 11:l_commitdate:string, 12:l_receiptdate:string, 13:l_shipinstruct:string, 14:l_shipmode:string, 15:l_comment:string, 16:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterExprAndExpr(children: SelectColumnIsNotNull(col 0:bigint), SelectColumnIsNotNull(col 17:double)(children: CastLongToDouble(col 0:bigint) -> 17:double))
                    predicate: (l_orderkey is not null and UDFToDouble(l_orderkey) is not null) (type: boolean)
                    Statistics: Num rows: 1 Data size: 1944 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: l_orderkey (type: bigint), l_partkey (type: bigint), l_suppkey (type: bigint), l_quantity (type: decimal(12,2)), l_extendedprice (type: decimal(12,2)), l_discount (type: decimal(12,2)), l_tax (type: decimal(12,2)), l_returnflag (type: string), l_linestatus (type: string), l_shipdate (type: string), l_commitdate (type: string), l_receiptdate (type: string), l_shipinstruct (type: string), l_shipmode (type: string), l_comment (type: string), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
                      Statistics: Num rows: 1 Data size: 1944 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: bigint)
                        null sort order: a
                        sort order: +
                        Map-reduce partition columns: _col0 (type: bigint)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumns: 0:bigint
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumns: 1:bigint, 2:bigint, 4:decimal(12,2), 5:decimal(12,2), 6:decimal(12,2), 7:decimal(12,2), 8:string, 9:string, 10:string, 11:string, 12:string, 13:string, 14:string, 15:string, 16:struct<writeid:bigint,bucketid:int,rowid:bigint>
                        Statistics: Num rows: 1 Data size: 1944 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: decimal(12,2)), _col4 (type: decimal(12,2)), _col5 (type: decimal(12,2)), _col6 (type: decimal(12,2)), _col7 (type: string), _col8 (type: string), _col9 (type: string), _col10 (type: string), _col11 (type: string), _col12 (type: string), _col13 (type: string), _col14 (type: string), _col15 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
            Execution mode: vectorized, llap
            LLAP IO: may be used (ACID table)
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 16
                    includeColumns: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    dataColumns: l_orderkey:bigint, l_partkey:bigint, l_suppkey:bigint, l_linenumber:int, l_quantity:decimal(12,2)/DECIMAL_64, l_extendedprice:decimal(12,2)/DECIMAL_64, l_discount:decimal(12,2)/DECIMAL_64, l_tax:decimal(12,2)/DECIMAL_64, l_returnflag:string, l_linestatus:string, l_shipdate:string, l_commitdate:string, l_receiptdate:string, l_shipinstruct:string, l_shipmode:string, l_comment:string
                    neededVirtualColumns: [ROWID]
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]
        Reducer 3 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type STRUCT not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 UDFToDouble(_col0) (type: double)
                    1 UDFToDouble(_col0) (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                  input vertices:
                    0 Map 1
                  Statistics: Num rows: 1 Data size: 2142 Basic stats: COMPLETE Column stats: NONE
                  Map Join Operator
                    condition map:
                         Left Outer Join 0 to 1
                    keys:
                      0 _col0 (type: bigint)
                      1 _col0 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32
                    input vertices:
                      1 Map 6
                    Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: _col31 (type: string), _col32 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col24 (type: string), _col4 (type: decimal(12,2)), _col28 (type: string), _col30 (type: string), _col15 (type: string), _col12 (type: string), _col13 (type: string), _col21 (type: decimal(12,2)), _col20 (type: decimal(12,2)), _col10 (type: string), _col25 (type: string), _col3 (type: int), _col27 (type: string), _col19 (type: bigint), _col6 (type: decimal(12,2)), _col11 (type: string), _col23 (type: decimal(12,2)), _col26 (type: string), _col8 (type: string), _col0 (type: bigint), _col22 (type: decimal(12,2)), _col1 (type: bigint), _col7 (type: decimal(12,2)), _col17 (type: bigint), _col9 (type: string), _col14 (type: string), _col18 (type: bigint), _col5 (type: decimal(12,2)), _col2 (type: bigint), _col29 (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31
                      Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                      Filter Operator
                        predicate: _col25 is null (type: boolean)
                        Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col21 (type: bigint), _col23 (type: bigint), _col30 (type: bigint), _col13 (type: int), _col3 (type: decimal(12,2)), _col29 (type: decimal(12,2)), _col16 (type: decimal(12,2)), _col24 (type: decimal(12,2)), _col20 (type: string), _col26 (type: string), _col11 (type: string), _col17 (type: string), _col7 (type: string), _col8 (type: string), _col27 (type: string), _col6 (type: string)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15
                          Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                          File Output Operator
                            compressed: false
                            Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                            table:
                                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                                name: default.lineitem_trs
                            Write Type: INSERT
                      Filter Operator
                        predicate: (_col21 = _col25) (type: boolean)
                        Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col25 (type: bigint), _col28 (type: bigint), _col15 (type: bigint), _col10 (type: decimal(12,2)), _col9 (type: decimal(12,2)), _col22 (type: decimal(12,2)), _col18 (type: decimal(12,2)), _col2 (type: string), _col12 (type: string), _col19 (type: string), _col14 (type: string), _col4 (type: string), _col31 (type: string), _col5 (type: string), _col0 (type: string)
                          outputColumnNames: _col0, _col1, _col2, _col3, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
                          Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                            null sort order: z
                            sort order: +
                            Map-reduce partition columns: UDFToInteger(_col0) (type: int)
                            Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint), _col5 (type: decimal(12,2)), _col6 (type: decimal(12,2)), _col7 (type: decimal(12,2)), _col8 (type: decimal(12,2)), _col9 (type: string), _col10 (type: string), _col11 (type: string), _col12 (type: string), _col13 (type: string), _col14 (type: string), _col15 (type: string), _col16 (type: string)
                      Filter Operator
                        predicate: (_col21 = _col25) (type: boolean)
                        Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                          outputColumnNames: _col1
                          Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                          Group By Operator
                            aggregations: count()
                            keys: _col1 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                            minReductionHashAggr: 0.99
                            mode: hash
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                            Reduce Output Operator
                              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                              null sort order: a
                              sort order: +
                              Map-reduce partition columns: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                              Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                              value expressions: _col1 (type: bigint)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: z
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 16
                    dataColumns: KEY.reducesinkkey0:struct<writeid:bigint,bucketid:int,rowid:bigint>, VALUE._col0:bigint, VALUE._col1:bigint, VALUE._col2:bigint, VALUE._col3:decimal(12,2)/DECIMAL_64, VALUE._col4:decimal(12,2)/DECIMAL_64, VALUE._col5:decimal(12,2)/DECIMAL_64, VALUE._col6:decimal(12,2)/DECIMAL_64, VALUE._col7:string, VALUE._col8:string, VALUE._col9:string, VALUE._col10:string, VALUE._col11:string, VALUE._col12:string, VALUE._col13:string, VALUE._col14:string
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [bigint]
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: bigint), VALUE._col1 (type: bigint), VALUE._col2 (type: bigint), 8 (type: int), VALUE._col3 (type: decimal(12,2)), VALUE._col4 (type: decimal(12,2)), VALUE._col5 (type: decimal(12,2)), VALUE._col6 (type: decimal(12,2)), VALUE._col7 (type: string), VALUE._col8 (type: string), VALUE._col9 (type: string), VALUE._col10 (type: string), VALUE._col11 (type: string), VALUE._col12 (type: string), VALUE._col13 (type: string), VALUE._col14 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3, 16, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    selectExpressions: ConstantVectorExpression(val 8) -> 16:int
                Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      name: default.lineitem_trs
                  Write Type: UPDATE
        Reducer 5 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Key expression for GROUPBY operator: Vectorizing complex type STRUCT not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                Filter Operator
                  predicate: (_col1 > 1L) (type: boolean)
                  Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cardinality_violation(_col0) (type: int)
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 2356 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.merge_tmp_table

  Stage: Stage-4
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.lineitem_trs
          Write Type: INSERT

  Stage: Stage-5
    Stats Work
      Basic Stats Work:

  Stage: Stage-1
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.lineitem_trs
          Write Type: UPDATE

  Stage: Stage-6
    Stats Work
      Basic Stats Work:

  Stage: Stage-2
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.merge_tmp_table

  Stage: Stage-7
    Stats Work
      Basic Stats Work:

