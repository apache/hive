PREHOOK: query: create table tmptable_n9(key string, value int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@tmptable_n9
POSTHOOK: query: create table tmptable_n9(key string, value int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tmptable_n9
PREHOOK: query: explain 
insert overwrite table tmptable_n9
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from src s3) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@tmptable_n9
POSTHOOK: query: explain 
insert overwrite table tmptable_n9
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from src s3) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@tmptable_n9
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 10 <- Map 9 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE), Union 5 (CONTAINS)
        Reducer 6 <- Union 5 (SIMPLE_EDGE)
        Reducer 7 <- Reducer 6 (CUSTOM_SIMPLE_EDGE)
        Reducer 8 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 5 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Reducer 10 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst2_n58' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst1_n93' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.5
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.tmptable_n9
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: int)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), min(value), max(value), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 8 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst3_n22' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Union 3 
            Vertex: Union 3
        Union 5 
            Vertex: Union 5

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.tmptable_n9

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, int
          Table: default.tmptable_n9

PREHOOK: query: insert overwrite table tmptable_n9
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from src s3) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@tmptable_n9
POSTHOOK: query: insert overwrite table tmptable_n9
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from src s3) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@tmptable_n9
POSTHOOK: Lineage: tmptable_n9.key EXPRESSION []
POSTHOOK: Lineage: tmptable_n9.value EXPRESSION [(src)s1.null, (src)s2.null, (src)s3.null, ]
PREHOOK: query: select * from tmptable_n9 x sort by x.key
PREHOOK: type: QUERY
PREHOOK: Input: default@tmptable_n9
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from tmptable_n9 x sort by x.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@tmptable_n9
POSTHOOK: Output: hdfs://### HDFS PATH ###
tst1_n93	500
tst2_n58	500
tst3_n22	500
PREHOOK: query: create table tmptable12(key string, value int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@tmptable12
POSTHOOK: query: create table tmptable12(key string, value int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tmptable12
PREHOOK: query: explain 
insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from srcbucket s3) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Input: default@src1
PREHOOK: Input: default@srcbucket
PREHOOK: Output: default@tmptable12
POSTHOOK: query: explain 
insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from srcbucket s3) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Input: default@src1
POSTHOOK: Input: default@srcbucket
POSTHOOK: Output: default@tmptable12
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 11 <- Map 10 (CUSTOM_SIMPLE_EDGE), Union 5 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE), Union 5 (CONTAINS)
        Reducer 6 <- Union 5 (SIMPLE_EDGE)
        Reducer 7 <- Reducer 6 (CUSTOM_SIMPLE_EDGE)
        Reducer 9 <- Map 8 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 10 
            Map Operator Tree:
                TableScan
                  alias: s3
                  Statistics: Num rows: 1000 Data size: 10603 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 1000 Data size: 10603 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.96
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Reducer 11 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst3_n22' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst1_n93' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.5
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.tmptable12
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: int)
                    outputColumnNames: key, value
                    Statistics: Num rows: 1 Data size: 96 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), min(value), max(value), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 9 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst2_n58' (type: string), _col0 (type: bigint)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: bigint)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: bigint)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                      Statistics: Num rows: 1 Data size: 100 Basic stats: COMPLETE Column stats: COMPLETE
        Union 3 
            Vertex: Union 3
        Union 5 
            Vertex: Union 5

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.tmptable12

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, int
          Table: default.tmptable12

PREHOOK: query: insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from srcbucket s3) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Input: default@src1
PREHOOK: Input: default@srcbucket
PREHOOK: Output: default@tmptable12
POSTHOOK: query: insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1_n93' as key, count(1) as value from src s1
                                        UNION DISTINCT  
                                            select 'tst2_n58' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3_n22' as key, count(1) as value from srcbucket s3) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Input: default@src1
POSTHOOK: Input: default@srcbucket
POSTHOOK: Output: default@tmptable12
POSTHOOK: Lineage: tmptable12.key EXPRESSION []
POSTHOOK: Lineage: tmptable12.value EXPRESSION [(src)s1.null, (src1)s2.null, (srcbucket)s3.null, ]
PREHOOK: query: select * from tmptable12 x sort by x.key
PREHOOK: type: QUERY
PREHOOK: Input: default@tmptable12
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from tmptable12 x sort by x.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@tmptable12
POSTHOOK: Output: hdfs://### HDFS PATH ###
tst1_n93	500
tst2_n58	25
tst3_n22	1000
PREHOOK: query: explain 
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT  
                                            select s2.key as key, s2.value as value from src s2) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain 
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT  
                                            select s2.key as key, s2.value as value from src s2) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 4 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT  
                                          select s2.key as key, s2.value as value from src s2) unionsrc
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT  
                                          select s2.key as key, s2.value as value from src s2) unionsrc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
10	val_10
100	val_100
103	val_103
104	val_104
105	val_105
11	val_11
111	val_111
114	val_114
116	val_116
118	val_118
125	val_125
126	val_126
131	val_131
133	val_133
134	val_134
143	val_143
15	val_15
150	val_150
152	val_152
155	val_155
157	val_157
163	val_163
164	val_164
167	val_167
168	val_168
169	val_169
17	val_17
170	val_170
174	val_174
175	val_175
176	val_176
177	val_177
18	val_18
180	val_180
183	val_183
187	val_187
189	val_189
19	val_19
190	val_190
191	val_191
192	val_192
193	val_193
195	val_195
197	val_197
199	val_199
2	val_2
20	val_20
200	val_200
201	val_201
202	val_202
203	val_203
205	val_205
207	val_207
208	val_208
213	val_213
214	val_214
216	val_216
217	val_217
221	val_221
229	val_229
230	val_230
237	val_237
238	val_238
239	val_239
24	val_24
241	val_241
244	val_244
248	val_248
252	val_252
258	val_258
26	val_26
263	val_263
27	val_27
272	val_272
273	val_273
274	val_274
278	val_278
281	val_281
282	val_282
283	val_283
285	val_285
286	val_286
287	val_287
288	val_288
291	val_291
298	val_298
30	val_30
302	val_302
305	val_305
306	val_306
307	val_307
309	val_309
315	val_315
316	val_316
321	val_321
322	val_322
323	val_323
325	val_325
33	val_33
332	val_332
333	val_333
335	val_335
336	val_336
338	val_338
34	val_34
344	val_344
348	val_348
35	val_35
353	val_353
360	val_360
362	val_362
366	val_366
367	val_367
373	val_373
379	val_379
386	val_386
394	val_394
399	val_399
4	val_4
401	val_401
402	val_402
404	val_404
406	val_406
409	val_409
41	val_41
411	val_411
413	val_413
418	val_418
419	val_419
421	val_421
424	val_424
427	val_427
429	val_429
431	val_431
432	val_432
435	val_435
436	val_436
437	val_437
438	val_438
444	val_444
452	val_452
453	val_453
455	val_455
457	val_457
459	val_459
463	val_463
466	val_466
47	val_47
472	val_472
475	val_475
478	val_478
479	val_479
482	val_482
483	val_483
484	val_484
492	val_492
494	val_494
498	val_498
5	val_5
54	val_54
57	val_57
65	val_65
69	val_69
72	val_72
76	val_76
78	val_78
8	val_8
80	val_80
90	val_90
98	val_98
0	val_0
113	val_113
119	val_119
12	val_12
120	val_120
128	val_128
129	val_129
136	val_136
137	val_137
138	val_138
145	val_145
146	val_146
149	val_149
153	val_153
156	val_156
158	val_158
160	val_160
162	val_162
165	val_165
166	val_166
172	val_172
178	val_178
179	val_179
181	val_181
186	val_186
194	val_194
196	val_196
209	val_209
218	val_218
219	val_219
222	val_222
223	val_223
224	val_224
226	val_226
228	val_228
233	val_233
235	val_235
242	val_242
247	val_247
249	val_249
255	val_255
256	val_256
257	val_257
260	val_260
262	val_262
265	val_265
266	val_266
275	val_275
277	val_277
28	val_28
280	val_280
284	val_284
289	val_289
292	val_292
296	val_296
308	val_308
310	val_310
311	val_311
317	val_317
318	val_318
327	val_327
331	val_331
339	val_339
341	val_341
342	val_342
345	val_345
351	val_351
356	val_356
364	val_364
365	val_365
368	val_368
369	val_369
37	val_37
374	val_374
375	val_375
377	val_377
378	val_378
382	val_382
384	val_384
389	val_389
392	val_392
393	val_393
395	val_395
396	val_396
397	val_397
400	val_400
403	val_403
407	val_407
414	val_414
417	val_417
42	val_42
43	val_43
430	val_430
439	val_439
44	val_44
443	val_443
446	val_446
448	val_448
449	val_449
454	val_454
458	val_458
460	val_460
462	val_462
467	val_467
468	val_468
469	val_469
470	val_470
477	val_477
480	val_480
481	val_481
485	val_485
487	val_487
489	val_489
490	val_490
491	val_491
493	val_493
495	val_495
496	val_496
497	val_497
51	val_51
53	val_53
58	val_58
64	val_64
66	val_66
67	val_67
70	val_70
74	val_74
77	val_77
82	val_82
83	val_83
84	val_84
85	val_85
86	val_86
87	val_87
9	val_9
92	val_92
95	val_95
96	val_96
97	val_97
PREHOOK: query: CREATE TABLE DEST1_n96(key STRING, value STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST1_n96
POSTHOOK: query: CREATE TABLE DEST1_n96(key STRING, value STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST1_n96
PREHOOK: query: CREATE TABLE DEST2_n26(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST2_n26
POSTHOOK: query: CREATE TABLE DEST2_n26(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST2_n26
PREHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1_n96 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2_n26 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest1_n96
PREHOOK: Output: default@dest2_n26
POSTHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1_n96 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2_n26 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest1_n96
POSTHOOK: Output: default@dest2_n26
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Map 8 <- Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
        Reducer 6 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
        Reducer 7 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), substr(_col1, 5) (type: string)
                        null sort order: zzz
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst1_n93' (type: string), CAST( _col0 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string), substr(_col1, 5) (type: string)
                      null sort order: zzz
                      sort order: +++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: count(DISTINCT substr(_col1, 5))
                  keys: _col0 (type: string), substr(_col1, 5) (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 125 Data size: 34875 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string)
                    Statistics: Num rows: 125 Data size: 34875 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: count(DISTINCT substr(_col1, 5))
                  keys: _col0 (type: string), _col1 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 250 Data size: 71000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), CAST( _col2 AS STRING) (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.dest2_n26
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                      outputColumnNames: key, val1, val2
                      Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(val1)), avg(COALESCE(length(val1),0)), count(val1), compute_bit_vector_hll(val1), max(length(val2)), avg(COALESCE(length(val2),0)), count(val2), compute_bit_vector_hll(val2)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                        Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary)
        Reducer 5 
            Execution mode: llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(DISTINCT KEY._col1:0._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 125 Data size: 12500 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), CAST( _col1 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest1_n96
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 336 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 500 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest1_n96

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.dest1_n96

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest2_n26

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: string, string, string
          Table: default.dest2_n26

PREHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1_n96 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2_n26 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest1_n96
PREHOOK: Output: default@dest2_n26
POSTHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1_n96 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2_n26 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest1_n96
POSTHOOK: Output: default@dest2_n26
POSTHOOK: Lineage: dest1_n96.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest1_n96.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest2_n26.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest2_n26.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest2_n26.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT DEST1_n96.* FROM DEST1_n96
PREHOOK: type: QUERY
PREHOOK: Input: default@dest1_n96
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST1_n96.* FROM DEST1_n96
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest1_n96
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	1
10	1
100	1
103	1
104	1
105	1
11	1
111	1
113	1
114	1
116	1
118	1
119	1
12	1
120	1
125	1
126	1
128	1
129	1
131	1
133	1
134	1
136	1
137	1
138	1
143	1
145	1
146	1
149	1
15	1
150	1
152	1
153	1
155	1
156	1
157	1
158	1
160	1
162	1
163	1
164	1
165	1
166	1
167	1
168	1
169	1
17	1
170	1
172	1
174	1
175	1
176	1
177	1
178	1
179	1
18	1
180	1
181	1
183	1
186	1
187	1
189	1
19	1
190	1
191	1
192	1
193	1
194	1
195	1
196	1
197	1
199	1
2	1
20	1
200	1
201	1
202	1
203	1
205	1
207	1
208	1
209	1
213	1
214	1
216	1
217	1
218	1
219	1
221	1
222	1
223	1
224	1
226	1
228	1
229	1
230	1
233	1
235	1
237	1
238	1
239	1
24	1
241	1
242	1
244	1
247	1
248	1
249	1
252	1
255	1
256	1
257	1
258	1
26	1
260	1
262	1
263	1
265	1
266	1
27	1
272	1
273	1
274	1
275	1
277	1
278	1
28	1
280	1
281	1
282	1
283	1
284	1
285	1
286	1
287	1
288	1
289	1
291	1
292	1
296	1
298	1
30	1
302	1
305	1
306	1
307	1
308	1
309	1
310	1
311	1
315	1
316	1
317	1
318	1
321	1
322	1
323	1
325	1
327	1
33	1
331	1
332	1
333	1
335	1
336	1
338	1
339	1
34	1
341	1
342	1
344	1
345	1
348	1
35	1
351	1
353	1
356	1
360	1
362	1
364	1
365	1
366	1
367	1
368	1
369	1
37	1
373	1
374	1
375	1
377	1
378	1
379	1
382	1
384	1
386	1
389	1
392	1
393	1
394	1
395	1
396	1
397	1
399	1
4	1
400	1
401	1
402	1
403	1
404	1
406	1
407	1
409	1
41	1
411	1
413	1
414	1
417	1
418	1
419	1
42	1
421	1
424	1
427	1
429	1
43	1
430	1
431	1
432	1
435	1
436	1
437	1
438	1
439	1
44	1
443	1
444	1
446	1
448	1
449	1
452	1
453	1
454	1
455	1
457	1
458	1
459	1
460	1
462	1
463	1
466	1
467	1
468	1
469	1
47	1
470	1
472	1
475	1
477	1
478	1
479	1
480	1
481	1
482	1
483	1
484	1
485	1
487	1
489	1
490	1
491	1
492	1
493	1
494	1
495	1
496	1
497	1
498	1
5	1
51	1
53	1
54	1
57	1
58	1
64	1
65	1
66	1
67	1
69	1
70	1
72	1
74	1
76	1
77	1
78	1
8	1
80	1
82	1
83	1
84	1
85	1
86	1
87	1
9	1
90	1
92	1
95	1
96	1
97	1
98	1
tst1_n93	1
PREHOOK: query: SELECT DEST2_n26.* FROM DEST2_n26
PREHOOK: type: QUERY
PREHOOK: Input: default@dest2_n26
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST2_n26.* FROM DEST2_n26
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2_n26
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0	1
10	val_10	1
100	val_100	1
103	val_103	1
104	val_104	1
105	val_105	1
11	val_11	1
111	val_111	1
113	val_113	1
114	val_114	1
116	val_116	1
118	val_118	1
119	val_119	1
12	val_12	1
120	val_120	1
125	val_125	1
126	val_126	1
128	val_128	1
129	val_129	1
131	val_131	1
133	val_133	1
134	val_134	1
136	val_136	1
137	val_137	1
138	val_138	1
143	val_143	1
145	val_145	1
146	val_146	1
149	val_149	1
15	val_15	1
150	val_150	1
152	val_152	1
153	val_153	1
155	val_155	1
156	val_156	1
157	val_157	1
158	val_158	1
160	val_160	1
162	val_162	1
163	val_163	1
164	val_164	1
165	val_165	1
166	val_166	1
167	val_167	1
168	val_168	1
169	val_169	1
17	val_17	1
170	val_170	1
172	val_172	1
174	val_174	1
175	val_175	1
176	val_176	1
177	val_177	1
178	val_178	1
179	val_179	1
18	val_18	1
180	val_180	1
181	val_181	1
183	val_183	1
186	val_186	1
187	val_187	1
189	val_189	1
19	val_19	1
190	val_190	1
191	val_191	1
192	val_192	1
193	val_193	1
194	val_194	1
195	val_195	1
196	val_196	1
197	val_197	1
199	val_199	1
2	val_2	1
20	val_20	1
200	val_200	1
201	val_201	1
202	val_202	1
203	val_203	1
205	val_205	1
207	val_207	1
208	val_208	1
209	val_209	1
213	val_213	1
214	val_214	1
216	val_216	1
217	val_217	1
218	val_218	1
219	val_219	1
221	val_221	1
222	val_222	1
223	val_223	1
224	val_224	1
226	val_226	1
228	val_228	1
229	val_229	1
230	val_230	1
233	val_233	1
235	val_235	1
237	val_237	1
238	val_238	1
239	val_239	1
24	val_24	1
241	val_241	1
242	val_242	1
244	val_244	1
247	val_247	1
248	val_248	1
249	val_249	1
252	val_252	1
255	val_255	1
256	val_256	1
257	val_257	1
258	val_258	1
26	val_26	1
260	val_260	1
262	val_262	1
263	val_263	1
265	val_265	1
266	val_266	1
27	val_27	1
272	val_272	1
273	val_273	1
274	val_274	1
275	val_275	1
277	val_277	1
278	val_278	1
28	val_28	1
280	val_280	1
281	val_281	1
282	val_282	1
283	val_283	1
284	val_284	1
285	val_285	1
286	val_286	1
287	val_287	1
288	val_288	1
289	val_289	1
291	val_291	1
292	val_292	1
296	val_296	1
298	val_298	1
30	val_30	1
302	val_302	1
305	val_305	1
306	val_306	1
307	val_307	1
308	val_308	1
309	val_309	1
310	val_310	1
311	val_311	1
315	val_315	1
316	val_316	1
317	val_317	1
318	val_318	1
321	val_321	1
322	val_322	1
323	val_323	1
325	val_325	1
327	val_327	1
33	val_33	1
331	val_331	1
332	val_332	1
333	val_333	1
335	val_335	1
336	val_336	1
338	val_338	1
339	val_339	1
34	val_34	1
341	val_341	1
342	val_342	1
344	val_344	1
345	val_345	1
348	val_348	1
35	val_35	1
351	val_351	1
353	val_353	1
356	val_356	1
360	val_360	1
362	val_362	1
364	val_364	1
365	val_365	1
366	val_366	1
367	val_367	1
368	val_368	1
369	val_369	1
37	val_37	1
373	val_373	1
374	val_374	1
375	val_375	1
377	val_377	1
378	val_378	1
379	val_379	1
382	val_382	1
384	val_384	1
386	val_386	1
389	val_389	1
392	val_392	1
393	val_393	1
394	val_394	1
395	val_395	1
396	val_396	1
397	val_397	1
399	val_399	1
4	val_4	1
400	val_400	1
401	val_401	1
402	val_402	1
403	val_403	1
404	val_404	1
406	val_406	1
407	val_407	1
409	val_409	1
41	val_41	1
411	val_411	1
413	val_413	1
414	val_414	1
417	val_417	1
418	val_418	1
419	val_419	1
42	val_42	1
421	val_421	1
424	val_424	1
427	val_427	1
429	val_429	1
43	val_43	1
430	val_430	1
431	val_431	1
432	val_432	1
435	val_435	1
436	val_436	1
437	val_437	1
438	val_438	1
439	val_439	1
44	val_44	1
443	val_443	1
444	val_444	1
446	val_446	1
448	val_448	1
449	val_449	1
452	val_452	1
453	val_453	1
454	val_454	1
455	val_455	1
457	val_457	1
458	val_458	1
459	val_459	1
460	val_460	1
462	val_462	1
463	val_463	1
466	val_466	1
467	val_467	1
468	val_468	1
469	val_469	1
47	val_47	1
470	val_470	1
472	val_472	1
475	val_475	1
477	val_477	1
478	val_478	1
479	val_479	1
480	val_480	1
481	val_481	1
482	val_482	1
483	val_483	1
484	val_484	1
485	val_485	1
487	val_487	1
489	val_489	1
490	val_490	1
491	val_491	1
492	val_492	1
493	val_493	1
494	val_494	1
495	val_495	1
496	val_496	1
497	val_497	1
498	val_498	1
5	val_5	1
51	val_51	1
53	val_53	1
54	val_54	1
57	val_57	1
58	val_58	1
64	val_64	1
65	val_65	1
66	val_66	1
67	val_67	1
69	val_69	1
70	val_70	1
72	val_72	1
74	val_74	1
76	val_76	1
77	val_77	1
78	val_78	1
8	val_8	1
80	val_80	1
82	val_82	1
83	val_83	1
84	val_84	1
85	val_85	1
86	val_86	1
87	val_87	1
9	val_9	1
90	val_90	1
92	val_92	1
95	val_95	1
96	val_96	1
97	val_97	1
98	val_98	1
tst1_n93	500	1
PREHOOK: query: CREATE TABLE DEST118(key STRING, value STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST118
POSTHOOK: query: CREATE TABLE DEST118(key STRING, value STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST118
PREHOOK: query: CREATE TABLE DEST218(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST218
POSTHOOK: query: CREATE TABLE DEST218(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST218
PREHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest118
PREHOOK: Output: default@dest218
POSTHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest118
POSTHOOK: Output: default@dest218
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Map 7 <- Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst1_n93' (type: string), CAST( _col0 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.dest118
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: key, value
                  Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                    Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                      value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), _col1 (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest218
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(val1)), avg(COALESCE(length(val1),0)), count(val1), compute_bit_vector_hll(val1), max(length(val2)), avg(COALESCE(length(val2),0)), count(val2), compute_bit_vector_hll(val2)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                      Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary)
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 336 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 500 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest118

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.dest118

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest218

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: string, string, string
          Table: default.dest218

PREHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest118
PREHOOK: Output: default@dest218
POSTHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest118
POSTHOOK: Output: default@dest218
POSTHOOK: Lineage: dest118.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest118.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest218.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest218.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest218.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT DEST118.* FROM DEST118 SORT BY DEST118.key, DEST118.value
PREHOOK: type: QUERY
PREHOOK: Input: default@dest118
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST118.* FROM DEST118 SORT BY DEST118.key, DEST118.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest118
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0
10	val_10
100	val_100
103	val_103
104	val_104
105	val_105
11	val_11
111	val_111
113	val_113
114	val_114
116	val_116
118	val_118
119	val_119
12	val_12
120	val_120
125	val_125
126	val_126
128	val_128
129	val_129
131	val_131
133	val_133
134	val_134
136	val_136
137	val_137
138	val_138
143	val_143
145	val_145
146	val_146
149	val_149
15	val_15
150	val_150
152	val_152
153	val_153
155	val_155
156	val_156
157	val_157
158	val_158
160	val_160
162	val_162
163	val_163
164	val_164
165	val_165
166	val_166
167	val_167
168	val_168
169	val_169
17	val_17
170	val_170
172	val_172
174	val_174
175	val_175
176	val_176
177	val_177
178	val_178
179	val_179
18	val_18
180	val_180
181	val_181
183	val_183
186	val_186
187	val_187
189	val_189
19	val_19
190	val_190
191	val_191
192	val_192
193	val_193
194	val_194
195	val_195
196	val_196
197	val_197
199	val_199
2	val_2
20	val_20
200	val_200
201	val_201
202	val_202
203	val_203
205	val_205
207	val_207
208	val_208
209	val_209
213	val_213
214	val_214
216	val_216
217	val_217
218	val_218
219	val_219
221	val_221
222	val_222
223	val_223
224	val_224
226	val_226
228	val_228
229	val_229
230	val_230
233	val_233
235	val_235
237	val_237
238	val_238
239	val_239
24	val_24
241	val_241
242	val_242
244	val_244
247	val_247
248	val_248
249	val_249
252	val_252
255	val_255
256	val_256
257	val_257
258	val_258
26	val_26
260	val_260
262	val_262
263	val_263
265	val_265
266	val_266
27	val_27
272	val_272
273	val_273
274	val_274
275	val_275
277	val_277
278	val_278
28	val_28
280	val_280
281	val_281
282	val_282
283	val_283
284	val_284
285	val_285
286	val_286
287	val_287
288	val_288
289	val_289
291	val_291
292	val_292
296	val_296
298	val_298
30	val_30
302	val_302
305	val_305
306	val_306
307	val_307
308	val_308
309	val_309
310	val_310
311	val_311
315	val_315
316	val_316
317	val_317
318	val_318
321	val_321
322	val_322
323	val_323
325	val_325
327	val_327
33	val_33
331	val_331
332	val_332
333	val_333
335	val_335
336	val_336
338	val_338
339	val_339
34	val_34
341	val_341
342	val_342
344	val_344
345	val_345
348	val_348
35	val_35
351	val_351
353	val_353
356	val_356
360	val_360
362	val_362
364	val_364
365	val_365
366	val_366
367	val_367
368	val_368
369	val_369
37	val_37
373	val_373
374	val_374
375	val_375
377	val_377
378	val_378
379	val_379
382	val_382
384	val_384
386	val_386
389	val_389
392	val_392
393	val_393
394	val_394
395	val_395
396	val_396
397	val_397
399	val_399
4	val_4
400	val_400
401	val_401
402	val_402
403	val_403
404	val_404
406	val_406
407	val_407
409	val_409
41	val_41
411	val_411
413	val_413
414	val_414
417	val_417
418	val_418
419	val_419
42	val_42
421	val_421
424	val_424
427	val_427
429	val_429
43	val_43
430	val_430
431	val_431
432	val_432
435	val_435
436	val_436
437	val_437
438	val_438
439	val_439
44	val_44
443	val_443
444	val_444
446	val_446
448	val_448
449	val_449
452	val_452
453	val_453
454	val_454
455	val_455
457	val_457
458	val_458
459	val_459
460	val_460
462	val_462
463	val_463
466	val_466
467	val_467
468	val_468
469	val_469
47	val_47
470	val_470
472	val_472
475	val_475
477	val_477
478	val_478
479	val_479
480	val_480
481	val_481
482	val_482
483	val_483
484	val_484
485	val_485
487	val_487
489	val_489
490	val_490
491	val_491
492	val_492
493	val_493
494	val_494
495	val_495
496	val_496
497	val_497
498	val_498
5	val_5
51	val_51
53	val_53
54	val_54
57	val_57
58	val_58
64	val_64
65	val_65
66	val_66
67	val_67
69	val_69
70	val_70
72	val_72
74	val_74
76	val_76
77	val_77
78	val_78
8	val_8
80	val_80
82	val_82
83	val_83
84	val_84
85	val_85
86	val_86
87	val_87
9	val_9
90	val_90
92	val_92
95	val_95
96	val_96
97	val_97
98	val_98
tst1_n93	500
PREHOOK: query: SELECT DEST218.* FROM DEST218 SORT BY DEST218.key, DEST218.val1, DEST218.val2
PREHOOK: type: QUERY
PREHOOK: Input: default@dest218
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST218.* FROM DEST218 SORT BY DEST218.key, DEST218.val1, DEST218.val2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest218
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0	val_0
10	val_10	val_10
100	val_100	val_100
103	val_103	val_103
104	val_104	val_104
105	val_105	val_105
11	val_11	val_11
111	val_111	val_111
113	val_113	val_113
114	val_114	val_114
116	val_116	val_116
118	val_118	val_118
119	val_119	val_119
12	val_12	val_12
120	val_120	val_120
125	val_125	val_125
126	val_126	val_126
128	val_128	val_128
129	val_129	val_129
131	val_131	val_131
133	val_133	val_133
134	val_134	val_134
136	val_136	val_136
137	val_137	val_137
138	val_138	val_138
143	val_143	val_143
145	val_145	val_145
146	val_146	val_146
149	val_149	val_149
15	val_15	val_15
150	val_150	val_150
152	val_152	val_152
153	val_153	val_153
155	val_155	val_155
156	val_156	val_156
157	val_157	val_157
158	val_158	val_158
160	val_160	val_160
162	val_162	val_162
163	val_163	val_163
164	val_164	val_164
165	val_165	val_165
166	val_166	val_166
167	val_167	val_167
168	val_168	val_168
169	val_169	val_169
17	val_17	val_17
170	val_170	val_170
172	val_172	val_172
174	val_174	val_174
175	val_175	val_175
176	val_176	val_176
177	val_177	val_177
178	val_178	val_178
179	val_179	val_179
18	val_18	val_18
180	val_180	val_180
181	val_181	val_181
183	val_183	val_183
186	val_186	val_186
187	val_187	val_187
189	val_189	val_189
19	val_19	val_19
190	val_190	val_190
191	val_191	val_191
192	val_192	val_192
193	val_193	val_193
194	val_194	val_194
195	val_195	val_195
196	val_196	val_196
197	val_197	val_197
199	val_199	val_199
2	val_2	val_2
20	val_20	val_20
200	val_200	val_200
201	val_201	val_201
202	val_202	val_202
203	val_203	val_203
205	val_205	val_205
207	val_207	val_207
208	val_208	val_208
209	val_209	val_209
213	val_213	val_213
214	val_214	val_214
216	val_216	val_216
217	val_217	val_217
218	val_218	val_218
219	val_219	val_219
221	val_221	val_221
222	val_222	val_222
223	val_223	val_223
224	val_224	val_224
226	val_226	val_226
228	val_228	val_228
229	val_229	val_229
230	val_230	val_230
233	val_233	val_233
235	val_235	val_235
237	val_237	val_237
238	val_238	val_238
239	val_239	val_239
24	val_24	val_24
241	val_241	val_241
242	val_242	val_242
244	val_244	val_244
247	val_247	val_247
248	val_248	val_248
249	val_249	val_249
252	val_252	val_252
255	val_255	val_255
256	val_256	val_256
257	val_257	val_257
258	val_258	val_258
26	val_26	val_26
260	val_260	val_260
262	val_262	val_262
263	val_263	val_263
265	val_265	val_265
266	val_266	val_266
27	val_27	val_27
272	val_272	val_272
273	val_273	val_273
274	val_274	val_274
275	val_275	val_275
277	val_277	val_277
278	val_278	val_278
28	val_28	val_28
280	val_280	val_280
281	val_281	val_281
282	val_282	val_282
283	val_283	val_283
284	val_284	val_284
285	val_285	val_285
286	val_286	val_286
287	val_287	val_287
288	val_288	val_288
289	val_289	val_289
291	val_291	val_291
292	val_292	val_292
296	val_296	val_296
298	val_298	val_298
30	val_30	val_30
302	val_302	val_302
305	val_305	val_305
306	val_306	val_306
307	val_307	val_307
308	val_308	val_308
309	val_309	val_309
310	val_310	val_310
311	val_311	val_311
315	val_315	val_315
316	val_316	val_316
317	val_317	val_317
318	val_318	val_318
321	val_321	val_321
322	val_322	val_322
323	val_323	val_323
325	val_325	val_325
327	val_327	val_327
33	val_33	val_33
331	val_331	val_331
332	val_332	val_332
333	val_333	val_333
335	val_335	val_335
336	val_336	val_336
338	val_338	val_338
339	val_339	val_339
34	val_34	val_34
341	val_341	val_341
342	val_342	val_342
344	val_344	val_344
345	val_345	val_345
348	val_348	val_348
35	val_35	val_35
351	val_351	val_351
353	val_353	val_353
356	val_356	val_356
360	val_360	val_360
362	val_362	val_362
364	val_364	val_364
365	val_365	val_365
366	val_366	val_366
367	val_367	val_367
368	val_368	val_368
369	val_369	val_369
37	val_37	val_37
373	val_373	val_373
374	val_374	val_374
375	val_375	val_375
377	val_377	val_377
378	val_378	val_378
379	val_379	val_379
382	val_382	val_382
384	val_384	val_384
386	val_386	val_386
389	val_389	val_389
392	val_392	val_392
393	val_393	val_393
394	val_394	val_394
395	val_395	val_395
396	val_396	val_396
397	val_397	val_397
399	val_399	val_399
4	val_4	val_4
400	val_400	val_400
401	val_401	val_401
402	val_402	val_402
403	val_403	val_403
404	val_404	val_404
406	val_406	val_406
407	val_407	val_407
409	val_409	val_409
41	val_41	val_41
411	val_411	val_411
413	val_413	val_413
414	val_414	val_414
417	val_417	val_417
418	val_418	val_418
419	val_419	val_419
42	val_42	val_42
421	val_421	val_421
424	val_424	val_424
427	val_427	val_427
429	val_429	val_429
43	val_43	val_43
430	val_430	val_430
431	val_431	val_431
432	val_432	val_432
435	val_435	val_435
436	val_436	val_436
437	val_437	val_437
438	val_438	val_438
439	val_439	val_439
44	val_44	val_44
443	val_443	val_443
444	val_444	val_444
446	val_446	val_446
448	val_448	val_448
449	val_449	val_449
452	val_452	val_452
453	val_453	val_453
454	val_454	val_454
455	val_455	val_455
457	val_457	val_457
458	val_458	val_458
459	val_459	val_459
460	val_460	val_460
462	val_462	val_462
463	val_463	val_463
466	val_466	val_466
467	val_467	val_467
468	val_468	val_468
469	val_469	val_469
47	val_47	val_47
470	val_470	val_470
472	val_472	val_472
475	val_475	val_475
477	val_477	val_477
478	val_478	val_478
479	val_479	val_479
480	val_480	val_480
481	val_481	val_481
482	val_482	val_482
483	val_483	val_483
484	val_484	val_484
485	val_485	val_485
487	val_487	val_487
489	val_489	val_489
490	val_490	val_490
491	val_491	val_491
492	val_492	val_492
493	val_493	val_493
494	val_494	val_494
495	val_495	val_495
496	val_496	val_496
497	val_497	val_497
498	val_498	val_498
5	val_5	val_5
51	val_51	val_51
53	val_53	val_53
54	val_54	val_54
57	val_57	val_57
58	val_58	val_58
64	val_64	val_64
65	val_65	val_65
66	val_66	val_66
67	val_67	val_67
69	val_69	val_69
70	val_70	val_70
72	val_72	val_72
74	val_74	val_74
76	val_76	val_76
77	val_77	val_77
78	val_78	val_78
8	val_8	val_8
80	val_80	val_80
82	val_82	val_82
83	val_83	val_83
84	val_84	val_84
85	val_85	val_85
86	val_86	val_86
87	val_87	val_87
9	val_9	val_9
90	val_90	val_90
92	val_92	val_92
95	val_95	val_95
96	val_96	val_96
97	val_97	val_97
98	val_98	val_98
tst1_n93	500	500
PREHOOK: query: CREATE TABLE DEST119(key STRING, value STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST119
POSTHOOK: query: CREATE TABLE DEST119(key STRING, value STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST119
PREHOOK: query: CREATE TABLE DEST219(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DEST219
POSTHOOK: query: CREATE TABLE DEST219(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DEST219
PREHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest119
PREHOOK: Output: default@dest219
POSTHOOK: query: explain 
FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest119
POSTHOOK: Output: default@dest219
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Map 7 <- Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: s1
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: s2
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'tst1_n93' (type: string), CAST( _col0 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string)
                      Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 69000 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: count(_col1)
                  keys: _col0 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 125 Data size: 12500 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), CAST( _col1 AS STRING) (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.dest119
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: string)
                      outputColumnNames: key, value
                      Statistics: Num rows: 125 Data size: 34500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), _col1 (type: string)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.dest219
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                    outputColumnNames: key, val1, val2
                    Statistics: Num rows: 250 Data size: 115000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(val1)), avg(COALESCE(length(val1),0)), count(val1), compute_bit_vector_hll(val1), max(length(val2)), avg(COALESCE(length(val2),0)), count(val2), compute_bit_vector_hll(val2)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                      Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 704 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary)
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 336 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 500 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 798 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest119

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.dest119

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dest219

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, val1, val2
          Column Types: string, string, string
          Table: default.dest219

PREHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dest119
PREHOOK: Output: default@dest219
POSTHOOK: query: FROM (select 'tst1_n93' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT  
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dest119
POSTHOOK: Output: default@dest219
POSTHOOK: Lineage: dest119.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest119.value EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest219.key EXPRESSION [(src)s2.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dest219.val1 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dest219.val2 EXPRESSION [(src)s1.null, (src)s2.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT DEST119.* FROM DEST119 SORT BY DEST119.key, DEST119.value
PREHOOK: type: QUERY
PREHOOK: Input: default@dest119
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST119.* FROM DEST119 SORT BY DEST119.key, DEST119.value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest119
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	1
10	1
100	1
103	1
104	1
105	1
11	1
111	1
113	1
114	1
116	1
118	1
119	1
12	1
120	1
125	1
126	1
128	1
129	1
131	1
133	1
134	1
136	1
137	1
138	1
143	1
145	1
146	1
149	1
15	1
150	1
152	1
153	1
155	1
156	1
157	1
158	1
160	1
162	1
163	1
164	1
165	1
166	1
167	1
168	1
169	1
17	1
170	1
172	1
174	1
175	1
176	1
177	1
178	1
179	1
18	1
180	1
181	1
183	1
186	1
187	1
189	1
19	1
190	1
191	1
192	1
193	1
194	1
195	1
196	1
197	1
199	1
2	1
20	1
200	1
201	1
202	1
203	1
205	1
207	1
208	1
209	1
213	1
214	1
216	1
217	1
218	1
219	1
221	1
222	1
223	1
224	1
226	1
228	1
229	1
230	1
233	1
235	1
237	1
238	1
239	1
24	1
241	1
242	1
244	1
247	1
248	1
249	1
252	1
255	1
256	1
257	1
258	1
26	1
260	1
262	1
263	1
265	1
266	1
27	1
272	1
273	1
274	1
275	1
277	1
278	1
28	1
280	1
281	1
282	1
283	1
284	1
285	1
286	1
287	1
288	1
289	1
291	1
292	1
296	1
298	1
30	1
302	1
305	1
306	1
307	1
308	1
309	1
310	1
311	1
315	1
316	1
317	1
318	1
321	1
322	1
323	1
325	1
327	1
33	1
331	1
332	1
333	1
335	1
336	1
338	1
339	1
34	1
341	1
342	1
344	1
345	1
348	1
35	1
351	1
353	1
356	1
360	1
362	1
364	1
365	1
366	1
367	1
368	1
369	1
37	1
373	1
374	1
375	1
377	1
378	1
379	1
382	1
384	1
386	1
389	1
392	1
393	1
394	1
395	1
396	1
397	1
399	1
4	1
400	1
401	1
402	1
403	1
404	1
406	1
407	1
409	1
41	1
411	1
413	1
414	1
417	1
418	1
419	1
42	1
421	1
424	1
427	1
429	1
43	1
430	1
431	1
432	1
435	1
436	1
437	1
438	1
439	1
44	1
443	1
444	1
446	1
448	1
449	1
452	1
453	1
454	1
455	1
457	1
458	1
459	1
460	1
462	1
463	1
466	1
467	1
468	1
469	1
47	1
470	1
472	1
475	1
477	1
478	1
479	1
480	1
481	1
482	1
483	1
484	1
485	1
487	1
489	1
490	1
491	1
492	1
493	1
494	1
495	1
496	1
497	1
498	1
5	1
51	1
53	1
54	1
57	1
58	1
64	1
65	1
66	1
67	1
69	1
70	1
72	1
74	1
76	1
77	1
78	1
8	1
80	1
82	1
83	1
84	1
85	1
86	1
87	1
9	1
90	1
92	1
95	1
96	1
97	1
98	1
tst1_n93	1
PREHOOK: query: SELECT DEST219.* FROM DEST219 SORT BY DEST219.key, DEST219.val1, DEST219.val2
PREHOOK: type: QUERY
PREHOOK: Input: default@dest219
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT DEST219.* FROM DEST219 SORT BY DEST219.key, DEST219.val1, DEST219.val2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest219
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0	val_0
10	val_10	val_10
100	val_100	val_100
103	val_103	val_103
104	val_104	val_104
105	val_105	val_105
11	val_11	val_11
111	val_111	val_111
113	val_113	val_113
114	val_114	val_114
116	val_116	val_116
118	val_118	val_118
119	val_119	val_119
12	val_12	val_12
120	val_120	val_120
125	val_125	val_125
126	val_126	val_126
128	val_128	val_128
129	val_129	val_129
131	val_131	val_131
133	val_133	val_133
134	val_134	val_134
136	val_136	val_136
137	val_137	val_137
138	val_138	val_138
143	val_143	val_143
145	val_145	val_145
146	val_146	val_146
149	val_149	val_149
15	val_15	val_15
150	val_150	val_150
152	val_152	val_152
153	val_153	val_153
155	val_155	val_155
156	val_156	val_156
157	val_157	val_157
158	val_158	val_158
160	val_160	val_160
162	val_162	val_162
163	val_163	val_163
164	val_164	val_164
165	val_165	val_165
166	val_166	val_166
167	val_167	val_167
168	val_168	val_168
169	val_169	val_169
17	val_17	val_17
170	val_170	val_170
172	val_172	val_172
174	val_174	val_174
175	val_175	val_175
176	val_176	val_176
177	val_177	val_177
178	val_178	val_178
179	val_179	val_179
18	val_18	val_18
180	val_180	val_180
181	val_181	val_181
183	val_183	val_183
186	val_186	val_186
187	val_187	val_187
189	val_189	val_189
19	val_19	val_19
190	val_190	val_190
191	val_191	val_191
192	val_192	val_192
193	val_193	val_193
194	val_194	val_194
195	val_195	val_195
196	val_196	val_196
197	val_197	val_197
199	val_199	val_199
2	val_2	val_2
20	val_20	val_20
200	val_200	val_200
201	val_201	val_201
202	val_202	val_202
203	val_203	val_203
205	val_205	val_205
207	val_207	val_207
208	val_208	val_208
209	val_209	val_209
213	val_213	val_213
214	val_214	val_214
216	val_216	val_216
217	val_217	val_217
218	val_218	val_218
219	val_219	val_219
221	val_221	val_221
222	val_222	val_222
223	val_223	val_223
224	val_224	val_224
226	val_226	val_226
228	val_228	val_228
229	val_229	val_229
230	val_230	val_230
233	val_233	val_233
235	val_235	val_235
237	val_237	val_237
238	val_238	val_238
239	val_239	val_239
24	val_24	val_24
241	val_241	val_241
242	val_242	val_242
244	val_244	val_244
247	val_247	val_247
248	val_248	val_248
249	val_249	val_249
252	val_252	val_252
255	val_255	val_255
256	val_256	val_256
257	val_257	val_257
258	val_258	val_258
26	val_26	val_26
260	val_260	val_260
262	val_262	val_262
263	val_263	val_263
265	val_265	val_265
266	val_266	val_266
27	val_27	val_27
272	val_272	val_272
273	val_273	val_273
274	val_274	val_274
275	val_275	val_275
277	val_277	val_277
278	val_278	val_278
28	val_28	val_28
280	val_280	val_280
281	val_281	val_281
282	val_282	val_282
283	val_283	val_283
284	val_284	val_284
285	val_285	val_285
286	val_286	val_286
287	val_287	val_287
288	val_288	val_288
289	val_289	val_289
291	val_291	val_291
292	val_292	val_292
296	val_296	val_296
298	val_298	val_298
30	val_30	val_30
302	val_302	val_302
305	val_305	val_305
306	val_306	val_306
307	val_307	val_307
308	val_308	val_308
309	val_309	val_309
310	val_310	val_310
311	val_311	val_311
315	val_315	val_315
316	val_316	val_316
317	val_317	val_317
318	val_318	val_318
321	val_321	val_321
322	val_322	val_322
323	val_323	val_323
325	val_325	val_325
327	val_327	val_327
33	val_33	val_33
331	val_331	val_331
332	val_332	val_332
333	val_333	val_333
335	val_335	val_335
336	val_336	val_336
338	val_338	val_338
339	val_339	val_339
34	val_34	val_34
341	val_341	val_341
342	val_342	val_342
344	val_344	val_344
345	val_345	val_345
348	val_348	val_348
35	val_35	val_35
351	val_351	val_351
353	val_353	val_353
356	val_356	val_356
360	val_360	val_360
362	val_362	val_362
364	val_364	val_364
365	val_365	val_365
366	val_366	val_366
367	val_367	val_367
368	val_368	val_368
369	val_369	val_369
37	val_37	val_37
373	val_373	val_373
374	val_374	val_374
375	val_375	val_375
377	val_377	val_377
378	val_378	val_378
379	val_379	val_379
382	val_382	val_382
384	val_384	val_384
386	val_386	val_386
389	val_389	val_389
392	val_392	val_392
393	val_393	val_393
394	val_394	val_394
395	val_395	val_395
396	val_396	val_396
397	val_397	val_397
399	val_399	val_399
4	val_4	val_4
400	val_400	val_400
401	val_401	val_401
402	val_402	val_402
403	val_403	val_403
404	val_404	val_404
406	val_406	val_406
407	val_407	val_407
409	val_409	val_409
41	val_41	val_41
411	val_411	val_411
413	val_413	val_413
414	val_414	val_414
417	val_417	val_417
418	val_418	val_418
419	val_419	val_419
42	val_42	val_42
421	val_421	val_421
424	val_424	val_424
427	val_427	val_427
429	val_429	val_429
43	val_43	val_43
430	val_430	val_430
431	val_431	val_431
432	val_432	val_432
435	val_435	val_435
436	val_436	val_436
437	val_437	val_437
438	val_438	val_438
439	val_439	val_439
44	val_44	val_44
443	val_443	val_443
444	val_444	val_444
446	val_446	val_446
448	val_448	val_448
449	val_449	val_449
452	val_452	val_452
453	val_453	val_453
454	val_454	val_454
455	val_455	val_455
457	val_457	val_457
458	val_458	val_458
459	val_459	val_459
460	val_460	val_460
462	val_462	val_462
463	val_463	val_463
466	val_466	val_466
467	val_467	val_467
468	val_468	val_468
469	val_469	val_469
47	val_47	val_47
470	val_470	val_470
472	val_472	val_472
475	val_475	val_475
477	val_477	val_477
478	val_478	val_478
479	val_479	val_479
480	val_480	val_480
481	val_481	val_481
482	val_482	val_482
483	val_483	val_483
484	val_484	val_484
485	val_485	val_485
487	val_487	val_487
489	val_489	val_489
490	val_490	val_490
491	val_491	val_491
492	val_492	val_492
493	val_493	val_493
494	val_494	val_494
495	val_495	val_495
496	val_496	val_496
497	val_497	val_497
498	val_498	val_498
5	val_5	val_5
51	val_51	val_51
53	val_53	val_53
54	val_54	val_54
57	val_57	val_57
58	val_58	val_58
64	val_64	val_64
65	val_65	val_65
66	val_66	val_66
67	val_67	val_67
69	val_69	val_69
70	val_70	val_70
72	val_72	val_72
74	val_74	val_74
76	val_76	val_76
77	val_77	val_77
78	val_78	val_78
8	val_8	val_8
80	val_80	val_80
82	val_82	val_82
83	val_83	val_83
84	val_84	val_84
85	val_85	val_85
86	val_86	val_86
87	val_87	val_87
9	val_9	val_9
90	val_90	val_90
92	val_92	val_92
95	val_95	val_95
96	val_96	val_96
97	val_97	val_97
98	val_98	val_98
tst1_n93	500	500
PREHOOK: query: create table dst_union22_n0(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@dst_union22_n0
POSTHOOK: query: create table dst_union22_n0(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dst_union22_n0
PREHOOK: query: create table dst_union22_delta_n0(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@dst_union22_delta_n0
POSTHOOK: query: create table dst_union22_delta_n0(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dst_union22_delta_n0
PREHOOK: query: insert overwrite table dst_union22_n0 partition (ds='1')
select key, value, key , value from src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dst_union22_n0@ds=1
POSTHOOK: query: insert overwrite table dst_union22_n0 partition (ds='1')
select key, value, key , value from src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dst_union22_n0@ds=1
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: insert overwrite table dst_union22_delta_n0 partition (ds='1')
select key, key, value, key, value, value from src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@dst_union22_delta_n0@ds=1
POSTHOOK: query: insert overwrite table dst_union22_delta_n0 partition (ds='1')
select key, key, value, key, value, value from src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@dst_union22_delta_n0@ds=1
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k0 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k1 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k2 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k3 SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k4 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: dst_union22_delta_n0 PARTITION(ds=1).k5 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: explain extended
insert overwrite table dst_union22_n0 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta_n0 where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22_n0 a left outer join (select * from dst_union22_delta_n0 where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq
PREHOOK: type: QUERY
PREHOOK: Input: default@dst_union22_delta_n0
PREHOOK: Input: default@dst_union22_delta_n0@ds=1
PREHOOK: Input: default@dst_union22_n0
PREHOOK: Input: default@dst_union22_n0@ds=1
PREHOOK: Output: default@dst_union22_n0@ds=2
POSTHOOK: query: explain extended
insert overwrite table dst_union22_n0 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta_n0 where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22_n0 a left outer join (select * from dst_union22_delta_n0 where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dst_union22_delta_n0
POSTHOOK: Input: default@dst_union22_delta_n0@ds=1
POSTHOOK: Input: default@dst_union22_n0
POSTHOOK: Input: default@dst_union22_n0@ds=1
POSTHOOK: Output: default@dst_union22_n0@ds=2
OPTIMIZED SQL: SELECT `k1`, `k2`, `k3`, `k4`
FROM (SELECT `k1`, `k2`, `k3`, `k4`
FROM `default`.`dst_union22_delta_n0`
WHERE `k0` <= 50 AND `ds` = '1'
UNION ALL
SELECT `t2`.`k1`, `t2`.`k2`, `t4`.`k3`, `t4`.`k4`
FROM (SELECT `k1`, `k2`, `ds` = '1' AS `=`
FROM `default`.`dst_union22_n0`
WHERE `k1` > 20) AS `t2`
LEFT JOIN (SELECT `k1`, `k3`, `k4`
FROM `default`.`dst_union22_delta_n0`
WHERE `k0` > 50 AND `k1` > 20 AND `ds` = '1') AS `t4` ON `t2`.`=` AND `t2`.`k1` = `t4`.`k1`)
GROUP BY `k1`, `k2`, `k3`, `k4`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 5 <- Map 6 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: dst_union22_delta_n0
                  filterExpr: ((UDFToDouble(k0) <= 50.0D) and (ds = '1')) (type: boolean)
                  Statistics: Num rows: 500 Data size: 221500 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(k0) <= 50.0D) (type: boolean)
                    Statistics: Num rows: 166 Data size: 73538 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: k1 (type: string), k2 (type: string), k3 (type: string), k4 (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 166 Data size: 59096 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3
                        Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                          null sort order: zzzz
                          numBuckets: -1
                          sort order: ++++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                          Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [dst_union22_delta_n0]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: ds=1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  partition values:
                    ds 1
                  properties:
                    column.name.delimiter ,
                    columns k0,k1,k2,k3,k4,k5
                    columns.types string:string:string:string:string:string
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.dst_union22_delta_n0
                    partition_columns ds
                    partition_columns.types string
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns k0,k1,k2,k3,k4,k5
                      columns.comments 
                      columns.types string:string:string:string:string:string
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.dst_union22_delta_n0
                      partition_columns ds
                      partition_columns.types string
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.dst_union22_delta_n0
                  name: default.dst_union22_delta_n0
            Truncated Path -> Alias:
              /dst_union22_delta_n0/ds=1 [dst_union22_delta_n0]
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: (UDFToDouble(k1) > 20.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 181000 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(k1) > 20.0D) (type: boolean)
                    Statistics: Num rows: 166 Data size: 60092 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: k1 (type: string), k2 (type: string), (ds = '1') (type: boolean)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 166 Data size: 30212 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Left Outer Join 0 to 1
                        filter mappings:
                          0 [1, 1]
                        filter predicates:
                          0 {_col2}
                          1 
                        Estimated key counts: Map 6 => 55
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col0, _col1, _col4, _col5
                        input vertices:
                          1 Map 6
                        Position of Big Table: 0
                        Statistics: Num rows: 221 Data size: 49306 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: _col0 (type: string), _col1 (type: string), _col4 (type: string), _col5 (type: string)
                          outputColumnNames: _col0, _col1, _col2, _col3
                          Statistics: Num rows: 221 Data size: 49306 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                            minReductionHashAggr: 0.4
                            mode: hash
                            outputColumnNames: _col0, _col1, _col2, _col3
                            Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              bucketingVersion: 2
                              key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                              null sort order: zzzz
                              numBuckets: -1
                              sort order: ++++
                              Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
                              Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                              tag: -1
                              auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [a]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: ds=1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  partition values:
                    ds 1
                  properties:
                    column.name.delimiter ,
                    columns k1,k2,k3,k4
                    columns.types string:string:string:string
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.dst_union22_n0
                    partition_columns ds
                    partition_columns.types string
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns k1,k2,k3,k4
                      columns.comments 
                      columns.types string:string:string:string
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.dst_union22_n0
                      partition_columns ds
                      partition_columns.types string
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.dst_union22_n0
                  name: default.dst_union22_n0
            Truncated Path -> Alias:
              /dst_union22_n0/ds=1 [a]
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: dst_union22_delta_n0
                  filterExpr: ((UDFToDouble(k0) > 50.0D) and (UDFToDouble(k1) > 20.0D)) (type: boolean)
                  Statistics: Num rows: 500 Data size: 176000 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: ((UDFToDouble(k0) > 50.0D) and (UDFToDouble(k1) > 20.0D)) (type: boolean)
                    Statistics: Num rows: 55 Data size: 19360 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: k1 (type: string), k3 (type: string), k4 (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 55 Data size: 14575 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: _col0 (type: string)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 55 Data size: 14575 Basic stats: COMPLETE Column stats: COMPLETE
                        tag: 1
                        value expressions: _col1 (type: string), _col2 (type: string)
                        auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [dst_union22_delta_n0]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: ds=1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  partition values:
                    ds 1
                  properties:
                    column.name.delimiter ,
                    columns k0,k1,k2,k3,k4,k5
                    columns.types string:string:string:string:string:string
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.dst_union22_delta_n0
                    partition_columns ds
                    partition_columns.types string
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns k0,k1,k2,k3,k4,k5
                      columns.comments 
                      columns.types string:string:string:string:string:string
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.dst_union22_delta_n0
                      partition_columns ds
                      partition_columns.types string
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.dst_union22_delta_n0
                  name: default.dst_union22_delta_n0
            Truncated Path -> Alias:
              /dst_union22_delta_n0/ds=1 [dst_union22_delta_n0]
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string), KEY._col3 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  bucketingVersion: 2
                  compressed: false
                  GlobalTableId: 1
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Static Partition Specification: ds=2/
                  Statistics: Num rows: 387 Data size: 108402 Basic stats: COMPLETE Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      properties:
                        bucketing_version 2
                        column.name.delimiter ,
                        columns k1,k2,k3,k4
                        columns.comments 
                        columns.types string:string:string:string
#### A masked pattern was here ####
                        location hdfs://### HDFS PATH ###
                        name default.dst_union22_n0
                        partition_columns ds
                        partition_columns.types string
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.dst_union22_n0
                  TotalFiles: 1
                  GatherStats: true
                  MultiFileSpray: false
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), '2' (type: string)
                  outputColumnNames: k1, k2, k3, k4, ds
                  Statistics: Num rows: 387 Data size: 141297 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: max(length(k1)), avg(COALESCE(length(k1),0)), count(1), count(k1), compute_bit_vector_hll(k1), max(length(k2)), avg(COALESCE(length(k2),0)), count(k2), compute_bit_vector_hll(k2), max(length(k3)), avg(COALESCE(length(k3),0)), count(k3), compute_bit_vector_hll(k3), max(length(k4)), avg(COALESCE(length(k4),0)), count(k4), compute_bit_vector_hll(k4)
                    keys: ds (type: string)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                    Statistics: Num rows: 1 Data size: 1021 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      bucketingVersion: 2
                      key expressions: _col0 (type: string)
                      null sort order: z
                      numBuckets: -1
                      sort order: +
                      Map-reduce partition columns: _col0 (type: string)
                      Statistics: Num rows: 1 Data size: 1021 Basic stats: COMPLETE Column stats: COMPLETE
                      tag: -1
                      value expressions: _col1 (type: int), _col2 (type: struct<count:bigint,sum:double,input:int>), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary), _col6 (type: int), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: bigint), _col9 (type: binary), _col10 (type: int), _col11 (type: struct<count:bigint,sum:double,input:int>), _col12 (type: bigint), _col13 (type: binary), _col14 (type: int), _col15 (type: struct<count:bigint,sum:double,input:int>), _col16 (type: bigint), _col17 (type: binary)
                      auto parallelism: true
        Reducer 4 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12), max(VALUE._col13), avg(VALUE._col14), count(VALUE._col15), compute_bit_vector_hll(VALUE._col16)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                Statistics: Num rows: 1 Data size: 749 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col1,0)) (type: bigint), COALESCE(_col2,0) (type: double), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col6,0)) (type: bigint), COALESCE(_col7,0) (type: double), (_col3 - _col8) (type: bigint), COALESCE(ndv_compute_bit_vector(_col9),0) (type: bigint), _col9 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col10,0)) (type: bigint), COALESCE(_col11,0) (type: double), (_col3 - _col12) (type: bigint), COALESCE(ndv_compute_bit_vector(_col13),0) (type: bigint), _col13 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col14,0)) (type: bigint), COALESCE(_col15,0) (type: double), (_col3 - _col16) (type: bigint), COALESCE(ndv_compute_bit_vector(_col17),0) (type: bigint), _col17 (type: binary), _col0 (type: string)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24
                  Statistics: Num rows: 1 Data size: 1149 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    bucketingVersion: 2
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://### HDFS PATH ###
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 1149 Basic stats: COMPLETE Column stats: COMPLETE
                    Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          bucketing_version -1
                          columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11,_col12,_col13,_col14,_col15,_col16,_col17,_col18,_col19,_col20,_col21,_col22,_col23,_col24
                          columns.types string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary:string:bigint:double:bigint:bigint:binary:string
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
        Union 2 
            Vertex: Union 2

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            ds 2
          replace: true
          source: hdfs://### HDFS PATH ###
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucketing_version 2
                column.name.delimiter ,
                columns k1,k2,k3,k4
                columns.comments 
                columns.types string:string:string:string
#### A masked pattern was here ####
                location hdfs://### HDFS PATH ###
                name default.dst_union22_n0
                partition_columns ds
                partition_columns.types string
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.dst_union22_n0

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: hdfs://### HDFS PATH ###
      Column Stats Desc:
          Columns: k1, k2, k3, k4
          Column Types: string, string, string, string
          Table: default.dst_union22_n0
          Is Table Level Stats: false

PREHOOK: query: insert overwrite table dst_union22_n0 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta_n0 where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22_n0 a left outer join (select * from dst_union22_delta_n0 where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq
PREHOOK: type: QUERY
PREHOOK: Input: default@dst_union22_delta_n0
PREHOOK: Input: default@dst_union22_delta_n0@ds=1
PREHOOK: Input: default@dst_union22_n0
PREHOOK: Input: default@dst_union22_n0@ds=1
PREHOOK: Output: default@dst_union22_n0@ds=2
POSTHOOK: query: insert overwrite table dst_union22_n0 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta_n0 where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22_n0 a left outer join (select * from dst_union22_delta_n0 where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dst_union22_delta_n0
POSTHOOK: Input: default@dst_union22_delta_n0@ds=1
POSTHOOK: Input: default@dst_union22_n0
POSTHOOK: Input: default@dst_union22_n0@ds=1
POSTHOOK: Output: default@dst_union22_n0@ds=2
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=2).k1 EXPRESSION [(dst_union22_delta_n0)dst_union22_delta_n0.FieldSchema(name:k1, type:string, comment:null), (dst_union22_n0)a.FieldSchema(name:k1, type:string, comment:null), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=2).k2 EXPRESSION [(dst_union22_delta_n0)dst_union22_delta_n0.FieldSchema(name:k2, type:string, comment:null), (dst_union22_n0)a.FieldSchema(name:k2, type:string, comment:null), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=2).k3 EXPRESSION [(dst_union22_delta_n0)dst_union22_delta_n0.FieldSchema(name:k3, type:string, comment:null), ]
POSTHOOK: Lineage: dst_union22_n0 PARTITION(ds=2).k4 EXPRESSION [(dst_union22_delta_n0)dst_union22_delta_n0.FieldSchema(name:k4, type:string, comment:null), ]
PREHOOK: query: select * from dst_union22_n0 where ds = '2'
PREHOOK: type: QUERY
PREHOOK: Input: default@dst_union22_n0
PREHOOK: Input: default@dst_union22_n0@ds=2
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from dst_union22_n0 where ds = '2'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dst_union22_n0
POSTHOOK: Input: default@dst_union22_n0@ds=2
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0	0	val_0	2
103	val_103	103	val_103	2
104	val_104	104	val_104	2
105	val_105	105	val_105	2
111	val_111	111	val_111	2
116	val_116	116	val_116	2
119	val_119	119	val_119	2
12	val_12	12	val_12	2
120	val_120	120	val_120	2
126	val_126	126	val_126	2
128	val_128	128	val_128	2
129	val_129	129	val_129	2
131	val_131	131	val_131	2
134	val_134	134	val_134	2
136	val_136	136	val_136	2
138	val_138	138	val_138	2
150	val_150	150	val_150	2
155	val_155	155	val_155	2
157	val_157	157	val_157	2
158	val_158	158	val_158	2
160	val_160	160	val_160	2
164	val_164	164	val_164	2
167	val_167	167	val_167	2
169	val_169	169	val_169	2
170	val_170	170	val_170	2
174	val_174	174	val_174	2
175	val_175	175	val_175	2
177	val_177	177	val_177	2
178	val_178	178	val_178	2
179	val_179	179	val_179	2
18	val_18	18	val_18	2
187	val_187	187	val_187	2
191	val_191	191	val_191	2
193	val_193	193	val_193	2
194	val_194	194	val_194	2
197	val_197	197	val_197	2
199	val_199	199	val_199	2
2	val_2	2	val_2	2
200	val_200	200	val_200	2
201	val_201	201	val_201	2
202	val_202	202	val_202	2
205	val_205	205	val_205	2
208	val_208	208	val_208	2
216	val_216	216	val_216	2
217	val_217	217	val_217	2
218	val_218	218	val_218	2
219	val_219	219	val_219	2
221	val_221	221	val_221	2
229	val_229	229	val_229	2
230	val_230	230	val_230	2
233	val_233	233	val_233	2
235	val_235	235	val_235	2
237	val_237	237	val_237	2
238	val_238	238	val_238	2
24	val_24	24	val_24	2
241	val_241	241	val_241	2
244	val_244	244	val_244	2
252	val_252	252	val_252	2
256	val_256	256	val_256	2
257	val_257	257	val_257	2
26	val_26	NULL	NULL	2
260	val_260	260	val_260	2
266	val_266	266	val_266	2
274	val_274	274	val_274	2
277	val_277	277	val_277	2
28	val_28	NULL	NULL	2
280	val_280	280	val_280	2
281	val_281	281	val_281	2
282	val_282	282	val_282	2
285	val_285	285	val_285	2
287	val_287	287	val_287	2
288	val_288	288	val_288	2
291	val_291	291	val_291	2
302	val_302	302	val_302	2
305	val_305	305	val_305	2
306	val_306	306	val_306	2
308	val_308	308	val_308	2
311	val_311	311	val_311	2
316	val_316	316	val_316	2
318	val_318	318	val_318	2
321	val_321	321	val_321	2
327	val_327	327	val_327	2
33	val_33	33	val_33	2
33	val_33	NULL	NULL	2
331	val_331	331	val_331	2
333	val_333	333	val_333	2
335	val_335	335	val_335	2
336	val_336	336	val_336	2
344	val_344	344	val_344	2
345	val_345	345	val_345	2
356	val_356	356	val_356	2
360	val_360	360	val_360	2
362	val_362	362	val_362	2
364	val_364	364	val_364	2
366	val_366	366	val_366	2
367	val_367	367	val_367	2
373	val_373	373	val_373	2
378	val_378	378	val_378	2
379	val_379	379	val_379	2
384	val_384	384	val_384	2
386	val_386	386	val_386	2
389	val_389	389	val_389	2
393	val_393	393	val_393	2
394	val_394	394	val_394	2
395	val_395	395	val_395	2
397	val_397	397	val_397	2
4	val_4	4	val_4	2
402	val_402	402	val_402	2
403	val_403	403	val_403	2
406	val_406	406	val_406	2
407	val_407	407	val_407	2
41	val_41	41	val_41	2
41	val_41	NULL	NULL	2
413	val_413	413	val_413	2
418	val_418	418	val_418	2
419	val_419	419	val_419	2
424	val_424	424	val_424	2
436	val_436	436	val_436	2
44	val_44	44	val_44	2
44	val_44	NULL	NULL	2
448	val_448	448	val_448	2
449	val_449	449	val_449	2
452	val_452	452	val_452	2
453	val_453	453	val_453	2
460	val_460	460	val_460	2
466	val_466	466	val_466	2
467	val_467	467	val_467	2
468	val_468	468	val_468	2
469	val_469	469	val_469	2
47	val_47	NULL	NULL	2
470	val_470	470	val_470	2
475	val_475	475	val_475	2
479	val_479	479	val_479	2
480	val_480	480	val_480	2
481	val_481	481	val_481	2
487	val_487	487	val_487	2
490	val_490	490	val_490	2
493	val_493	493	val_493	2
494	val_494	494	val_494	2
495	val_495	495	val_495	2
496	val_496	496	val_496	2
5	val_5	5	val_5	2
51	val_51	51	val_51	2
53	val_53	53	val_53	2
64	val_64	64	val_64	2
66	val_66	66	val_66	2
69	val_69	69	val_69	2
74	val_74	74	val_74	2
76	val_76	76	val_76	2
8	val_8	8	val_8	2
82	val_82	82	val_82	2
9	val_9	9	val_9	2
90	val_90	90	val_90	2
92	val_92	92	val_92	2
95	val_95	95	val_95	2
10	val_10	10	val_10	2
100	val_100	100	val_100	2
11	val_11	11	val_11	2
113	val_113	113	val_113	2
114	val_114	114	val_114	2
118	val_118	118	val_118	2
125	val_125	125	val_125	2
133	val_133	133	val_133	2
137	val_137	137	val_137	2
143	val_143	143	val_143	2
145	val_145	145	val_145	2
146	val_146	146	val_146	2
149	val_149	149	val_149	2
15	val_15	15	val_15	2
152	val_152	152	val_152	2
153	val_153	153	val_153	2
156	val_156	156	val_156	2
162	val_162	162	val_162	2
163	val_163	163	val_163	2
165	val_165	165	val_165	2
166	val_166	166	val_166	2
168	val_168	168	val_168	2
17	val_17	17	val_17	2
172	val_172	172	val_172	2
176	val_176	176	val_176	2
180	val_180	180	val_180	2
181	val_181	181	val_181	2
183	val_183	183	val_183	2
186	val_186	186	val_186	2
189	val_189	189	val_189	2
19	val_19	19	val_19	2
190	val_190	190	val_190	2
192	val_192	192	val_192	2
195	val_195	195	val_195	2
196	val_196	196	val_196	2
20	val_20	20	val_20	2
203	val_203	203	val_203	2
207	val_207	207	val_207	2
209	val_209	209	val_209	2
213	val_213	213	val_213	2
214	val_214	214	val_214	2
222	val_222	222	val_222	2
223	val_223	223	val_223	2
224	val_224	224	val_224	2
226	val_226	226	val_226	2
228	val_228	228	val_228	2
239	val_239	239	val_239	2
24	val_24	NULL	NULL	2
242	val_242	242	val_242	2
247	val_247	247	val_247	2
248	val_248	248	val_248	2
249	val_249	249	val_249	2
255	val_255	255	val_255	2
258	val_258	258	val_258	2
26	val_26	26	val_26	2
262	val_262	262	val_262	2
263	val_263	263	val_263	2
265	val_265	265	val_265	2
27	val_27	27	val_27	2
27	val_27	NULL	NULL	2
272	val_272	272	val_272	2
273	val_273	273	val_273	2
275	val_275	275	val_275	2
278	val_278	278	val_278	2
28	val_28	28	val_28	2
283	val_283	283	val_283	2
284	val_284	284	val_284	2
286	val_286	286	val_286	2
289	val_289	289	val_289	2
292	val_292	292	val_292	2
296	val_296	296	val_296	2
298	val_298	298	val_298	2
30	val_30	30	val_30	2
30	val_30	NULL	NULL	2
307	val_307	307	val_307	2
309	val_309	309	val_309	2
310	val_310	310	val_310	2
315	val_315	315	val_315	2
317	val_317	317	val_317	2
322	val_322	322	val_322	2
323	val_323	323	val_323	2
325	val_325	325	val_325	2
332	val_332	332	val_332	2
338	val_338	338	val_338	2
339	val_339	339	val_339	2
34	val_34	34	val_34	2
34	val_34	NULL	NULL	2
341	val_341	341	val_341	2
342	val_342	342	val_342	2
348	val_348	348	val_348	2
35	val_35	35	val_35	2
35	val_35	NULL	NULL	2
351	val_351	351	val_351	2
353	val_353	353	val_353	2
365	val_365	365	val_365	2
368	val_368	368	val_368	2
369	val_369	369	val_369	2
37	val_37	37	val_37	2
37	val_37	NULL	NULL	2
374	val_374	374	val_374	2
375	val_375	375	val_375	2
377	val_377	377	val_377	2
382	val_382	382	val_382	2
392	val_392	392	val_392	2
396	val_396	396	val_396	2
399	val_399	399	val_399	2
400	val_400	400	val_400	2
401	val_401	401	val_401	2
404	val_404	404	val_404	2
409	val_409	409	val_409	2
411	val_411	411	val_411	2
414	val_414	414	val_414	2
417	val_417	417	val_417	2
42	val_42	42	val_42	2
42	val_42	NULL	NULL	2
421	val_421	421	val_421	2
427	val_427	427	val_427	2
429	val_429	429	val_429	2
43	val_43	43	val_43	2
43	val_43	NULL	NULL	2
430	val_430	430	val_430	2
431	val_431	431	val_431	2
432	val_432	432	val_432	2
435	val_435	435	val_435	2
437	val_437	437	val_437	2
438	val_438	438	val_438	2
439	val_439	439	val_439	2
443	val_443	443	val_443	2
444	val_444	444	val_444	2
446	val_446	446	val_446	2
454	val_454	454	val_454	2
455	val_455	455	val_455	2
457	val_457	457	val_457	2
458	val_458	458	val_458	2
459	val_459	459	val_459	2
462	val_462	462	val_462	2
463	val_463	463	val_463	2
47	val_47	47	val_47	2
472	val_472	472	val_472	2
477	val_477	477	val_477	2
478	val_478	478	val_478	2
482	val_482	482	val_482	2
483	val_483	483	val_483	2
484	val_484	484	val_484	2
485	val_485	485	val_485	2
489	val_489	489	val_489	2
491	val_491	491	val_491	2
492	val_492	492	val_492	2
497	val_497	497	val_497	2
498	val_498	498	val_498	2
54	val_54	54	val_54	2
57	val_57	57	val_57	2
58	val_58	58	val_58	2
65	val_65	65	val_65	2
67	val_67	67	val_67	2
70	val_70	70	val_70	2
72	val_72	72	val_72	2
77	val_77	77	val_77	2
78	val_78	78	val_78	2
80	val_80	80	val_80	2
83	val_83	83	val_83	2
84	val_84	84	val_84	2
85	val_85	85	val_85	2
86	val_86	86	val_86	2
87	val_87	87	val_87	2
96	val_96	96	val_96	2
97	val_97	97	val_97	2
98	val_98	98	val_98	2
PREHOOK: query: explain
select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT 
  select key as key2, value as value2 from src) s
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT 
  select key as key2, value as value2 from src) s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 4 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Transform Operator
                      command: cat
                      output info:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT 
  select key as key2, value as value2 from src) s
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT 
  select key as key2, value as value2 from src) s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
10	val_10
100	val_100
103	val_103
104	val_104
105	val_105
11	val_11
111	val_111
114	val_114
116	val_116
118	val_118
125	val_125
126	val_126
131	val_131
133	val_133
134	val_134
143	val_143
15	val_15
150	val_150
152	val_152
155	val_155
157	val_157
163	val_163
164	val_164
167	val_167
168	val_168
169	val_169
17	val_17
170	val_170
174	val_174
175	val_175
176	val_176
177	val_177
18	val_18
180	val_180
183	val_183
187	val_187
189	val_189
19	val_19
190	val_190
191	val_191
192	val_192
193	val_193
195	val_195
197	val_197
199	val_199
2	val_2
20	val_20
200	val_200
201	val_201
202	val_202
203	val_203
205	val_205
207	val_207
208	val_208
213	val_213
214	val_214
216	val_216
217	val_217
221	val_221
229	val_229
230	val_230
237	val_237
238	val_238
239	val_239
24	val_24
241	val_241
244	val_244
248	val_248
252	val_252
258	val_258
26	val_26
263	val_263
27	val_27
272	val_272
273	val_273
274	val_274
278	val_278
281	val_281
282	val_282
283	val_283
285	val_285
286	val_286
287	val_287
288	val_288
291	val_291
298	val_298
30	val_30
302	val_302
305	val_305
306	val_306
307	val_307
309	val_309
315	val_315
316	val_316
321	val_321
322	val_322
323	val_323
325	val_325
33	val_33
332	val_332
333	val_333
335	val_335
336	val_336
338	val_338
34	val_34
344	val_344
348	val_348
35	val_35
353	val_353
360	val_360
362	val_362
366	val_366
367	val_367
373	val_373
379	val_379
386	val_386
394	val_394
399	val_399
4	val_4
401	val_401
402	val_402
404	val_404
406	val_406
409	val_409
41	val_41
411	val_411
413	val_413
418	val_418
419	val_419
421	val_421
424	val_424
427	val_427
429	val_429
431	val_431
432	val_432
435	val_435
436	val_436
437	val_437
438	val_438
444	val_444
452	val_452
453	val_453
455	val_455
457	val_457
459	val_459
463	val_463
466	val_466
47	val_47
472	val_472
475	val_475
478	val_478
479	val_479
482	val_482
483	val_483
484	val_484
492	val_492
494	val_494
498	val_498
5	val_5
54	val_54
57	val_57
65	val_65
69	val_69
72	val_72
76	val_76
78	val_78
8	val_8
80	val_80
90	val_90
98	val_98
0	val_0
113	val_113
119	val_119
12	val_12
120	val_120
128	val_128
129	val_129
136	val_136
137	val_137
138	val_138
145	val_145
146	val_146
149	val_149
153	val_153
156	val_156
158	val_158
160	val_160
162	val_162
165	val_165
166	val_166
172	val_172
178	val_178
179	val_179
181	val_181
186	val_186
194	val_194
196	val_196
209	val_209
218	val_218
219	val_219
222	val_222
223	val_223
224	val_224
226	val_226
228	val_228
233	val_233
235	val_235
242	val_242
247	val_247
249	val_249
255	val_255
256	val_256
257	val_257
260	val_260
262	val_262
265	val_265
266	val_266
275	val_275
277	val_277
28	val_28
280	val_280
284	val_284
289	val_289
292	val_292
296	val_296
308	val_308
310	val_310
311	val_311
317	val_317
318	val_318
327	val_327
331	val_331
339	val_339
341	val_341
342	val_342
345	val_345
351	val_351
356	val_356
364	val_364
365	val_365
368	val_368
369	val_369
37	val_37
374	val_374
375	val_375
377	val_377
378	val_378
382	val_382
384	val_384
389	val_389
392	val_392
393	val_393
395	val_395
396	val_396
397	val_397
400	val_400
403	val_403
407	val_407
414	val_414
417	val_417
42	val_42
43	val_43
430	val_430
439	val_439
44	val_44
443	val_443
446	val_446
448	val_448
449	val_449
454	val_454
458	val_458
460	val_460
462	val_462
467	val_467
468	val_468
469	val_469
470	val_470
477	val_477
480	val_480
481	val_481
485	val_485
487	val_487
489	val_489
490	val_490
491	val_491
493	val_493
495	val_495
496	val_496
497	val_497
51	val_51
53	val_53
58	val_58
64	val_64
66	val_66
67	val_67
70	val_70
74	val_74
77	val_77
82	val_82
83	val_83
84	val_84
85	val_85
86	val_86
87	val_87
9	val_9
92	val_92
95	val_95
96	val_96
97	val_97
PREHOOK: query: create table src2_n2 as select key, count(1) as count from src group by key
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@src2_n2
POSTHOOK: query: create table src2_n2 as select key, count(1) as count from src group by key
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src2_n2
POSTHOOK: Lineage: src2_n2.count EXPRESSION [(src)src.null, ]
POSTHOOK: Lineage: src2_n2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: create table src3 as select * from src2_n2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2_n2
PREHOOK: Output: database:default
PREHOOK: Output: default@src3
POSTHOOK: query: create table src3 as select * from src2_n2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2_n2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src3
POSTHOOK: Lineage: src3.count SIMPLE [(src2_n2)src2_n2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src3.key SIMPLE [(src2_n2)src2_n2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: create table src4 as select * from src2_n2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2_n2
PREHOOK: Output: database:default
PREHOOK: Output: default@src4
POSTHOOK: query: create table src4 as select * from src2_n2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2_n2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src4
POSTHOOK: Lineage: src4.count SIMPLE [(src2_n2)src2_n2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src4.key SIMPLE [(src2_n2)src2_n2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: create table src5_n1 as select * from src2_n2
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src2_n2
PREHOOK: Output: database:default
PREHOOK: Output: default@src5_n1
POSTHOOK: query: create table src5_n1 as select * from src2_n2
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src2_n2
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src5_n1
POSTHOOK: Lineage: src5_n1.count SIMPLE [(src2_n2)src2_n2.FieldSchema(name:count, type:bigint, comment:null), ]
POSTHOOK: Lineage: src5_n1.key SIMPLE [(src2_n2)src2_n2.FieldSchema(name:key, type:string, comment:null), ]
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5_n1 where key < 10 group by key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5_n1 where key < 10 group by key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
OPTIMIZED SQL: SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM `default`.`src2_n2`
WHERE `key` < 10
UNION ALL
SELECT `key`, `count`
FROM `default`.`src3`
WHERE `key` < 10)
GROUP BY `key`, `count`
UNION ALL
SELECT `key`, `count`
FROM `default`.`src4`
WHERE `key` < 10)
GROUP BY `key`, `count`
UNION ALL
SELECT `key`, COUNT(*) AS `$f1`
FROM `default`.`src5_n1`
WHERE `key` < 10
GROUP BY `key`)
GROUP BY `key`, `count`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 8 <- Union 2 (CONTAINS)
        Map 9 <- Union 4 (CONTAINS)
        Reducer 11 <- Map 10 (SIMPLE_EDGE), Union 6 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 5 <- Union 4 (SIMPLE_EDGE), Union 6 (CONTAINS)
        Reducer 7 <- Union 6 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src2_n2
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src2_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src2_n2
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src2_n2
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src2_n2
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src2_n2
                  name: default.src2_n2
            Truncated Path -> Alias:
              /src2_n2 [src2_n2]
        Map 10 
            Map Operator Tree:
                TableScan
                  alias: src5_n1
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 26883 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5048544
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: _col0 (type: string)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                        tag: -1
                        value expressions: _col1 (type: bigint)
                        auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src5_n1]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src5_n1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src5_n1
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src5_n1
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src5_n1
                  name: default.src5_n1
            Truncated Path -> Alias:
              /src5_n1 [src5_n1]
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: src3
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src3]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src3
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src3
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src3
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src3
                  name: default.src3
            Truncated Path -> Alias:
              /src3 [src3]
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: src4
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src4]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src4
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src4
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src4
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src4
                  name: default.src4
            Truncated Path -> Alias:
              /src4 [src4]
        Reducer 11 
            Execution mode: vectorized
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Reducer 5 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Reducer 7 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  bucketingVersion: 2
                  compressed: false
                  GlobalTableId: 0
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        bucketing_version -1
                        columns _col0,_col1
                        columns.types string:bigint
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
        Union 2 
            Vertex: Union 2
        Union 4 
            Vertex: Union 4
        Union 6 
            Vertex: Union 6

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5_n1 where key < 10 group by key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5_n1 where key < 10 group by key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	1
0	3
2	1
4	1
5	3
5	1
8	1
9	1
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
OPTIMIZED SQL: SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM `default`.`src2_n2`
WHERE `key` < 10
UNION ALL
SELECT `key`, `count`
FROM `default`.`src3`
WHERE `key` < 10)
GROUP BY `key`, `count`
UNION ALL
SELECT `t6`.`key`, `t8`.`count`
FROM (SELECT `key`
FROM `default`.`src4`
WHERE `key` < 10) AS `t6`
INNER JOIN (SELECT `key`, `count`
FROM `default`.`src5_n1`
WHERE `key` < 10) AS `t8` ON `t6`.`key` = `t8`.`key`)
GROUP BY `key`, `count`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 6 <- Union 2 (CONTAINS)
        Map 8 <- Map 7 (BROADCAST_EDGE), Union 4 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 5 <- Union 4 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src2_n2
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src2_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src2_n2
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src2_n2
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src2_n2
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src2_n2
                  name: default.src2_n2
            Truncated Path -> Alias:
              /src2_n2 [src2_n2]
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: src3
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src3]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src3
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src3
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src3
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src3
                  name: default.src3
            Truncated Path -> Alias:
              /src3 [src3]
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 26883 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: _col0 (type: string)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                        tag: 0
                        auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [a]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src4
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src4
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src4
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src4
                  name: default.src4
            Truncated Path -> Alias:
              /src4 [a]
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        Estimated key counts: Map 7 => 103
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col0, _col2
                        input vertices:
                          0 Map 7
                        Position of Big Table: 1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: _col0 (type: string), _col2 (type: bigint)
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: string), _col1 (type: bigint)
                            minReductionHashAggr: 0.4
                            mode: hash
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              bucketingVersion: 2
                              key expressions: _col0 (type: string), _col1 (type: bigint)
                              null sort order: zz
                              numBuckets: -1
                              sort order: ++
                              Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                              Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                              tag: -1
                              auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [b]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src5_n1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src5_n1
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src5_n1
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src5_n1
                  name: default.src5_n1
            Truncated Path -> Alias:
              /src5_n1 [b]
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Reducer 5 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  bucketingVersion: 2
                  compressed: false
                  GlobalTableId: 0
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        bucketing_version -1
                        columns _col0,_col1
                        columns.types string:bigint
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
        Union 2 
            Vertex: Union 2
        Union 4 
            Vertex: Union 4

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	3
2	1
4	1
5	3
8	1
9	1
PREHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10 group by a.key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain extended
select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10 group by a.key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
OPTIMIZED SQL: SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM (SELECT `key`, `count`
FROM `default`.`src2_n2`
WHERE `key` < 10
UNION ALL
SELECT `key`, `count`
FROM `default`.`src3`
WHERE `key` < 10)
GROUP BY `key`, `count`
UNION ALL
SELECT `t6`.`key`, COUNT(*) AS `$f1`
FROM (SELECT `key`
FROM `default`.`src4`
WHERE `key` < 10) AS `t6`
INNER JOIN (SELECT `key`
FROM `default`.`src5_n1`
WHERE `key` < 10) AS `t8` ON `t6`.`key` = `t8`.`key`
GROUP BY `t6`.`key`)
GROUP BY `key`, `count`
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 6 <- Union 2 (CONTAINS)
        Map 7 <- Map 9 (BROADCAST_EDGE)
        Reducer 3 <- Union 2 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 5 <- Union 4 (SIMPLE_EDGE)
        Reducer 8 <- Map 7 (SIMPLE_EDGE), Union 4 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src2_n2
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src2_n2]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src2_n2
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src2_n2
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src2_n2
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src2_n2
                  name: default.src2_n2
            Truncated Path -> Alias:
              /src2_n2 [src2_n2]
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: src3
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 29355 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), count (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: bigint)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          bucketingVersion: 2
                          key expressions: _col0 (type: string), _col1 (type: bigint)
                          null sort order: zz
                          numBuckets: -1
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                          Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                          tag: -1
                          auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [src3]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src3
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src3
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src3
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src3
                  name: default.src3
            Truncated Path -> Alias:
              /src3 [src3]
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 26883 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        Estimated key counts: Map 9 => 103
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 9
                        Position of Big Table: 0
                        Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: string)
                          minReductionHashAggr: 0.5048544
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                          Reduce Output Operator
                            bucketingVersion: 2
                            key expressions: _col0 (type: string)
                            null sort order: z
                            numBuckets: -1
                            sort order: +
                            Map-reduce partition columns: _col0 (type: string)
                            Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                            tag: -1
                            value expressions: _col1 (type: bigint)
                            auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [a]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src4
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src4
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src4
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src4
                  name: default.src4
            Truncated Path -> Alias:
              /src4 [a]
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: (UDFToDouble(key) < 10.0D) (type: boolean)
                  Statistics: Num rows: 309 Data size: 26883 Basic stats: COMPLETE Column stats: COMPLETE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: (UDFToDouble(key) < 10.0D) (type: boolean)
                    Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: _col0 (type: string)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 103 Data size: 8961 Basic stats: COMPLETE Column stats: COMPLETE
                        tag: 1
                        auto parallelism: true
            Execution mode: vectorized
            Path -> Alias:
              hdfs://### HDFS PATH ### [b]
            Path -> Partition:
              hdfs://### HDFS PATH ### 
                Partition
                  base file name: src5_n1
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    bucket_count -1
                    bucketing_version 2
                    column.name.delimiter ,
                    columns key,count
                    columns.types string:bigint
#### A masked pattern was here ####
                    location hdfs://### HDFS PATH ###
                    name default.src5_n1
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns key,count
                      columns.comments 
                      columns.types string:bigint
#### A masked pattern was here ####
                      location hdfs://### HDFS PATH ###
                      name default.src5_n1
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.src5_n1
                  name: default.src5_n1
            Truncated Path -> Alias:
              /src5_n1 [b]
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 103 Data size: 9785 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Reducer 5 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  bucketingVersion: 2
                  compressed: false
                  GlobalTableId: 0
                  directory: hdfs://### HDFS PATH ###
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Stats Publishing Key Prefix: hdfs://### HDFS PATH ###
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        bucketing_version -1
                        columns _col0,_col1
                        columns.types string:bigint
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
        Reducer 8 
            Execution mode: vectorized
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 51 Data size: 4845 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    bucketingVersion: 2
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    numBuckets: -1
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 77 Data size: 7315 Basic stats: COMPLETE Column stats: COMPLETE
                    tag: -1
                    auto parallelism: true
        Union 2 
            Vertex: Union 2
        Union 4 
            Vertex: Union 4

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10 group by a.key
)s
PREHOOK: type: QUERY
PREHOOK: Input: default@src2_n2
PREHOOK: Input: default@src3
PREHOOK: Input: default@src4
PREHOOK: Input: default@src5_n1
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select s.key, s.count from (
  select key, count from src2_n2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5_n1 b on a.key=b.key where a.key < 10 group by a.key
)s
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src2_n2
POSTHOOK: Input: default@src3
POSTHOOK: Input: default@src4
POSTHOOK: Input: default@src5_n1
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	1
0	3
2	1
4	1
5	3
5	1
8	1
9	1
PREHOOK: query: create table tmp_srcpart_n0 like srcpart
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@tmp_srcpart_n0
POSTHOOK: query: create table tmp_srcpart_n0 like srcpart
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tmp_srcpart_n0
PREHOOK: query: insert overwrite table tmp_srcpart_n0 partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11'
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Output: default@tmp_srcpart_n0@ds=2008-04-08/hr=11
POSTHOOK: query: insert overwrite table tmp_srcpart_n0 partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Output: default@tmp_srcpart_n0@ds=2008-04-08/hr=11
POSTHOOK: Lineage: tmp_srcpart_n0 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: tmp_srcpart_n0 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: explain
create table tmp_unionall_n0 as
SELECT count(1) as counts, key, value
FROM
(
  SELECT key, value FROM srcpart a WHERE a.ds='2008-04-08' and a.hr='11'

    UNION DISTINCT

  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM tmp_srcpart_n0 a WHERE a.ds='2008-04-08' and a.hr='11'
        UNION DISTINCT
      SELECT key, value FROM tmp_srcpart_n0 b WHERE b.ds='2008-04-08' and b.hr='11'
    )t
  ) master_table
) a GROUP BY key, value
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@tmp_srcpart_n0
PREHOOK: Input: default@tmp_srcpart_n0@ds=2008-04-08/hr=11
PREHOOK: Output: database:default
PREHOOK: Output: default@tmp_unionall_n0
POSTHOOK: query: explain
create table tmp_unionall_n0 as
SELECT count(1) as counts, key, value
FROM
(
  SELECT key, value FROM srcpart a WHERE a.ds='2008-04-08' and a.hr='11'

    UNION DISTINCT

  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM tmp_srcpart_n0 a WHERE a.ds='2008-04-08' and a.hr='11'
        UNION DISTINCT
      SELECT key, value FROM tmp_srcpart_n0 b WHERE b.ds='2008-04-08' and b.hr='11'
    )t
  ) master_table
) a GROUP BY key, value
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Input: default@tmp_srcpart_n0
POSTHOOK: Input: default@tmp_srcpart_n0@ds=2008-04-08/hr=11
POSTHOOK: Output: database:default
POSTHOOK: Output: default@tmp_unionall_n0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-4 depends on stages: Stage-0, Stage-2
  Stage-3 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 5 <- Union 6 (CONTAINS)
        Map 8 <- Union 6 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 7 <- Union 2 (CONTAINS), Union 6 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: ((ds = '2008-04-08') and (hr = '11')) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 375 Data size: 66750 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 375 Data size: 66750 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: ((ds = '2008-04-08') and (hr = '11')) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: ((ds = '2008-04-08') and (hr = '11')) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 375 Data size: 66750 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: count()
                  keys: _col0 (type: string), _col1 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 187 Data size: 34782 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col2 (type: bigint), _col0 (type: string), _col1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 187 Data size: 34782 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 187 Data size: 34782 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.tmp_unionall_n0
                    Select Operator
                      expressions: _col0 (type: bigint), _col1 (type: string), _col2 (type: string)
                      outputColumnNames: col1, col2, col3
                      Statistics: Num rows: 187 Data size: 34782 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: min(col1), max(col1), count(1), count(col1), compute_bit_vector_hll(col1), max(length(col2)), avg(COALESCE(length(col2),0)), count(col2), compute_bit_vector_hll(col2), max(length(col3)), avg(COALESCE(length(col3),0)), count(col3), compute_bit_vector_hll(col3)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                        Statistics: Num rows: 1 Data size: 640 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 640 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: bigint), _col1 (type: bigint), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary), _col9 (type: int), _col10 (type: struct<count:bigint,sum:double,input:int>), _col11 (type: bigint), _col12 (type: binary)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8), max(VALUE._col9), avg(VALUE._col10), count(VALUE._col11), compute_bit_vector_hll(VALUE._col12)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                Statistics: Num rows: 1 Data size: 504 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'LONG' (type: string), _col0 (type: bigint), _col1 (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col9,0)) (type: bigint), COALESCE(_col10,0) (type: double), (_col2 - _col11) (type: bigint), COALESCE(ndv_compute_bit_vector(_col12),0) (type: bigint), _col12 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                  Statistics: Num rows: 1 Data size: 796 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 796 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string)
                  outputColumnNames: _col0
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string)
                    mode: complete
                    outputColumnNames: _col0
                    Statistics: Num rows: 250 Data size: 21750 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: _col0 (type: string), _col0 (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 375 Data size: 66750 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 375 Data size: 66750 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2
        Union 6 
            Vertex: Union 6

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-4
    Create Table
      columns: counts bigint, key string, value string
      name: default.tmp_unionall_n0
      input format: org.apache.hadoop.mapred.TextInputFormat
      output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
      serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: counts, key, value
          Column Types: bigint, string, string
          Table: default.tmp_unionall_n0

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
          destination: hdfs://### HDFS PATH ###

PREHOOK: query: EXPLAIN
SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Map 5 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 key (type: string)
                        1 key (type: string)
                      outputColumnNames: _col0, _col1
                      input vertices:
                        1 Map 4
                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.99
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: string)
                      null sort order: z
                      sort order: +
                      Map-reduce partition columns: key (type: string)
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  filterExpr: ((ds = '2008-04-08') and (hr = '11')) (type: boolean)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  Lateral View Forward
                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: key, value
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      Lateral View Join Operator
                        outputColumnNames: _col0, _col1, _col7
                        Statistics: Num rows: 500 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
                        Select Operator
                          expressions: _col0 (type: string), _col1 (type: string)
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 500 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
                          Group By Operator
                            keys: _col0 (type: string), _col1 (type: string)
                            minReductionHashAggr: 0.99
                            mode: hash
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
                            Reduce Output Operator
                              key expressions: _col0 (type: string), _col1 (type: string)
                              null sort order: zz
                              sort order: ++
                              Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                              Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: array(1,2,3) (type: array<int>)
                      outputColumnNames: _col0
                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                      UDTF Operator
                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                        function name: explode
                        Lateral View Join Operator
                          outputColumnNames: _col0, _col1, _col7
                          Statistics: Num rows: 500 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
                          Select Operator
                            expressions: _col0 (type: string), _col1 (type: string)
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 500 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
                            Group By Operator
                              keys: _col0 (type: string), _col1 (type: string)
                              minReductionHashAggr: 0.99
                              mode: hash
                              outputColumnNames: _col0, _col1
                              Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
                              Reduce Output Operator
                                key expressions: _col0 (type: string), _col1 (type: string)
                                null sort order: zz
                                sort order: ++
                                Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                                Statistics: Num rows: 1050 Data size: 16467 Basic stats: COMPLETE Column stats: NONE
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 525 Data size: 8233 Basic stats: COMPLETE Column stats: NONE
                Group By Operator
                  aggregations: count(1)
                  keys: _col0 (type: string), _col1 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 262 Data size: 4108 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: bigint), _col0 (type: string), _col1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 262 Data size: 4108 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 262 Data size: 4108 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	0	val_0
1	10	val_10
1	100	val_100
1	103	val_103
1	104	val_104
1	105	val_105
1	11	val_11
1	111	val_111
1	113	val_113
1	114	val_114
1	116	val_116
1	118	val_118
1	119	val_119
1	12	val_12
1	120	val_120
1	125	val_125
1	126	val_126
1	128	val_128
1	129	val_129
1	131	val_131
1	133	val_133
1	134	val_134
1	136	val_136
1	137	val_137
1	138	val_138
1	143	val_143
1	145	val_145
1	146	val_146
1	149	val_149
1	15	val_15
1	150	val_150
1	152	val_152
1	153	val_153
1	155	val_155
1	156	val_156
1	157	val_157
1	158	val_158
1	160	val_160
1	162	val_162
1	163	val_163
1	164	val_164
1	165	val_165
1	166	val_166
1	167	val_167
1	168	val_168
1	169	val_169
1	17	val_17
1	170	val_170
1	172	val_172
1	174	val_174
1	175	val_175
1	176	val_176
1	177	val_177
1	178	val_178
1	179	val_179
1	18	val_18
1	180	val_180
1	181	val_181
1	183	val_183
1	186	val_186
1	187	val_187
1	189	val_189
1	19	val_19
1	190	val_190
1	191	val_191
1	192	val_192
1	193	val_193
1	194	val_194
1	195	val_195
1	196	val_196
1	197	val_197
1	199	val_199
1	2	val_2
1	20	val_20
1	200	val_200
1	201	val_201
1	202	val_202
1	203	val_203
1	205	val_205
1	207	val_207
1	208	val_208
1	209	val_209
1	213	val_213
1	214	val_214
1	216	val_216
1	217	val_217
1	218	val_218
1	219	val_219
1	221	val_221
1	222	val_222
1	223	val_223
1	224	val_224
1	226	val_226
1	228	val_228
1	229	val_229
1	230	val_230
1	233	val_233
1	235	val_235
1	237	val_237
1	238	val_238
1	239	val_239
1	24	val_24
1	241	val_241
1	242	val_242
1	244	val_244
1	247	val_247
1	248	val_248
1	249	val_249
1	252	val_252
1	255	val_255
1	256	val_256
1	257	val_257
1	258	val_258
1	26	val_26
1	260	val_260
1	262	val_262
1	263	val_263
1	265	val_265
1	266	val_266
1	27	val_27
1	272	val_272
1	273	val_273
1	274	val_274
1	275	val_275
1	277	val_277
1	278	val_278
1	28	val_28
1	280	val_280
1	281	val_281
1	282	val_282
1	283	val_283
1	284	val_284
1	285	val_285
1	286	val_286
1	287	val_287
1	288	val_288
1	289	val_289
1	291	val_291
1	292	val_292
1	296	val_296
1	298	val_298
1	30	val_30
1	302	val_302
1	305	val_305
1	306	val_306
1	307	val_307
1	308	val_308
1	309	val_309
1	310	val_310
1	311	val_311
1	315	val_315
1	316	val_316
1	317	val_317
1	318	val_318
1	321	val_321
1	322	val_322
1	323	val_323
1	325	val_325
1	327	val_327
1	33	val_33
1	331	val_331
1	332	val_332
1	333	val_333
1	335	val_335
1	336	val_336
1	338	val_338
1	339	val_339
1	34	val_34
1	341	val_341
1	342	val_342
1	344	val_344
1	345	val_345
1	348	val_348
1	35	val_35
1	351	val_351
1	353	val_353
1	356	val_356
1	360	val_360
1	362	val_362
1	364	val_364
1	365	val_365
1	366	val_366
1	367	val_367
1	368	val_368
1	369	val_369
1	37	val_37
1	373	val_373
1	374	val_374
1	375	val_375
1	377	val_377
1	378	val_378
1	379	val_379
1	382	val_382
1	384	val_384
1	386	val_386
1	389	val_389
1	392	val_392
1	393	val_393
1	394	val_394
1	395	val_395
1	396	val_396
1	397	val_397
1	399	val_399
1	4	val_4
1	400	val_400
1	401	val_401
1	402	val_402
1	403	val_403
1	404	val_404
1	406	val_406
1	407	val_407
1	409	val_409
1	41	val_41
1	411	val_411
1	413	val_413
1	414	val_414
1	417	val_417
1	418	val_418
1	419	val_419
1	42	val_42
1	421	val_421
1	424	val_424
1	427	val_427
1	429	val_429
1	43	val_43
1	430	val_430
1	431	val_431
1	432	val_432
1	435	val_435
1	436	val_436
1	437	val_437
1	438	val_438
1	439	val_439
1	44	val_44
1	443	val_443
1	444	val_444
1	446	val_446
1	448	val_448
1	449	val_449
1	452	val_452
1	453	val_453
1	454	val_454
1	455	val_455
1	457	val_457
1	458	val_458
1	459	val_459
1	460	val_460
1	462	val_462
1	463	val_463
1	466	val_466
1	467	val_467
1	468	val_468
1	469	val_469
1	47	val_47
1	470	val_470
1	472	val_472
1	475	val_475
1	477	val_477
1	478	val_478
1	479	val_479
1	480	val_480
1	481	val_481
1	482	val_482
1	483	val_483
1	484	val_484
1	485	val_485
1	487	val_487
1	489	val_489
1	490	val_490
1	491	val_491
1	492	val_492
1	493	val_493
1	494	val_494
1	495	val_495
1	496	val_496
1	497	val_497
1	498	val_498
1	5	val_5
1	51	val_51
1	53	val_53
1	54	val_54
1	57	val_57
1	58	val_58
1	64	val_64
1	65	val_65
1	66	val_66
1	67	val_67
1	69	val_69
1	70	val_70
1	72	val_72
1	74	val_74
1	76	val_76
1	77	val_77
1	78	val_78
1	8	val_8
1	80	val_80
1	82	val_82
1	83	val_83
1	84	val_84
1	85	val_85
1	86	val_86
1	87	val_87
1	9	val_9
1	90	val_90
1	92	val_92
1	95	val_95
1	96	val_96
1	97	val_97
1	98	val_98
PREHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	10	val_10
1	100	val_100
1	103	val_103
1	104	val_104
1	105	val_105
1	11	val_11
1	111	val_111
1	114	val_114
1	116	val_116
1	118	val_118
1	125	val_125
1	126	val_126
1	131	val_131
1	133	val_133
1	134	val_134
1	143	val_143
1	15	val_15
1	150	val_150
1	152	val_152
1	155	val_155
1	157	val_157
1	163	val_163
1	164	val_164
1	167	val_167
1	168	val_168
1	169	val_169
1	17	val_17
1	170	val_170
1	174	val_174
1	175	val_175
1	176	val_176
1	177	val_177
1	18	val_18
1	180	val_180
1	183	val_183
1	187	val_187
1	189	val_189
1	19	val_19
1	190	val_190
1	191	val_191
1	192	val_192
1	193	val_193
1	195	val_195
1	197	val_197
1	199	val_199
1	2	val_2
1	20	val_20
1	200	val_200
1	201	val_201
1	202	val_202
1	203	val_203
1	205	val_205
1	207	val_207
1	208	val_208
1	213	val_213
1	214	val_214
1	216	val_216
1	217	val_217
1	221	val_221
1	229	val_229
1	230	val_230
1	237	val_237
1	238	val_238
1	239	val_239
1	24	val_24
1	241	val_241
1	244	val_244
1	248	val_248
1	252	val_252
1	258	val_258
1	26	val_26
1	263	val_263
1	27	val_27
1	272	val_272
1	273	val_273
1	274	val_274
1	278	val_278
1	281	val_281
1	282	val_282
1	283	val_283
1	285	val_285
1	286	val_286
1	287	val_287
1	288	val_288
1	291	val_291
1	298	val_298
1	30	val_30
1	302	val_302
1	305	val_305
1	306	val_306
1	307	val_307
1	309	val_309
1	315	val_315
1	316	val_316
1	321	val_321
1	322	val_322
1	323	val_323
1	325	val_325
1	33	val_33
1	332	val_332
1	333	val_333
1	335	val_335
1	336	val_336
1	338	val_338
1	34	val_34
1	344	val_344
1	348	val_348
1	35	val_35
1	353	val_353
1	360	val_360
1	362	val_362
1	366	val_366
1	367	val_367
1	373	val_373
1	379	val_379
1	386	val_386
1	394	val_394
1	399	val_399
1	4	val_4
1	401	val_401
1	402	val_402
1	404	val_404
1	406	val_406
1	409	val_409
1	41	val_41
1	411	val_411
1	413	val_413
1	418	val_418
1	419	val_419
1	421	val_421
1	424	val_424
1	427	val_427
1	429	val_429
1	431	val_431
1	432	val_432
1	435	val_435
1	436	val_436
1	437	val_437
1	438	val_438
1	444	val_444
1	452	val_452
1	453	val_453
1	455	val_455
1	457	val_457
1	459	val_459
1	463	val_463
1	466	val_466
1	47	val_47
1	472	val_472
1	475	val_475
1	478	val_478
1	479	val_479
1	482	val_482
1	483	val_483
1	484	val_484
1	492	val_492
1	494	val_494
1	498	val_498
1	5	val_5
1	54	val_54
1	57	val_57
1	65	val_65
1	69	val_69
1	72	val_72
1	76	val_76
1	78	val_78
1	8	val_8
1	80	val_80
1	90	val_90
1	98	val_98
1	0	val_0
1	113	val_113
1	119	val_119
1	12	val_12
1	120	val_120
1	128	val_128
1	129	val_129
1	136	val_136
1	137	val_137
1	138	val_138
1	145	val_145
1	146	val_146
1	149	val_149
1	153	val_153
1	156	val_156
1	158	val_158
1	160	val_160
1	162	val_162
1	165	val_165
1	166	val_166
1	172	val_172
1	178	val_178
1	179	val_179
1	181	val_181
1	186	val_186
1	194	val_194
1	196	val_196
1	209	val_209
1	218	val_218
1	219	val_219
1	222	val_222
1	223	val_223
1	224	val_224
1	226	val_226
1	228	val_228
1	233	val_233
1	235	val_235
1	242	val_242
1	247	val_247
1	249	val_249
1	255	val_255
1	256	val_256
1	257	val_257
1	260	val_260
1	262	val_262
1	265	val_265
1	266	val_266
1	275	val_275
1	277	val_277
1	28	val_28
1	280	val_280
1	284	val_284
1	289	val_289
1	292	val_292
1	296	val_296
1	308	val_308
1	310	val_310
1	311	val_311
1	317	val_317
1	318	val_318
1	327	val_327
1	331	val_331
1	339	val_339
1	341	val_341
1	342	val_342
1	345	val_345
1	351	val_351
1	356	val_356
1	364	val_364
1	365	val_365
1	368	val_368
1	369	val_369
1	37	val_37
1	374	val_374
1	375	val_375
1	377	val_377
1	378	val_378
1	382	val_382
1	384	val_384
1	389	val_389
1	392	val_392
1	393	val_393
1	395	val_395
1	396	val_396
1	397	val_397
1	400	val_400
1	403	val_403
1	407	val_407
1	414	val_414
1	417	val_417
1	42	val_42
1	43	val_43
1	430	val_430
1	439	val_439
1	44	val_44
1	443	val_443
1	446	val_446
1	448	val_448
1	449	val_449
1	454	val_454
1	458	val_458
1	460	val_460
1	462	val_462
1	467	val_467
1	468	val_468
1	469	val_469
1	470	val_470
1	477	val_477
1	480	val_480
1	481	val_481
1	485	val_485
1	487	val_487
1	489	val_489
1	490	val_490
1	491	val_491
1	493	val_493
1	495	val_495
1	496	val_496
1	497	val_497
1	51	val_51
1	53	val_53
1	58	val_58
1	64	val_64
1	66	val_66
1	67	val_67
1	70	val_70
1	74	val_74
1	77	val_77
1	82	val_82
1	83	val_83
1	84	val_84
1	85	val_85
1	86	val_86
1	87	val_87
1	9	val_9
1	92	val_92
1	95	val_95
1	96	val_96
1	97	val_97
PREHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@srcpart
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT 
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b 
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key 

UNION DISTINCT

select key, value 
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@srcpart
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
POSTHOOK: Output: hdfs://### HDFS PATH ###
1	10	val_10
1	100	val_100
1	103	val_103
1	104	val_104
1	105	val_105
1	11	val_11
1	111	val_111
1	114	val_114
1	116	val_116
1	118	val_118
1	125	val_125
1	126	val_126
1	131	val_131
1	133	val_133
1	134	val_134
1	143	val_143
1	15	val_15
1	150	val_150
1	152	val_152
1	155	val_155
1	157	val_157
1	163	val_163
1	164	val_164
1	167	val_167
1	168	val_168
1	169	val_169
1	17	val_17
1	170	val_170
1	174	val_174
1	175	val_175
1	176	val_176
1	177	val_177
1	18	val_18
1	180	val_180
1	183	val_183
1	187	val_187
1	189	val_189
1	19	val_19
1	190	val_190
1	191	val_191
1	192	val_192
1	193	val_193
1	195	val_195
1	197	val_197
1	199	val_199
1	2	val_2
1	20	val_20
1	200	val_200
1	201	val_201
1	202	val_202
1	203	val_203
1	205	val_205
1	207	val_207
1	208	val_208
1	213	val_213
1	214	val_214
1	216	val_216
1	217	val_217
1	221	val_221
1	229	val_229
1	230	val_230
1	237	val_237
1	238	val_238
1	239	val_239
1	24	val_24
1	241	val_241
1	244	val_244
1	248	val_248
1	252	val_252
1	258	val_258
1	26	val_26
1	263	val_263
1	27	val_27
1	272	val_272
1	273	val_273
1	274	val_274
1	278	val_278
1	281	val_281
1	282	val_282
1	283	val_283
1	285	val_285
1	286	val_286
1	287	val_287
1	288	val_288
1	291	val_291
1	298	val_298
1	30	val_30
1	302	val_302
1	305	val_305
1	306	val_306
1	307	val_307
1	309	val_309
1	315	val_315
1	316	val_316
1	321	val_321
1	322	val_322
1	323	val_323
1	325	val_325
1	33	val_33
1	332	val_332
1	333	val_333
1	335	val_335
1	336	val_336
1	338	val_338
1	34	val_34
1	344	val_344
1	348	val_348
1	35	val_35
1	353	val_353
1	360	val_360
1	362	val_362
1	366	val_366
1	367	val_367
1	373	val_373
1	379	val_379
1	386	val_386
1	394	val_394
1	399	val_399
1	4	val_4
1	401	val_401
1	402	val_402
1	404	val_404
1	406	val_406
1	409	val_409
1	41	val_41
1	411	val_411
1	413	val_413
1	418	val_418
1	419	val_419
1	421	val_421
1	424	val_424
1	427	val_427
1	429	val_429
1	431	val_431
1	432	val_432
1	435	val_435
1	436	val_436
1	437	val_437
1	438	val_438
1	444	val_444
1	452	val_452
1	453	val_453
1	455	val_455
1	457	val_457
1	459	val_459
1	463	val_463
1	466	val_466
1	47	val_47
1	472	val_472
1	475	val_475
1	478	val_478
1	479	val_479
1	482	val_482
1	483	val_483
1	484	val_484
1	492	val_492
1	494	val_494
1	498	val_498
1	5	val_5
1	54	val_54
1	57	val_57
1	65	val_65
1	69	val_69
1	72	val_72
1	76	val_76
1	78	val_78
1	8	val_8
1	80	val_80
1	90	val_90
1	98	val_98
1	0	val_0
1	113	val_113
1	119	val_119
1	12	val_12
1	120	val_120
1	128	val_128
1	129	val_129
1	136	val_136
1	137	val_137
1	138	val_138
1	145	val_145
1	146	val_146
1	149	val_149
1	153	val_153
1	156	val_156
1	158	val_158
1	160	val_160
1	162	val_162
1	165	val_165
1	166	val_166
1	172	val_172
1	178	val_178
1	179	val_179
1	181	val_181
1	186	val_186
1	194	val_194
1	196	val_196
1	209	val_209
1	218	val_218
1	219	val_219
1	222	val_222
1	223	val_223
1	224	val_224
1	226	val_226
1	228	val_228
1	233	val_233
1	235	val_235
1	242	val_242
1	247	val_247
1	249	val_249
1	255	val_255
1	256	val_256
1	257	val_257
1	260	val_260
1	262	val_262
1	265	val_265
1	266	val_266
1	275	val_275
1	277	val_277
1	28	val_28
1	280	val_280
1	284	val_284
1	289	val_289
1	292	val_292
1	296	val_296
1	308	val_308
1	310	val_310
1	311	val_311
1	317	val_317
1	318	val_318
1	327	val_327
1	331	val_331
1	339	val_339
1	341	val_341
1	342	val_342
1	345	val_345
1	351	val_351
1	356	val_356
1	364	val_364
1	365	val_365
1	368	val_368
1	369	val_369
1	37	val_37
1	374	val_374
1	375	val_375
1	377	val_377
1	378	val_378
1	382	val_382
1	384	val_384
1	389	val_389
1	392	val_392
1	393	val_393
1	395	val_395
1	396	val_396
1	397	val_397
1	400	val_400
1	403	val_403
1	407	val_407
1	414	val_414
1	417	val_417
1	42	val_42
1	43	val_43
1	430	val_430
1	439	val_439
1	44	val_44
1	443	val_443
1	446	val_446
1	448	val_448
1	449	val_449
1	454	val_454
1	458	val_458
1	460	val_460
1	462	val_462
1	467	val_467
1	468	val_468
1	469	val_469
1	470	val_470
1	477	val_477
1	480	val_480
1	481	val_481
1	485	val_485
1	487	val_487
1	489	val_489
1	490	val_490
1	491	val_491
1	493	val_493
1	495	val_495
1	496	val_496
1	497	val_497
1	51	val_51
1	53	val_53
1	58	val_58
1	64	val_64
1	66	val_66
1	67	val_67
1	70	val_70
1	74	val_74
1	77	val_77
1	82	val_82
1	83	val_83
1	84	val_84
1	85	val_85
1	86	val_86
1	87	val_87
1	9	val_9
1	92	val_92
1	95	val_95
1	96	val_96
1	97	val_97
PREHOOK: query: create table jackson_sev_same as select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@jackson_sev_same
POSTHOOK: query: create table jackson_sev_same as select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@jackson_sev_same
POSTHOOK: Lineage: jackson_sev_same.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: jackson_sev_same.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table dim_pho as select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@dim_pho
POSTHOOK: query: create table dim_pho as select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@dim_pho
POSTHOOK: Lineage: dim_pho.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: dim_pho.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table jackson_sev_add as select * from src
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@jackson_sev_add
POSTHOOK: query: create table jackson_sev_add as select * from src
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@jackson_sev_add
POSTHOOK: Lineage: jackson_sev_add.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: jackson_sev_add.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: explain select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97
PREHOOK: type: QUERY
PREHOOK: Input: default@dim_pho
PREHOOK: Input: default@jackson_sev_add
PREHOOK: Input: default@jackson_sev_same
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dim_pho
POSTHOOK: Input: default@jackson_sev_add
POSTHOOK: Input: default@jackson_sev_same
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 2 <- Union 3 (CONTAINS)
        Map 5 <- Union 3 (CONTAINS)
        Reducer 4 <- Map 1 (BROADCAST_EDGE), Union 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: (UDFToDouble(key) = 97.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) = 97.0D) (type: boolean)
                    Statistics: Num rows: 250 Data size: 21750 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 250 Data size: 21750 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 250 Data size: 21750 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 2 
            Map Operator Tree:
                TableScan
                  alias: dim_pho
                  filterExpr: (UDFToDouble(key) = 97.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) = 97.0D) (type: boolean)
                    Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: jackson_sev_add
                  filterExpr: (UDFToDouble(key) = 97.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) = 97.0D) (type: boolean)
                    Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 4 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: string)
                    1 _col0 (type: string)
                  outputColumnNames: _col1, _col2
                  input vertices:
                    0 Map 1
                  Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col1 (type: string), _col2 (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97
PREHOOK: type: QUERY
PREHOOK: Input: default@dim_pho
PREHOOK: Input: default@jackson_sev_add
PREHOOK: Input: default@jackson_sev_same
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dim_pho
POSTHOOK: Input: default@jackson_sev_add
POSTHOOK: Input: default@jackson_sev_same
POSTHOOK: Output: hdfs://### HDFS PATH ###
97	val_97
97	val_97
PREHOOK: query: create table union_subq_union_n0(key int, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@union_subq_union_n0
POSTHOOK: query: create table union_subq_union_n0(key int, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@union_subq_union_n0
PREHOOK: query: explain
insert overwrite table union_subq_union_n0 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union_n0
POSTHOOK: query: explain
insert overwrite table union_subq_union_n0 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union_n0
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Reducer 10 <- Map 9 (SIMPLE_EDGE), Union 7 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Map 5 (SIMPLE_EDGE), Union 7 (CONTAINS)
        Reducer 8 <- Union 2 (CONTAINS), Union 7 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string), value (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: bigint)
            Execution mode: vectorized
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string), value (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: bigint)
            Execution mode: vectorized
        Reducer 10 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    null sort order: zzz
                    sort order: +++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.union_subq_union_n0
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: min(key), max(key), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    null sort order: zzz
                    sort order: +++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 8 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2
        Union 7 
            Vertex: Union 7

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.union_subq_union_n0

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: int, string
          Table: default.union_subq_union_n0

PREHOOK: query: insert overwrite table union_subq_union_n0 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union_n0
POSTHOOK: query: insert overwrite table union_subq_union_n0 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union_n0
POSTHOOK: Lineage: union_subq_union_n0.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: union_subq_union_n0.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from union_subq_union_n0 order by key, value limit 20
PREHOOK: type: QUERY
PREHOOK: Input: default@union_subq_union_n0
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from union_subq_union_n0 order by key, value limit 20
POSTHOOK: type: QUERY
POSTHOOK: Input: default@union_subq_union_n0
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0
2	val_2
4	val_4
5	val_5
8	val_8
9	val_9
10	val_10
11	val_11
12	val_12
15	val_15
17	val_17
18	val_18
19	val_19
20	val_20
24	val_24
26	val_26
27	val_27
28	val_28
30	val_30
33	val_33
PREHOOK: query: create table union_subq_union29(key int, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@union_subq_union29
POSTHOOK: query: create table union_subq_union29(key int, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@union_subq_union29
PREHOOK: query: explain
insert overwrite table union_subq_union29 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value from src 
    UNION DISTINCT
    select key, value from src
  ) subq
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union29
POSTHOOK: query: explain
insert overwrite table union_subq_union29 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value from src 
    UNION DISTINCT
    select key, value from src
  ) subq
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union29
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 5 <- Union 6 (CONTAINS)
        Map 8 <- Union 6 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 7 <- Union 2 (CONTAINS), Union 6 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.union_subq_union29
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: min(key), max(key), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2
        Union 6 
            Vertex: Union 6

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.union_subq_union29

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: int, string
          Table: default.union_subq_union29

PREHOOK: query: insert overwrite table union_subq_union29 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value from src 
    UNION DISTINCT
    select key, value from src
  ) subq
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union29
POSTHOOK: query: insert overwrite table union_subq_union29 
select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value from src 
    UNION DISTINCT
    select key, value from src
  ) subq
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union29
POSTHOOK: Lineage: union_subq_union29.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: union_subq_union29.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from union_subq_union29 order by key, value limit 20
PREHOOK: type: QUERY
PREHOOK: Input: default@union_subq_union29
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from union_subq_union29 order by key, value limit 20
POSTHOOK: type: QUERY
POSTHOOK: Input: default@union_subq_union29
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0
2	val_2
4	val_4
5	val_5
8	val_8
9	val_9
10	val_10
11	val_11
12	val_12
15	val_15
17	val_17
18	val_18
19	val_19
20	val_20
24	val_24
26	val_26
27	val_27
28	val_28
30	val_30
33	val_33
PREHOOK: query: explain
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 10 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 5 (CONTAINS)
        Reducer 12 <- Map 11 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE), Union 5 (CONTAINS)
        Reducer 6 <- Union 5 (SIMPLE_EDGE), Union 7 (CONTAINS)
        Reducer 8 <- Union 7 (SIMPLE_EDGE)
        Reducer 9 <- Map 1 (CUSTOM_SIMPLE_EDGE), Union 7 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Limit
                    Number of rows: 1
                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        TopN Hash Memory Usage: 0.1
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        TopN Hash Memory Usage: 0.1
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Map 11 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                  Limit
                    Number of rows: 1
                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        TopN Hash Memory Usage: 0.1
            Execution mode: vectorized
        Reducer 10 
            Execution mode: vectorized
            Reduce Operator Tree:
              Limit
                Number of rows: 1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 3 (type: int)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: int)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: z
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 12 
            Execution mode: vectorized
            Reduce Operator Tree:
              Limit
                Number of rows: 1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 2 (type: int)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: int)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: z
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Limit
                Number of rows: 1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 1 (type: int)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: int)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: z
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: int)
                  minReductionHashAggr: 0.5
                  mode: hash
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    null sort order: z
                    sort order: +
                    Map-reduce partition columns: _col0 (type: int)
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: int)
                  minReductionHashAggr: 0.5
                  mode: hash
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: _col0 (type: int)
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 8 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: int)
                  outputColumnNames: _col0
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 9 
            Execution mode: vectorized
            Reduce Operator Tree:
              Limit
                Number of rows: 1
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 4 (type: int)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: int)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
        Union 3 
            Vertex: Union 3
        Union 5 
            Vertex: Union 5
        Union 7 
            Vertex: Union 7

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: CREATE TABLE union_out (id int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@union_out
POSTHOOK: query: CREATE TABLE union_out (id int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@union_out
PREHOOK: query: insert overwrite table union_out 
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_out
POSTHOOK: query: insert overwrite table union_out 
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_out
POSTHOOK: Lineage: union_out.id EXPRESSION []
PREHOOK: query: select * from union_out
PREHOOK: type: QUERY
PREHOOK: Input: default@union_out
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from union_out
POSTHOOK: type: QUERY
POSTHOOK: Input: default@union_out
POSTHOOK: Output: hdfs://### HDFS PATH ###
1
2
3
4
PREHOOK: query: create table union_subq_union30(key int, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@union_subq_union30
POSTHOOK: query: create table union_subq_union30(key int, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@union_subq_union30
PREHOOK: query: explain
insert overwrite table union_subq_union30 
select * from (

select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union30
POSTHOOK: query: explain
insert overwrite table union_subq_union30 
select * from (

select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union30
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 13 <- Union 4 (CONTAINS)
        Reducer 10 <- Union 2 (CONTAINS), Union 9 (SIMPLE_EDGE)
        Reducer 12 <- Map 11 (SIMPLE_EDGE), Union 9 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE), Union 4 (CONTAINS)
        Reducer 5 <- Union 4 (SIMPLE_EDGE)
        Reducer 6 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
        Reducer 8 <- Map 7 (SIMPLE_EDGE), Union 9 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 11 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string), value (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: bigint)
            Execution mode: vectorized
        Map 13 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string), value (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col2 (type: bigint)
            Execution mode: vectorized
        Reducer 10 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 12 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    null sort order: zzz
                    sort order: +++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.union_subq_union30
                  Select Operator
                    expressions: _col0 (type: int), _col1 (type: string)
                    outputColumnNames: key, value
                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: min(key), max(key), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 8 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 46500 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    null sort order: zzz
                    sort order: +++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
                    Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2
        Union 4 
            Vertex: Union 4
        Union 9 
            Vertex: Union 9

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.union_subq_union30

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: int, string
          Table: default.union_subq_union30

PREHOOK: query: insert overwrite table union_subq_union30 
select * from (

select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@union_subq_union30
POSTHOOK: query: insert overwrite table union_subq_union30 
select * from (

select * from (
  select key, value from src 
  UNION DISTINCT 
  select key, value from 
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@union_subq_union30
POSTHOOK: Lineage: union_subq_union30.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: union_subq_union30.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select * from union_subq_union30 order by key, value limit 20
PREHOOK: type: QUERY
PREHOOK: Input: default@union_subq_union30
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from union_subq_union30 order by key, value limit 20
POSTHOOK: type: QUERY
POSTHOOK: Input: default@union_subq_union30
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	val_0
2	val_2
4	val_4
5	val_5
8	val_8
9	val_9
10	val_10
11	val_11
12	val_12
15	val_15
17	val_17
18	val_18
19	val_19
20	val_20
24	val_24
26	val_26
27	val_27
28	val_28
30	val_30
33	val_33
PREHOOK: query: drop table t1_n93
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table t1_n93
POSTHOOK: type: DROPTABLE
PREHOOK: query: drop table t2_n58
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table t2_n58
POSTHOOK: type: DROPTABLE
PREHOOK: query: create table t1_n93 as select * from src where key < 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@t1_n93
POSTHOOK: query: create table t1_n93 as select * from src where key < 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1_n93
POSTHOOK: Lineage: t1_n93.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t1_n93.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table t2_n58 as select * from src where key < 10
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@t2_n58
POSTHOOK: query: create table t2_n58 as select * from src where key < 10
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t2_n58
POSTHOOK: Lineage: t2_n58.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t2_n58.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: create table t3_n22(key string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t3_n22
POSTHOOK: query: create table t3_n22(key string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t3_n22
PREHOOK: query: create table t4_n11(value string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t4_n11
POSTHOOK: query: create table t4_n11(value string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t4_n11
PREHOOK: query: explain
from
(select * from t1_n93
 UNION DISTINCT
 select * from t2_n58
) x
insert overwrite table t3_n22
  select key, count(1) group by key
insert overwrite table t4_n11
  select value, count(1) group by value
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: default@t3_n22
PREHOOK: Output: default@t4_n11
POSTHOOK: query: explain
from
(select * from t1_n93
 UNION DISTINCT
 select * from t2_n58
) x
insert overwrite table t3_n22
  select key, count(1) group by key
insert overwrite table t4_n11
  select value, count(1) group by value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: default@t3_n22
POSTHOOK: Output: default@t4_n11
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 7 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 5 <- Reducer 3 (SIMPLE_EDGE)
        Reducer 6 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n93
                  Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string)
                  outputColumnNames: _col0
                  Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: count(1)
                    keys: _col0 (type: string)
                    mode: complete
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      File Output Operator
                        compressed: false
                        Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                        table:
                            input format: org.apache.hadoop.mapred.TextInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                            name: default.t3_n22
                      Select Operator
                        expressions: _col0 (type: string), _col1 (type: int)
                        outputColumnNames: key, cnt
                        Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                          minReductionHashAggr: 0.8
                          mode: hash
                          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                          Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                          Reduce Output Operator
                            null sort order: 
                            sort order: 
                            Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                            value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
                Select Operator
                  expressions: _col1 (type: string)
                  outputColumnNames: _col1
                  Statistics: Num rows: 10 Data size: 1740 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: count(1)
                    keys: _col1 (type: string)
                    minReductionHashAggr: 0.5
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 485 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string)
                      null sort order: z
                      sort order: +
                      Map-reduce partition columns: _col0 (type: string)
                      Statistics: Num rows: 5 Data size: 485 Basic stats: COMPLETE Column stats: COMPLETE
                      value expressions: _col1 (type: bigint)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 485 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        name: default.t4_n11
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: int)
                    outputColumnNames: value, cnt
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: max(length(value)), avg(COALESCE(length(value),0)), count(1), count(value), compute_bit_vector_hll(value), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                      minReductionHashAggr: 0.8
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t3_n22

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, cnt
          Column Types: string, int
          Table: default.t3_n22

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t4_n11

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: value, cnt
          Column Types: string, int
          Table: default.t4_n11

PREHOOK: query: from
(select * from t1_n93
 UNION DISTINCT
 select * from t2_n58
) x
insert overwrite table t3_n22
  select key, count(1) group by key
insert overwrite table t4_n11
  select value, count(1) group by value
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: default@t3_n22
PREHOOK: Output: default@t4_n11
POSTHOOK: query: from
(select * from t1_n93
 UNION DISTINCT
 select * from t2_n58
) x
insert overwrite table t3_n22
  select key, count(1) group by key
insert overwrite table t4_n11
  select value, count(1) group by value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: default@t3_n22
POSTHOOK: Output: default@t4_n11
POSTHOOK: Lineage: t3_n22.cnt EXPRESSION [(t1_n93)t1_n93.null, (t2_n58)t2_n58.null, ]
POSTHOOK: Lineage: t3_n22.key EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:key, type:string, comment:null), (t2_n58)t2_n58.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: t4_n11.cnt EXPRESSION [(t1_n93)t1_n93.null, (t2_n58)t2_n58.null, ]
POSTHOOK: Lineage: t4_n11.value EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:value, type:string, comment:null), (t2_n58)t2_n58.FieldSchema(name:value, type:string, comment:null), ]
PREHOOK: query: select * from t3_n22
PREHOOK: type: QUERY
PREHOOK: Input: default@t3_n22
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t3_n22
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t3_n22
POSTHOOK: Output: hdfs://### HDFS PATH ###
0	1
2	1
4	1
5	1
8	1
9	1
PREHOOK: query: select * from t4_n11
PREHOOK: type: QUERY
PREHOOK: Input: default@t4_n11
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t4_n11
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t4_n11
POSTHOOK: Output: hdfs://### HDFS PATH ###
val_0	1
val_5	1
val_8	1
val_9	1
val_2	1
val_4	1
PREHOOK: query: create table t5_n4(c1 string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t5_n4
POSTHOOK: query: create table t5_n4(c1 string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t5_n4
PREHOOK: query: create table t6_n3(c1 string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t6_n3
POSTHOOK: query: create table t6_n3(c1 string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t6_n3
PREHOOK: query: explain
from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2_n58 group by key
) x
insert overwrite table t5_n4
  select c1, sum(cnt) group by c1
insert overwrite table t6_n3
  select c1, sum(cnt) group by c1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: default@t5_n4
PREHOOK: Output: default@t6_n3
POSTHOOK: query: explain
from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2_n58 group by key
) x
insert overwrite table t5_n4
  select c1, sum(cnt) group by c1
insert overwrite table t6_n3
  select c1, sum(cnt) group by c1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: default@t5_n4
POSTHOOK: Output: default@t6_n3
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
        Reducer 6 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
        Reducer 7 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
        Reducer 9 <- Map 8 (SIMPLE_EDGE), Union 3 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n93
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  null sort order: z
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col1 (type: bigint)
        Reducer 5 
            Execution mode: llap
            Reduce Operator Tree:
              Forward
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: sum(VALUE._col0)
                  keys: KEY._col0 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t5_n4
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: int)
                      outputColumnNames: c1, cnt
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(c1)), avg(COALESCE(length(c1),0)), count(1), count(c1), compute_bit_vector_hll(c1), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                        minReductionHashAggr: 0.8
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
                Group By Operator
                  aggregations: sum(VALUE._col0)
                  keys: KEY._col0 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t6_n3
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: int)
                      outputColumnNames: c1, cnt
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(c1)), avg(COALESCE(length(c1),0)), count(1), count(c1), compute_bit_vector_hll(c1), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                        minReductionHashAggr: 0.8
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 9 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
        Union 3 
            Vertex: Union 3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t5_n4

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: c1, cnt
          Column Types: string, int
          Table: default.t5_n4

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t6_n3

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: c1, cnt
          Column Types: string, int
          Table: default.t6_n3

PREHOOK: query: from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2_n58 group by key
) x
insert overwrite table t5_n4
  select c1, sum(cnt) group by c1
insert overwrite table t6_n3
  select c1, sum(cnt) group by c1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: default@t5_n4
PREHOOK: Output: default@t6_n3
POSTHOOK: query: from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2_n58 group by key
) x
insert overwrite table t5_n4
  select c1, sum(cnt) group by c1
insert overwrite table t6_n3
  select c1, sum(cnt) group by c1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: default@t5_n4
POSTHOOK: Output: default@t6_n3
POSTHOOK: Lineage: t5_n4.c1 EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:key, type:string, comment:null), (t2_n58)t2_n58.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: t5_n4.cnt EXPRESSION [(t1_n93)t1_n93.null, (t2_n58)t2_n58.null, ]
POSTHOOK: Lineage: t6_n3.c1 EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:key, type:string, comment:null), (t2_n58)t2_n58.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: t6_n3.cnt EXPRESSION [(t1_n93)t1_n93.null, (t2_n58)t2_n58.null, ]
PREHOOK: query: select * from t5_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@t5_n4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t5_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t5_n4
POSTHOOK: Output: hdfs://### HDFS PATH ###
4	1
0	3
2	1
5	3
8	1
9	1
PREHOOK: query: select * from t6_n3
PREHOOK: type: QUERY
PREHOOK: Input: default@t6_n3
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t6_n3
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t6_n3
POSTHOOK: Output: hdfs://### HDFS PATH ###
4	1
0	3
2	1
5	3
8	1
9	1
PREHOOK: query: create table t9_n1 as select key, count(1) as cnt from src where key < 10 group by key
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@src
PREHOOK: Output: database:default
PREHOOK: Output: default@t9_n1
POSTHOOK: query: create table t9_n1 as select key, count(1) as cnt from src where key < 10 group by key
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@src
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t9_n1
POSTHOOK: Lineage: t9_n1.cnt EXPRESSION [(src)src.null, ]
POSTHOOK: Lineage: t9_n1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: create table t7_n4(c1 string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t7_n4
POSTHOOK: query: create table t7_n4(c1 string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t7_n4
PREHOOK: query: create table t8_n2(c1 string, cnt int)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t8_n2
POSTHOOK: query: create table t8_n2(c1 string, cnt int)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t8_n2
PREHOOK: query: explain
from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, cnt from t9_n1
) x
insert overwrite table t7_n4
  select c1, count(1) group by c1
insert overwrite table t8_n2
  select c1, count(1) group by c1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t9_n1
PREHOOK: Output: default@t7_n4
PREHOOK: Output: default@t8_n2
POSTHOOK: query: explain
from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, cnt from t9_n1
) x
insert overwrite table t7_n4
  select c1, count(1) group by c1
insert overwrite table t8_n2
  select c1, count(1) group by c1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t9_n1
POSTHOOK: Output: default@t7_n4
POSTHOOK: Output: default@t8_n2
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Map 8 <- Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
        Reducer 6 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
        Reducer 7 <- Reducer 5 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n93
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: t9_n1
                  Statistics: Num rows: 6 Data size: 558 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), cnt (type: bigint)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 6 Data size: 558 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: bigint)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: bigint)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                        Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: bigint)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: bigint)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: bigint)
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: bigint)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string)
                  outputColumnNames: _col0
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string)
                    null sort order: z
                    sort order: +
                    Map-reduce partition columns: _col0 (type: string)
                    Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 5 
            Execution mode: llap
            Reduce Operator Tree:
              Forward
                Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  aggregations: count(1)
                  keys: KEY._col0 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t7_n4
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: int)
                      outputColumnNames: c1, cnt
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(c1)), avg(COALESCE(length(c1),0)), count(1), count(c1), compute_bit_vector_hll(c1), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                        minReductionHashAggr: 0.8
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
                Group By Operator
                  aggregations: count(1)
                  keys: KEY._col0 (type: string)
                  mode: complete
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 5 Data size: 465 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.t8_n2
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: int)
                      outputColumnNames: c1, cnt
                      Statistics: Num rows: 5 Data size: 445 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        aggregations: max(length(c1)), avg(COALESCE(length(c1),0)), count(1), count(c1), compute_bit_vector_hll(c1), min(cnt), max(cnt), count(cnt), compute_bit_vector_hll(cnt)
                        minReductionHashAggr: 0.8
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          null sort order: 
                          sort order: 
                          Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: int), _col7 (type: bigint), _col8 (type: binary)
        Reducer 6 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), min(VALUE._col5), max(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'LONG' (type: string), UDFToLong(_col5) (type: bigint), UDFToLong(_col6) (type: bigint), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t7_n4

  Stage: Stage-4
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: c1, cnt
          Column Types: string, int
          Table: default.t7_n4

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t8_n2

  Stage: Stage-5
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: c1, cnt
          Column Types: string, int
          Table: default.t8_n2

PREHOOK: query: from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, cnt from t9_n1
) x
insert overwrite table t7_n4
  select c1, count(1) group by c1
insert overwrite table t8_n2
  select c1, count(1) group by c1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t9_n1
PREHOOK: Output: default@t7_n4
PREHOOK: Output: default@t8_n2
POSTHOOK: query: from
(
 select key as c1, count(1) as cnt from t1_n93 group by key
   UNION DISTINCT
 select key as c1, cnt from t9_n1
) x
insert overwrite table t7_n4
  select c1, count(1) group by c1
insert overwrite table t8_n2
  select c1, count(1) group by c1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t9_n1
POSTHOOK: Output: default@t7_n4
POSTHOOK: Output: default@t8_n2
POSTHOOK: Lineage: t7_n4.c1 EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:key, type:string, comment:null), (t9_n1)t9_n1.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: t7_n4.cnt EXPRESSION [(t1_n93)t1_n93.null, (t9_n1)t9_n1.null, ]
POSTHOOK: Lineage: t8_n2.c1 EXPRESSION [(t1_n93)t1_n93.FieldSchema(name:key, type:string, comment:null), (t9_n1)t9_n1.FieldSchema(name:key, type:string, comment:null), ]
POSTHOOK: Lineage: t8_n2.cnt EXPRESSION [(t1_n93)t1_n93.null, (t9_n1)t9_n1.null, ]
PREHOOK: query: select * from t7_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@t7_n4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t7_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t7_n4
POSTHOOK: Output: hdfs://### HDFS PATH ###
4	1
0	1
2	1
5	1
8	1
9	1
PREHOOK: query: select * from t8_n2
PREHOOK: type: QUERY
PREHOOK: Input: default@t8_n2
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: select * from t8_n2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t8_n2
POSTHOOK: Output: hdfs://### HDFS PATH ###
4	1
0	1
2	1
5	1
8	1
9	1
PREHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t1_n93
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t1_n93
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 4 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t1_n93
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(key) (type: double)
                    outputColumnNames: _col0
                    Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double)
                      minReductionHashAggr: 0.7
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: double)
                        Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(UDFToLong(key)) (type: double)
                    outputColumnNames: _col0
                    Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double)
                      minReductionHashAggr: 0.7
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: double)
                        Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: double)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t1_n93
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t1_n93
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
4.0
0.0
2.0
5.0
8.0
9.0
PREHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Map 5 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), UDFToLong(key) (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col1
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 16 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: UDFToDouble(_col1) (type: double)
                          outputColumnNames: _col0
                          Statistics: Num rows: 16 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: double)
                            minReductionHashAggr: 0.7692308
                            mode: hash
                            outputColumnNames: _col0
                            Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              key expressions: _col0 (type: double)
                              null sort order: z
                              sort order: +
                              Map-reduce partition columns: _col0 (type: double)
                              Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(key) (type: double)
                    outputColumnNames: _col0
                    Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double)
                      minReductionHashAggr: 0.7692308
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: double)
                        Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: double)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
4.0
0.0
2.0
5.0
8.0
9.0
PREHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 4 <- Map 5 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(key) (type: double)
                    outputColumnNames: _col0
                    Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double)
                      minReductionHashAggr: 0.7692308
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: double)
                        Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), UDFToLong(key) (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col1
                        input vertices:
                          1 Map 5
                        Statistics: Num rows: 16 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: UDFToDouble(_col1) (type: double)
                          outputColumnNames: _col0
                          Statistics: Num rows: 16 Data size: 128 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: double)
                            minReductionHashAggr: 0.7692308
                            mode: hash
                            outputColumnNames: _col0
                            Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              key expressions: _col0 (type: double)
                              null sort order: z
                              sort order: +
                              Map-reduce partition columns: _col0 (type: double)
                              Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: double)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 6 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
4.0
0.0
2.0
5.0
8.0
9.0
PREHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Map 5 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), UDFToLong(key) (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col1, _col2
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 16 Data size: 1488 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: UDFToDouble(_col1) (type: double), _col2 (type: string)
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 16 Data size: 1488 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: double), _col1 (type: string)
                            minReductionHashAggr: 0.46153843
                            mode: hash
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              key expressions: _col0 (type: double), _col1 (type: string)
                              null sort order: zz
                              sort order: ++
                              Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                              Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(key) (type: double), key (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double), _col1 (type: string)
                      minReductionHashAggr: 0.46153843
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                        Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: double), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 13 Data size: 1209 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM 
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
0.0	0
2.0	2
5.0	5
4.0	4
8.0	8
9.0	9
PREHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: EXPLAIN
SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 5 <- Map 4 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: t2_n58
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: UDFToDouble(key) (type: double), key (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: double), _col1 (type: string)
                      minReductionHashAggr: 0.46153843
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: double), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                        Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: a
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), UDFToLong(key) (type: bigint)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 930 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: b
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 850 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), CAST( key AS varchar(20)) (type: varchar(20))
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1890 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col1, _col3
                        input vertices:
                          0 Map 4
                        Statistics: Num rows: 16 Data size: 1792 Basic stats: COMPLETE Column stats: COMPLETE
                        Select Operator
                          expressions: UDFToDouble(_col1) (type: double), CAST( _col3 AS STRING) (type: string)
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 16 Data size: 3072 Basic stats: COMPLETE Column stats: COMPLETE
                          Group By Operator
                            keys: _col0 (type: double), _col1 (type: string)
                            minReductionHashAggr: 0.46153843
                            mode: hash
                            outputColumnNames: _col0, _col1
                            Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              key expressions: _col0 (type: double), _col1 (type: string)
                              null sort order: zz
                              sort order: ++
                              Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                              Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: double), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 13 Data size: 2496 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
PREHOOK: type: QUERY
PREHOOK: Input: default@t1_n93
PREHOOK: Input: default@t2_n58
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM 
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2_n58
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1_n93 a JOIN t2_n58 b ON a.key = b.key) a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1_n93
POSTHOOK: Input: default@t2_n58
POSTHOOK: Output: hdfs://### HDFS PATH ###
0.0	0
2.0	2
5.0	5
4.0	4
8.0	8
9.0	9
PREHOOK: query: drop table if exists test_src
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table if exists test_src
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE test_src (key STRING, value STRING)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@test_src
POSTHOOK: query: CREATE TABLE test_src (key STRING, value STRING)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@test_src
PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, value FROM src 
	WHERE key = 0
UNION DISTINCT
 	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
)a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_src
POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, value FROM src 
	WHERE key = 0
UNION DISTINCT
 	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
)a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_src
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)
        Reducer 6 <- Map 5 (SIMPLE_EDGE), Union 2 (CONTAINS)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  filterExpr: (UDFToDouble(key) = 0.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) = 0.0D) (type: boolean)
                    Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.test_src
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: key, value
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                    Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                      value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 336 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), CAST( _col1 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.test_src

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.test_src

PREHOOK: query: INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, value FROM src 
	WHERE key = 0
UNION DISTINCT
 	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
)a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_src
POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, value FROM src 
	WHERE key = 0
UNION DISTINCT
 	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
)a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_src
POSTHOOK: Lineage: test_src.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_src.value EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), (src)src.null, ]
PREHOOK: query: SELECT COUNT(*) FROM test_src
PREHOOK: type: QUERY
PREHOOK: Input: default@test_src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT COUNT(*) FROM test_src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_src
POSTHOOK: Output: hdfs://### HDFS PATH ###
310
PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src 
	WHERE key = 0
)a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_src
POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src 
	WHERE key = 0
)a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_src
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 6 <- Union 3 (CONTAINS)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string)
                    outputColumnNames: key
                    Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      aggregations: count()
                      keys: key (type: string)
                      minReductionHashAggr: 0.5
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: bigint)
            Execution mode: vectorized
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: src
                  filterExpr: (UDFToDouble(key) = 0.0D) (type: boolean)
                  Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (UDFToDouble(key) = 0.0D) (type: boolean)
                    Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 250 Data size: 44500 Basic stats: COMPLETE Column stats: COMPLETE
                      Group By Operator
                        keys: _col0 (type: string), _col1 (type: string)
                        minReductionHashAggr: 0.4
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col0 (type: string), _col1 (type: string)
                          null sort order: zz
                          sort order: ++
                          Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                          Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 23750 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: string), CAST( _col1 AS STRING) (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    keys: _col0 (type: string), _col1 (type: string)
                    minReductionHashAggr: 0.4
                    mode: hash
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: zz
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                      Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.test_src
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string)
                  outputColumnNames: key, value
                  Statistics: Num rows: 250 Data size: 67750 Basic stats: COMPLETE Column stats: COMPLETE
                  Group By Operator
                    aggregations: max(length(key)), avg(COALESCE(length(key),0)), count(1), count(key), compute_bit_vector_hll(key), max(length(value)), avg(COALESCE(length(value),0)), count(value), compute_bit_vector_hll(value)
                    minReductionHashAggr: 0.99
                    mode: hash
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                    Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      null sort order: 
                      sort order: 
                      Statistics: Num rows: 1 Data size: 472 Basic stats: COMPLETE Column stats: COMPLETE
                      value expressions: _col0 (type: int), _col1 (type: struct<count:bigint,sum:double,input:int>), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
        Reducer 5 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0), avg(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 336 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: 'STRING' (type: string), UDFToLong(COALESCE(_col0,0)) (type: bigint), COALESCE(_col1,0) (type: double), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 532 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 3 
            Vertex: Union 3

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.test_src

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: key, value
          Column Types: string, string
          Table: default.test_src

PREHOOK: query: INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src 
	WHERE key = 0
)a
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_src
POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
SELECT key, value FROM (
	SELECT key, cast(COUNT(*) as string) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src 
	WHERE key = 0
)a
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_src
POSTHOOK: Lineage: test_src.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_src.value EXPRESSION [(src)src.null, (src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT COUNT(*) FROM test_src
PREHOOK: type: QUERY
PREHOOK: Input: default@test_src
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT COUNT(*) FROM test_src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_src
POSTHOOK: Output: hdfs://### HDFS PATH ###
310
PREHOOK: query: -- union34.q

create table src10_1 (key string, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@src10_1
POSTHOOK: query: -- union34.q

create table src10_1 (key string, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src10_1
PREHOOK: query: create table src10_2 (key string, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@src10_2
POSTHOOK: query: create table src10_2 (key string, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src10_2
PREHOOK: query: create table src10_3 (key string, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@src10_3
POSTHOOK: query: create table src10_3 (key string, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src10_3
PREHOOK: query: create table src10_4 (key string, value string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@src10_4
POSTHOOK: query: create table src10_4 (key string, value string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@src10_4
PREHOOK: query: from (select * from src tablesample (10 rows)) a
insert overwrite table src10_1 select *
insert overwrite table src10_2 select *
insert overwrite table src10_3 select *
insert overwrite table src10_4 select *
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@src10_1
PREHOOK: Output: default@src10_2
PREHOOK: Output: default@src10_3
PREHOOK: Output: default@src10_4
POSTHOOK: query: from (select * from src tablesample (10 rows)) a
insert overwrite table src10_1 select *
insert overwrite table src10_2 select *
insert overwrite table src10_3 select *
insert overwrite table src10_4 select *
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@src10_1
POSTHOOK: Output: default@src10_2
POSTHOOK: Output: default@src10_3
POSTHOOK: Output: default@src10_4
POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: analyze table src10_1 compute statistics
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_1
PREHOOK: Output: default@src10_1
POSTHOOK: query: analyze table src10_1 compute statistics
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_1
POSTHOOK: Output: default@src10_1
PREHOOK: query: analyze table src10_2 compute statistics
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_2
PREHOOK: Output: default@src10_2
POSTHOOK: query: analyze table src10_2 compute statistics
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_2
POSTHOOK: Output: default@src10_2
PREHOOK: query: analyze table src10_3 compute statistics
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_3
PREHOOK: Output: default@src10_3
POSTHOOK: query: analyze table src10_3 compute statistics
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_3
POSTHOOK: Output: default@src10_3
PREHOOK: query: analyze table src10_4 compute statistics
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_4
PREHOOK: Output: default@src10_4
POSTHOOK: query: analyze table src10_4 compute statistics
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_4
POSTHOOK: Output: default@src10_4
PREHOOK: query: explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_1
PREHOOK: Input: default@src10_2
PREHOOK: Input: default@src10_3
PREHOOK: Input: default@src10_4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_1
POSTHOOK: Input: default@src10_2
POSTHOOK: Input: default@src10_3
POSTHOOK: Input: default@src10_4
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Union 2 (CONTAINS)
        Map 5 <- Union 6 (CONTAINS)
        Map 8 <- Union 6 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
        Reducer 7 <- Union 2 (CONTAINS), Union 6 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src10_1
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: string)
                          1 _col0 (type: string)
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: string), _col1 (type: string)
                          minReductionHashAggr: 0.4
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                          Reduce Output Operator
                            key expressions: _col0 (type: string), _col1 (type: string)
                            null sort order: zz
                            sort order: ++
                            Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                            Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src10_2
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src10_3
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: src10_4
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
        Union 2 
            Vertex: Union 2
        Union 6 
            Vertex: Union 6

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_1
PREHOOK: Input: default@src10_2
PREHOOK: Input: default@src10_3
PREHOOK: Input: default@src10_4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_1
POSTHOOK: Input: default@src10_2
POSTHOOK: Input: default@src10_3
POSTHOOK: Input: default@src10_4
POSTHOOK: Output: hdfs://### HDFS PATH ###
238	val_238
27	val_27
278	val_278
409	val_409
484	val_484
98	val_98
165	val_165
255	val_255
311	val_311
86	val_86
PREHOOK: query: explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_1
PREHOOK: Input: default@src10_2
PREHOOK: Input: default@src10_3
PREHOOK: Input: default@src10_4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_1
POSTHOOK: Input: default@src10_2
POSTHOOK: Input: default@src10_3
POSTHOOK: Input: default@src10_4
POSTHOOK: Output: hdfs://### HDFS PATH ###
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 6 <- Union 7 (CONTAINS)
        Map 9 <- Union 7 (CONTAINS)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE), Union 3 (CONTAINS)
        Reducer 4 <- Union 3 (SIMPLE_EDGE)
        Reducer 8 <- Union 3 (CONTAINS), Union 7 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src10_1
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string), value (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: src10_2
                  filterExpr: key is not null (type: boolean)
                  Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 10 Data size: 870 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: src10_3
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: src10_4
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: key (type: string), value (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                    Group By Operator
                      keys: _col0 (type: string), _col1 (type: string)
                      minReductionHashAggr: 0.4
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: zz
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                        Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col0 (type: string)
                  1 _col0 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
        Reducer 4 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 8 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                Group By Operator
                  keys: _col0 (type: string), _col1 (type: string)
                  minReductionHashAggr: 0.4
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    null sort order: zz
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 10 Data size: 1780 Basic stats: COMPLETE Column stats: COMPLETE
        Union 3 
            Vertex: Union 3
        Union 7 
            Vertex: Union 7

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
PREHOOK: type: QUERY
PREHOOK: Input: default@src10_1
PREHOOK: Input: default@src10_2
PREHOOK: Input: default@src10_3
PREHOOK: Input: default@src10_4
PREHOOK: Output: hdfs://### HDFS PATH ###
POSTHOOK: query: SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src10_1
POSTHOOK: Input: default@src10_2
POSTHOOK: Input: default@src10_3
POSTHOOK: Input: default@src10_4
POSTHOOK: Output: hdfs://### HDFS PATH ###
238	val_238
27	val_27
278	val_278
409	val_409
484	val_484
98	val_98
165	val_165
255	val_255
311	val_311
86	val_86
