PREHOOK: query: DROP TABLE part_staging
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE part_staging
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE part_orc
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE part_orc
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE part_staging( 
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_staging
POSTHOOK: query: CREATE TABLE part_staging( 
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_staging
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/part_tiny.txt' overwrite into table part_staging
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@part_staging
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/part_tiny.txt' overwrite into table part_staging
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@part_staging
PREHOOK: query: CREATE TABLE part_orc( 
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
) STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_orc
POSTHOOK: query: CREATE TABLE part_orc( 
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
) STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_orc
PREHOOK: query: DESCRIBE EXTENDED part_orc
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@part_orc
POSTHOOK: query: DESCRIBE EXTENDED part_orc
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@part_orc
p_partkey           	int                 	                    
p_name              	string              	                    
p_mfgr              	string              	                    
p_brand             	string              	                    
p_type              	string              	                    
p_size              	int                 	                    
p_container         	string              	                    
p_retailprice       	double              	                    
p_comment           	string              	                    
	 	 
#### A masked pattern was here ####
PREHOOK: query: insert into table part_orc select * from part_staging
PREHOOK: type: QUERY
PREHOOK: Input: default@part_staging
PREHOOK: Output: default@part_orc
POSTHOOK: query: insert into table part_orc select * from part_staging
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_staging
POSTHOOK: Output: default@part_orc
POSTHOOK: Lineage: part_orc.p_brand SIMPLE [(part_staging)part_staging.FieldSchema(name:p_brand, type:string, comment:null), ]
POSTHOOK: Lineage: part_orc.p_comment SIMPLE [(part_staging)part_staging.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_orc.p_container SIMPLE [(part_staging)part_staging.FieldSchema(name:p_container, type:string, comment:null), ]
POSTHOOK: Lineage: part_orc.p_mfgr SIMPLE [(part_staging)part_staging.FieldSchema(name:p_mfgr, type:string, comment:null), ]
POSTHOOK: Lineage: part_orc.p_name SIMPLE [(part_staging)part_staging.FieldSchema(name:p_name, type:string, comment:null), ]
POSTHOOK: Lineage: part_orc.p_partkey SIMPLE [(part_staging)part_staging.FieldSchema(name:p_partkey, type:int, comment:null), ]
POSTHOOK: Lineage: part_orc.p_retailprice SIMPLE [(part_staging)part_staging.FieldSchema(name:p_retailprice, type:double, comment:null), ]
POSTHOOK: Lineage: part_orc.p_size SIMPLE [(part_staging)part_staging.FieldSchema(name:p_size, type:int, comment:null), ]
POSTHOOK: Lineage: part_orc.p_type SIMPLE [(part_staging)part_staging.FieldSchema(name:p_type, type:string, comment:null), ]
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  )
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  )
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  )
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  )
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.650000000001
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.070000000001
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.730000000001
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.360000000001
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.620000000001
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.3500000000004
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: p1
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: p_partkey is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: p_partkey (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: p_partkey (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      value expressions: p_name (type: string), p_mfgr (type: string), p_size (type: int)
                      auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [p1]
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: p2
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: p_partkey is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: p_partkey (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: p_partkey (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 1
                      auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [p2]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 p_partkey (type: int)
                  1 p_partkey (type: int)
                outputColumnNames: _col1, _col2, _col5
                Position of Big Table: 0
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col2 (type: string), _col1 (type: string)
                  null sort order: aa
                  sort order: ++
                  Map-reduce partition columns: _col2 (type: string)
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col5 (type: int)
                  auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: j
                        output shape: _col1: string, _col2: string, _col5: int
                        type: SUBQUERY
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: lag_window_0
                              arguments: _col5, 1, _col5
                              name: lag
                              window function: GenericUDAFLagEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), (_col5 - lag_window_0) (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3
                            columns.types string:string:int:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	0
Manufacturer#1	almond antique burnished rose metallic	2	0
Manufacturer#1	almond antique burnished rose metallic	2	0
Manufacturer#1	almond antique burnished rose metallic	2	0
Manufacturer#1	almond antique chartreuse lavender yellow	34	32
Manufacturer#1	almond antique salmon chartreuse burlywood	6	-28
Manufacturer#1	almond aquamarine burnished black steel	28	22
Manufacturer#1	almond aquamarine pink moccasin thistle	42	14
Manufacturer#2	almond antique violet chocolate turquoise	14	0
Manufacturer#2	almond antique violet turquoise frosted	40	26
Manufacturer#2	almond aquamarine midnight light salmon	2	-38
Manufacturer#2	almond aquamarine rose maroon antique	25	23
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	-7
Manufacturer#3	almond antique chartreuse khaki white	17	0
Manufacturer#3	almond antique forest lavender goldenrod	14	-3
Manufacturer#3	almond antique metallic orange dim	19	5
Manufacturer#3	almond antique misty red olive	1	-18
Manufacturer#3	almond antique olive coral navajo	45	44
Manufacturer#4	almond antique gainsboro frosted violet	10	0
Manufacturer#4	almond antique violet mint lemon	39	29
Manufacturer#4	almond aquamarine floral ivory bisque	27	-12
Manufacturer#4	almond aquamarine yellow dodger mint	7	-20
Manufacturer#4	almond azure aquamarine papaya violet	12	5
Manufacturer#5	almond antique blue firebrick mint	31	0
Manufacturer#5	almond antique medium spring khaki	6	-25
Manufacturer#5	almond antique sky peru orange	2	-4
Manufacturer#5	almond aquamarine dodger light gainsboro	46	44
Manufacturer#5	almond azure blanched chiffon midnight	23	-23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2
                            columns.types string:string:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2
Manufacturer#1	almond antique burnished rose metallic	2
Manufacturer#1	almond antique chartreuse lavender yellow	34
Manufacturer#1	almond antique salmon chartreuse burlywood	6
Manufacturer#1	almond aquamarine burnished black steel	28
Manufacturer#1	almond aquamarine pink moccasin thistle	42
Manufacturer#2	almond antique violet chocolate turquoise	14
Manufacturer#2	almond antique violet turquoise frosted	40
Manufacturer#2	almond aquamarine midnight light salmon	2
Manufacturer#2	almond aquamarine rose maroon antique	25
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18
Manufacturer#3	almond antique chartreuse khaki white	17
Manufacturer#3	almond antique forest lavender goldenrod	14
Manufacturer#3	almond antique metallic orange dim	19
Manufacturer#3	almond antique misty red olive	1
Manufacturer#3	almond antique olive coral navajo	45
Manufacturer#4	almond antique gainsboro frosted violet	10
Manufacturer#4	almond antique violet mint lemon	39
Manufacturer#4	almond aquamarine floral ivory bisque	27
Manufacturer#4	almond aquamarine yellow dodger mint	7
Manufacturer#4	almond azure aquamarine papaya violet	12
Manufacturer#5	almond antique blue firebrick mint	31
Manufacturer#5	almond antique medium spring khaki	6
Manufacturer#5	almond antique sky peru orange	2
Manufacturer#5	almond aquamarine dodger light gainsboro	46
Manufacturer#5	almond azure blanched chiffon midnight	23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  ) abc
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  ) abc
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: abc
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  ) abc
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc 
  partition by p_mfgr
  order by p_name
  ) abc
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.65
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.07
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.73
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.36
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.62
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.35
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          )
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          )
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: lag_window_2
                              arguments: _col5, 1, _col5
                              name: lag
                              window function: GenericUDAFLagEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), (_col5 - lag_window_2) (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                            columns.types string:string:int:int:int:int:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          )
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          )
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2	0
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2	0
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	34	32
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	6	-28
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	28	22
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	42	14
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	14	0
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	40	26
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	2	-38
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	25	23
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	18	-7
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	17	0
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	14	-3
Manufacturer#3	almond antique metallic orange dim	19	3	3	19	5
Manufacturer#3	almond antique misty red olive	1	4	4	1	-18
Manufacturer#3	almond antique olive coral navajo	45	5	5	45	44
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	10	0
Manufacturer#4	almond antique violet mint lemon	39	2	2	39	29
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	27	-12
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	7	-20
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	12	5
Manufacturer#5	almond antique blue firebrick mint	31	1	1	31	0
Manufacturer#5	almond antique medium spring khaki	6	2	2	6	-25
Manufacturer#5	almond antique sky peru orange	2	3	3	2	-4
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	46	44
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	23	-23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          ) 
group by p_mfgr, p_name, p_size
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          ) 
group by p_mfgr, p_name, p_size
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                    outputColumnNames: _col2, _col1, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      keys: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int)
                        null sort order: aaa
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                        tag: -1
                        auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col0: string, _col1: string, _col2: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col0
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: lag_window_2
                              arguments: _col2, 1, _col2
                              name: lag
                              window function: GenericUDAFLagEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                  Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col2 (type: int), (_col2 - lag_window_2) (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                            columns.types string:string:int:int:int:int:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          ) 
group by p_mfgr, p_name, p_size
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz 
from noop(on part_orc 
          partition by p_mfgr 
          order by p_name 
          ) 
group by p_mfgr, p_name, p_size
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2	0
Manufacturer#1	almond antique chartreuse lavender yellow	34	2	2	34	32
Manufacturer#1	almond antique salmon chartreuse burlywood	6	3	3	6	-28
Manufacturer#1	almond aquamarine burnished black steel	28	4	4	28	22
Manufacturer#1	almond aquamarine pink moccasin thistle	42	5	5	42	14
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	14	0
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	40	26
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	2	-38
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	25	23
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	18	-7
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	17	0
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	14	-3
Manufacturer#3	almond antique metallic orange dim	19	3	3	19	5
Manufacturer#3	almond antique misty red olive	1	4	4	1	-18
Manufacturer#3	almond antique olive coral navajo	45	5	5	45	44
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	10	0
Manufacturer#4	almond antique violet mint lemon	39	2	2	39	29
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	27	-12
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	7	-20
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	12	5
Manufacturer#5	almond antique blue firebrick mint	31	1	1	31	0
Manufacturer#5	almond antique medium spring khaki	6	2	2	6	-25
Manufacturer#5	almond antique sky peru orange	2	3	3	2	-4
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	46	44
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	23	-23
PREHOOK: query: explain vectorization extended
select abc.* 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select abc.* 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Map 4 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_partkey (type: int), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: p1
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: p_partkey is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: p_partkey (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: p_partkey (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 1
                      auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [p1]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col1 (type: string), VALUE._col2 (type: string), VALUE._col3 (type: int), VALUE._col4 (type: string), VALUE._col5 (type: double), VALUE._col6 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col0: int, _col1: string, _col2: string, _col3: string, _col4: string, _col5: int, _col6: string, _col7: double, _col8: string
                        type: TABLE
                      Partition table definition
                        input alias: abc
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col0: int, _col1: string, _col2: string, _col3: string, _col4: string, _col5: int, _col6: string, _col7: double, _col8: string
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    isSamplingPred: false
                    predicate: _col0 is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: string), _col5 (type: int), _col6 (type: string), _col7 (type: double), _col8 (type: string)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col0 (type: int)
                  1 p_partkey (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Position of Big Table: 0
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                        columns.types int:string:string:string:string:int:string:double:string
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select abc.* 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select abc.* 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
105685	almond antique violet chocolate turquoise	Manufacturer#2	Brand#22	MEDIUM ANODIZED COPPER	14	MED CAN	1690.68	ly pending requ
110592	almond antique salmon chartreuse burlywood	Manufacturer#1	Brand#15	PROMO BURNISHED NICKEL	6	JUMBO PKG	1602.59	 to the furiously
112398	almond antique metallic orange dim	Manufacturer#3	Brand#32	MEDIUM BURNISHED BRASS	19	JUMBO JAR	1410.39	ole car
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
132666	almond aquamarine rose maroon antique	Manufacturer#2	Brand#24	SMALL POLISHED NICKEL	25	MED BOX	1698.66	even 
144293	almond antique olive coral navajo	Manufacturer#3	Brand#34	STANDARD POLISHED STEEL	45	JUMBO CAN	1337.29	ag furiously about 
146985	almond aquamarine midnight light salmon	Manufacturer#2	Brand#23	MEDIUM BURNISHED COPPER	2	SM CASE	2031.98	s cajole caref
15103	almond aquamarine dodger light gainsboro	Manufacturer#5	Brand#53	ECONOMY BURNISHED STEEL	46	LG PACK	1018.1	packages hinder carefu
155733	almond antique sky peru orange	Manufacturer#5	Brand#53	SMALL PLATED BRASS	2	WRAP DRUM	1788.73	furiously. bra
17273	almond antique forest lavender goldenrod	Manufacturer#3	Brand#35	PROMO ANODIZED TIN	14	JUMBO CASE	1190.27	along the
17927	almond aquamarine yellow dodger mint	Manufacturer#4	Brand#41	ECONOMY BRUSHED COPPER	7	SM PKG	1844.92	ites. eve
191709	almond antique violet turquoise frosted	Manufacturer#2	Brand#22	ECONOMY POLISHED STEEL	40	MED BOX	1800.7	 haggle
192697	almond antique blue firebrick mint	Manufacturer#5	Brand#52	MEDIUM BURNISHED TIN	31	LG DRUM	1789.69	ickly ir
195606	almond aquamarine sandy cyan gainsboro	Manufacturer#2	Brand#25	STANDARD PLATED TIN	18	SM PKG	1701.6	ic de
33357	almond azure aquamarine papaya violet	Manufacturer#4	Brand#41	STANDARD ANODIZED TIN	12	WRAP CASE	1290.35	reful
40982	almond antique misty red olive	Manufacturer#3	Brand#32	ECONOMY PLATED COPPER	1	LG PKG	1922.98	c foxes can s
42669	almond antique medium spring khaki	Manufacturer#5	Brand#51	STANDARD BURNISHED TIN	6	MED CAN	1611.66	sits haggl
45261	almond aquamarine floral ivory bisque	Manufacturer#4	Brand#42	SMALL PLATED STEEL	27	WRAP CASE	1206.26	careful
48427	almond antique violet mint lemon	Manufacturer#4	Brand#42	PROMO POLISHED STEEL	39	SM CASE	1375.42	hely ironic i
49671	almond antique gainsboro frosted violet	Manufacturer#4	Brand#41	SMALL BRUSHED BRASS	10	SM BOX	1620.67	ccounts run quick
65667	almond aquamarine pink moccasin thistle	Manufacturer#1	Brand#12	LARGE BURNISHED STEEL	42	JUMBO CASE	1632.66	e across the expr
78486	almond azure blanched chiffon midnight	Manufacturer#5	Brand#52	LARGE BRUSHED BRASS	23	MED BAG	1464.48	hely blith
85768	almond antique chartreuse lavender yellow	Manufacturer#1	Brand#12	LARGE BRUSHED STEEL	34	SM BAG	1753.76	refull
86428	almond aquamarine burnished black steel	Manufacturer#1	Brand#12	STANDARD ANODIZED STEEL	28	WRAP BAG	1414.42	arefully 
90681	almond antique chartreuse khaki white	Manufacturer#3	Brand#31	MEDIUM BURNISHED TIN	17	SM CASE	1671.68	are slyly after the sl
PREHOOK: query: explain vectorization extended
select abc.* 
from part_orc p1 join noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select abc.* 
from part_orc p1 join noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)
        Reducer 4 <- Map 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: p1
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: p_partkey is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: p_partkey (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: p_partkey (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [p1]
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_partkey (type: int), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 p_partkey (type: int)
                  1 _col0 (type: int)
                outputColumnNames: _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20
                Position of Big Table: 1
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col12 (type: int), _col13 (type: string), _col14 (type: string), _col15 (type: string), _col16 (type: string), _col17 (type: int), _col18 (type: string), _col19 (type: double), _col20 (type: string)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                          columns.types int:string:string:string:string:int:string:double:string
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col1 (type: string), VALUE._col2 (type: string), VALUE._col3 (type: int), VALUE._col4 (type: string), VALUE._col5 (type: double), VALUE._col6 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col0: int, _col1: string, _col2: string, _col3: string, _col4: string, _col5: int, _col6: string, _col7: double, _col8: string
                        type: TABLE
                      Partition table definition
                        input alias: abc
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col0: int, _col1: string, _col2: string, _col3: string, _col4: string, _col5: int, _col6: string, _col7: double, _col8: string
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    isSamplingPred: false
                    predicate: _col0 is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 1
                      value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: string), _col5 (type: int), _col6 (type: string), _col7 (type: double), _col8 (type: string)
                      auto parallelism: true

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select abc.* 
from part_orc p1 join noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select abc.* 
from part_orc p1 join noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
105685	almond antique violet chocolate turquoise	Manufacturer#2	Brand#22	MEDIUM ANODIZED COPPER	14	MED CAN	1690.68	ly pending requ
110592	almond antique salmon chartreuse burlywood	Manufacturer#1	Brand#15	PROMO BURNISHED NICKEL	6	JUMBO PKG	1602.59	 to the furiously
112398	almond antique metallic orange dim	Manufacturer#3	Brand#32	MEDIUM BURNISHED BRASS	19	JUMBO JAR	1410.39	ole car
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
121152	almond antique burnished rose metallic	Manufacturer#1	Brand#14	PROMO PLATED TIN	2	JUMBO BOX	1173.15	e pinto beans h
132666	almond aquamarine rose maroon antique	Manufacturer#2	Brand#24	SMALL POLISHED NICKEL	25	MED BOX	1698.66	even 
144293	almond antique olive coral navajo	Manufacturer#3	Brand#34	STANDARD POLISHED STEEL	45	JUMBO CAN	1337.29	ag furiously about 
146985	almond aquamarine midnight light salmon	Manufacturer#2	Brand#23	MEDIUM BURNISHED COPPER	2	SM CASE	2031.98	s cajole caref
15103	almond aquamarine dodger light gainsboro	Manufacturer#5	Brand#53	ECONOMY BURNISHED STEEL	46	LG PACK	1018.1	packages hinder carefu
155733	almond antique sky peru orange	Manufacturer#5	Brand#53	SMALL PLATED BRASS	2	WRAP DRUM	1788.73	furiously. bra
17273	almond antique forest lavender goldenrod	Manufacturer#3	Brand#35	PROMO ANODIZED TIN	14	JUMBO CASE	1190.27	along the
17927	almond aquamarine yellow dodger mint	Manufacturer#4	Brand#41	ECONOMY BRUSHED COPPER	7	SM PKG	1844.92	ites. eve
191709	almond antique violet turquoise frosted	Manufacturer#2	Brand#22	ECONOMY POLISHED STEEL	40	MED BOX	1800.7	 haggle
192697	almond antique blue firebrick mint	Manufacturer#5	Brand#52	MEDIUM BURNISHED TIN	31	LG DRUM	1789.69	ickly ir
195606	almond aquamarine sandy cyan gainsboro	Manufacturer#2	Brand#25	STANDARD PLATED TIN	18	SM PKG	1701.6	ic de
33357	almond azure aquamarine papaya violet	Manufacturer#4	Brand#41	STANDARD ANODIZED TIN	12	WRAP CASE	1290.35	reful
40982	almond antique misty red olive	Manufacturer#3	Brand#32	ECONOMY PLATED COPPER	1	LG PKG	1922.98	c foxes can s
42669	almond antique medium spring khaki	Manufacturer#5	Brand#51	STANDARD BURNISHED TIN	6	MED CAN	1611.66	sits haggl
45261	almond aquamarine floral ivory bisque	Manufacturer#4	Brand#42	SMALL PLATED STEEL	27	WRAP CASE	1206.26	careful
48427	almond antique violet mint lemon	Manufacturer#4	Brand#42	PROMO POLISHED STEEL	39	SM CASE	1375.42	hely ironic i
49671	almond antique gainsboro frosted violet	Manufacturer#4	Brand#41	SMALL BRUSHED BRASS	10	SM BOX	1620.67	ccounts run quick
65667	almond aquamarine pink moccasin thistle	Manufacturer#1	Brand#12	LARGE BURNISHED STEEL	42	JUMBO CASE	1632.66	e across the expr
78486	almond azure blanched chiffon midnight	Manufacturer#5	Brand#52	LARGE BRUSHED BRASS	23	MED BAG	1464.48	hely blith
85768	almond antique chartreuse lavender yellow	Manufacturer#1	Brand#12	LARGE BRUSHED STEEL	34	SM BAG	1753.76	refull
86428	almond aquamarine burnished black steel	Manufacturer#1	Brand#12	STANDARD ANODIZED STEEL	28	WRAP BAG	1414.42	arefully 
90681	almond antique chartreuse khaki white	Manufacturer#3	Brand#31	MEDIUM BURNISHED TIN	17	SM CASE	1671.68	are slyly after the sl
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: part_orc
                          output shape: p_name: string, p_mfgr: string, p_size: int
                          type: TABLE
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: p_name ASC NULLS FIRST, p_size DESC NULLS LAST
                          output shape: p_name: string, p_mfgr: string, p_size: int
                          partition by: p_mfgr
                          raw input shape:
                          transforms raw input: true
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: p_mfgr (type: string), p_name (type: string), p_size (type: int)
                      null sort order: aaz
                      sort order: ++-
                      Map-reduce partition columns: p_mfgr (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      auto parallelism: true
            Execution mode: llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), KEY.reducesinkkey2 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col1 ASC NULLS FIRST, _col5 DESC NULLS LAST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                        transforms raw input: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                    null sort order: aaz
                    sort order: ++-
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), KEY.reducesinkkey2 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST, _col5 DESC NULLS LAST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1, _col5
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3
                            columns.types string:string:int:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1
Manufacturer#1	almond antique burnished rose metallic	2	1
Manufacturer#1	almond antique chartreuse lavender yellow	34	3
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4
Manufacturer#1	almond aquamarine burnished black steel	28	5
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6
Manufacturer#2	almond antique violet chocolate turquoise	14	1
Manufacturer#2	almond antique violet turquoise frosted	40	2
Manufacturer#2	almond aquamarine midnight light salmon	2	3
Manufacturer#2	almond aquamarine rose maroon antique	25	4
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5
Manufacturer#3	almond antique chartreuse khaki white	17	1
Manufacturer#3	almond antique forest lavender goldenrod	14	2
Manufacturer#3	almond antique metallic orange dim	19	3
Manufacturer#3	almond antique misty red olive	1	4
Manufacturer#3	almond antique olive coral navajo	45	5
Manufacturer#4	almond antique gainsboro frosted violet	10	1
Manufacturer#4	almond antique violet mint lemon	39	2
Manufacturer#4	almond aquamarine floral ivory bisque	27	3
Manufacturer#4	almond aquamarine yellow dodger mint	7	4
Manufacturer#4	almond azure aquamarine papaya violet	12	5
Manufacturer#5	almond antique blue firebrick mint	31	1
Manufacturer#5	almond antique medium spring khaki	6	2
Manufacturer#5	almond antique sky peru orange	2	3
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4
Manufacturer#5	almond azure blanched chiffon midnight	23	5
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noopwithmap(on part_orc 
  partition by p_mfgr
  order by p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noopwithmap(on part_orc 
  partition by p_mfgr
  order by p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: part_orc
                          output shape: p_name: string, p_mfgr: string, p_size: int, p_retailprice: double
                          type: TABLE
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: p_name ASC NULLS FIRST
                          output shape: p_name: string, p_mfgr: string, p_size: int, p_retailprice: double
                          partition by: p_mfgr
                          raw input shape:
                          transforms raw input: true
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: p_mfgr (type: string), p_name (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: p_mfgr (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: p_size (type: int), p_retailprice (type: double)
                      auto parallelism: true
            Execution mode: llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                        transforms raw input: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noopwithmap(on part_orc 
  partition by p_mfgr
  order by p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noopwithmap(on part_orc 
  partition by p_mfgr
  order by p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.65
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.07
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.73
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.36
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.62
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.35
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.65
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.07
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.73
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.36
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.62
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.35
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on noopwithmap(on noop(on part_orc 
partition by p_mfgr 
order by p_mfgr, p_name
)))
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on noopwithmap(on noop(on part_orc 
partition by p_mfgr 
order by p_mfgr, p_name
)))
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: ptf_0
                          output shape: _col1: string, _col2: string, _col5: int, _col7: double
                          type: PTFCOMPONENT
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int, _col7: double
                          partition by: _col2
                          raw input shape:
                          transforms raw input: true
                        Partition table definition
                          input alias: ptf_2
                          name: noop
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int, _col7: double
                          partition by: _col2
                          raw input shape:
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: _col2 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col2 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col5 (type: int), _col7 (type: double)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                        transforms raw input: true
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on noopwithmap(on noop(on part_orc 
partition by p_mfgr 
order by p_mfgr, p_name
)))
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, p_size, 
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
round(sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row),2) as s1
from noop(on noopwithmap(on noop(on part_orc 
partition by p_mfgr 
order by p_mfgr, p_name
)))
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.65
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.07
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.73
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.36
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.62
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.35
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name, 
sub1.cd, sub1.s1 
from (select p_mfgr, p_name, 
count(p_size) over (partition by p_mfgr order by p_name) as cd, 
p_retailprice, 
round(sum(p_retailprice) over w1,2) as s1
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following) 
) sub1
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name, 
sub1.cd, sub1.s1 
from (select p_mfgr, p_name, 
count(p_size) over (partition by p_mfgr order by p_name) as cd, 
p_retailprice, 
round(sum(p_retailprice) over w1,2) as s1
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following) 
) sub1
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: count_window_0
                              arguments: _col5
                              name: count
                              window function: GenericUDAFCountEvaluator
                              window frame: PRECEDING(MAX)~CURRENT
                            window function definition
                              alias: sum_window_1
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(2)~FOLLOWING(2)
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), count_window_0 (type: bigint), round(sum_window_1, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3
                            columns.types string:string:bigint:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name, 
sub1.cd, sub1.s1 
from (select p_mfgr, p_name, 
count(p_size) over (partition by p_mfgr order by p_name) as cd, 
p_retailprice, 
round(sum(p_retailprice) over w1,2) as s1
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following) 
) sub1
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name, 
sub1.cd, sub1.s1 
from (select p_mfgr, p_name, 
count(p_size) over (partition by p_mfgr order by p_name) as cd, 
p_retailprice, 
round(sum(p_retailprice) over w1,2) as s1
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following) 
) sub1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	4100.06
Manufacturer#1	almond antique burnished rose metallic	2	5702.65
Manufacturer#1	almond antique chartreuse lavender yellow	3	7117.07
Manufacturer#1	almond antique salmon chartreuse burlywood	4	7576.58
Manufacturer#1	almond aquamarine burnished black steel	5	6403.43
Manufacturer#1	almond aquamarine pink moccasin thistle	6	4649.67
Manufacturer#2	almond antique violet chocolate turquoise	1	5523.36
Manufacturer#2	almond antique violet turquoise frosted	2	7222.02
Manufacturer#2	almond aquamarine midnight light salmon	3	8923.62
Manufacturer#2	almond aquamarine rose maroon antique	4	7232.94
Manufacturer#2	almond aquamarine sandy cyan gainsboro	5	5432.24
Manufacturer#3	almond antique chartreuse khaki white	1	4272.34
Manufacturer#3	almond antique forest lavender goldenrod	2	6195.32
Manufacturer#3	almond antique metallic orange dim	3	7532.61
Manufacturer#3	almond antique misty red olive	4	5860.93
Manufacturer#3	almond antique olive coral navajo	5	4670.66
Manufacturer#4	almond antique gainsboro frosted violet	1	4202.35
Manufacturer#4	almond antique violet mint lemon	2	6047.27
Manufacturer#4	almond aquamarine floral ivory bisque	3	7337.62
Manufacturer#4	almond aquamarine yellow dodger mint	4	5716.95
Manufacturer#4	almond azure aquamarine papaya violet	5	4341.53
Manufacturer#5	almond antique blue firebrick mint	1	5190.08
Manufacturer#5	almond antique medium spring khaki	2	6208.18
Manufacturer#5	almond antique sky peru orange	3	7672.66
Manufacturer#5	almond aquamarine dodger light gainsboro	4	5882.97
Manufacturer#5	almond azure blanched chiffon midnight	5	4271.31
PREHOOK: query: explain vectorization extended
select abc.p_mfgr, abc.p_name, 
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r, 
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr, 
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd, 
abc.p_retailprice, round(sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row),2) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select abc.p_mfgr, abc.p_name, 
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r, 
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr, 
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd, 
abc.p_retailprice, round(sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row),2) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Map 5 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_partkey (type: int), p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: p1
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Filter Operator
                    isSamplingPred: false
                    predicate: p_partkey is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: p_partkey (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: p_partkey (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 1
                      auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [p1]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col0: int, _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: abc
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col0: int, _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    isSamplingPred: false
                    predicate: _col0 is not null (type: boolean)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: 0
                      value expressions: _col1 (type: string), _col2 (type: string), _col5 (type: int), _col7 (type: double)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col0 (type: int)
                  1 p_partkey (type: int)
                outputColumnNames: _col1, _col2, _col5, _col7
                Position of Big Table: 0
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col2 (type: string), _col1 (type: string)
                  null sort order: aa
                  sort order: ++
                  Map-reduce partition columns: _col2 (type: string)
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  tag: -1
                  value expressions: _col5 (type: int), _col7 (type: double)
                  auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: count_window_2
                              arguments: _col1
                              name: count
                              window function: GenericUDAFCountEvaluator
                              window frame: PRECEDING(MAX)~CURRENT
                            window function definition
                              alias: sum_window_3
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                            window function definition
                              alias: lag_window_4
                              arguments: _col5, 1, _col5
                              name: lag
                              window function: GenericUDAFLagEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                  Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), count_window_2 (type: bigint), _col7 (type: double), round(sum_window_3, 2) (type: double), _col5 (type: int), (_col5 - lag_window_4) (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                    Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 28 Data size: 17646 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8
                            columns.types string:string:int:int:bigint:double:double:int:int
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select abc.p_mfgr, abc.p_name, 
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r, 
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr, 
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd, 
abc.p_retailprice, round(sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row),2) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select abc.p_mfgr, abc.p_name, 
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r, 
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr, 
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd, 
abc.p_retailprice, round(sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row),2) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz 
from noop(on part_orc 
partition by p_mfgr 
order by p_name 
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	4	1173.15	1173.15	2	0
Manufacturer#1	almond antique burnished rose metallic	1	1	4	1173.15	2346.3	2	0
Manufacturer#1	almond antique burnished rose metallic	1	1	4	1173.15	3519.45	2	0
Manufacturer#1	almond antique burnished rose metallic	1	1	4	1173.15	4692.6	2	0
Manufacturer#1	almond antique chartreuse lavender yellow	5	2	5	1753.76	6446.36	34	32
Manufacturer#1	almond antique salmon chartreuse burlywood	6	3	6	1602.59	8048.95	6	-28
Manufacturer#1	almond aquamarine burnished black steel	7	4	7	1414.42	9463.37	28	22
Manufacturer#1	almond aquamarine pink moccasin thistle	8	5	8	1632.66	11096.03	42	14
Manufacturer#2	almond antique violet chocolate turquoise	1	1	1	1690.68	1690.68	14	0
Manufacturer#2	almond antique violet turquoise frosted	2	2	2	1800.7	3491.38	40	26
Manufacturer#2	almond aquamarine midnight light salmon	3	3	3	2031.98	5523.36	2	-38
Manufacturer#2	almond aquamarine rose maroon antique	4	4	4	1698.66	7222.02	25	23
Manufacturer#2	almond aquamarine sandy cyan gainsboro	5	5	5	1701.6	8923.62	18	-7
Manufacturer#3	almond antique chartreuse khaki white	1	1	1	1671.68	1671.68	17	0
Manufacturer#3	almond antique forest lavender goldenrod	2	2	2	1190.27	2861.95	14	-3
Manufacturer#3	almond antique metallic orange dim	3	3	3	1410.39	4272.34	19	5
Manufacturer#3	almond antique misty red olive	4	4	4	1922.98	6195.32	1	-18
Manufacturer#3	almond antique olive coral navajo	5	5	5	1337.29	7532.61	45	44
Manufacturer#4	almond antique gainsboro frosted violet	1	1	1	1620.67	1620.67	10	0
Manufacturer#4	almond antique violet mint lemon	2	2	2	1375.42	2996.09	39	29
Manufacturer#4	almond aquamarine floral ivory bisque	3	3	3	1206.26	4202.35	27	-12
Manufacturer#4	almond aquamarine yellow dodger mint	4	4	4	1844.92	6047.27	7	-20
Manufacturer#4	almond azure aquamarine papaya violet	5	5	5	1290.35	7337.62	12	5
Manufacturer#5	almond antique blue firebrick mint	1	1	1	1789.69	1789.69	31	0
Manufacturer#5	almond antique medium spring khaki	2	2	2	1611.66	3401.35	6	-25
Manufacturer#5	almond antique sky peru orange	3	3	3	1788.73	5190.08	2	-4
Manufacturer#5	almond aquamarine dodger light gainsboro	4	4	4	1018.1	6208.18	46	44
Manufacturer#5	almond azure blanched chiffon midnight	5	5	5	1464.48	7672.66	23	-23
PREHOOK: query: explain vectorization extended
select DISTINCT p_mfgr, p_name, p_size 
from noop(on part_orc 
partition by p_mfgr 
order by p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select DISTINCT p_mfgr, p_name, p_size 
from noop(on part_orc 
partition by p_mfgr 
order by p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                    outputColumnNames: _col2, _col1, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      keys: _col2 (type: string), _col1 (type: string), _col5 (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int)
                        null sort order: aaa
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: int)
                        Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                        tag: -1
                        auto parallelism: true
        Reducer 3 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                groupByVectorOutput: true
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1,_col2
                        columns.types string:string:int
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select DISTINCT p_mfgr, p_name, p_size 
from noop(on part_orc 
partition by p_mfgr 
order by p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select DISTINCT p_mfgr, p_name, p_size 
from noop(on part_orc 
partition by p_mfgr 
order by p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2
Manufacturer#1	almond antique chartreuse lavender yellow	34
Manufacturer#1	almond antique salmon chartreuse burlywood	6
Manufacturer#1	almond aquamarine burnished black steel	28
Manufacturer#1	almond aquamarine pink moccasin thistle	42
Manufacturer#2	almond antique violet chocolate turquoise	14
Manufacturer#2	almond antique violet turquoise frosted	40
Manufacturer#2	almond aquamarine midnight light salmon	2
Manufacturer#2	almond aquamarine rose maroon antique	25
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18
Manufacturer#3	almond antique chartreuse khaki white	17
Manufacturer#3	almond antique forest lavender goldenrod	14
Manufacturer#3	almond antique metallic orange dim	19
Manufacturer#3	almond antique misty red olive	1
Manufacturer#3	almond antique olive coral navajo	45
Manufacturer#4	almond antique gainsboro frosted violet	10
Manufacturer#4	almond antique violet mint lemon	39
Manufacturer#4	almond aquamarine floral ivory bisque	27
Manufacturer#4	almond aquamarine yellow dodger mint	7
Manufacturer#4	almond azure aquamarine papaya violet	12
Manufacturer#5	almond antique blue firebrick mint	31
Manufacturer#5	almond antique medium spring khaki	6
Manufacturer#5	almond antique sky peru orange	2
Manufacturer#5	almond aquamarine dodger light gainsboro	46
Manufacturer#5	almond azure blanched chiffon midnight	23
PREHOOK: query: create view IF NOT EXISTS mfgr_price_view as 
select p_mfgr, p_brand, 
round(sum(p_retailprice),2) as s
from part_orc 
group by p_mfgr, p_brand
PREHOOK: type: CREATEVIEW
PREHOOK: Input: default@part_orc
PREHOOK: Output: database:default
PREHOOK: Output: default@mfgr_price_view
POSTHOOK: query: create view IF NOT EXISTS mfgr_price_view as 
select p_mfgr, p_brand, 
round(sum(p_retailprice),2) as s
from part_orc 
group by p_mfgr, p_brand
POSTHOOK: type: CREATEVIEW
POSTHOOK: Input: default@part_orc
POSTHOOK: Output: database:default
POSTHOOK: Output: default@mfgr_price_view
POSTHOOK: Lineage: mfgr_price_view.p_brand SIMPLE [(part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), ]
POSTHOOK: Lineage: mfgr_price_view.p_mfgr SIMPLE [(part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), ]
POSTHOOK: Lineage: mfgr_price_view.s EXPRESSION [(part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), ]
PREHOOK: query: explain vectorization extended
select p_mfgr, p_brand, s, 
round(sum(s) over w1,2) as s1
from noop(on mfgr_price_view 
partition by p_mfgr 
order by p_mfgr)  
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_brand, s, 
round(sum(s) over w1,2) as s1
from noop(on mfgr_price_view 
partition by p_mfgr 
order by p_mfgr)  
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Select Operator
                    expressions: p_mfgr (type: string), p_brand (type: string), p_retailprice (type: double)
                    outputColumnNames: p_mfgr, p_brand, p_retailprice
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(p_retailprice)
                      keys: p_mfgr (type: string), p_brand (type: string)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string), _col1 (type: string)
                        null sort order: aa
                        sort order: ++
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                        tag: -1
                        value expressions: _col2 (type: double)
                        auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: string), _col1 (type: string), round(_col2, 2) (type: double)
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: mfgr_price_view
                          output shape: _col0: string, _col1: string, _col2: double
                          type: TABLE
                        Partition table definition
                          input alias: ptf_1
                          name: noop
                          order by: _col0 ASC NULLS FIRST
                          output shape: _col0: string, _col1: string, _col2: double
                          partition by: _col0
                          raw input shape:
                    Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col0 (type: string)
                      Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col2 (type: double)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: double)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col0: string, _col1: string, _col2: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col0
                        raw input shape:
                        window functions:
                            window function definition
                              alias: sum_window_0
                              arguments: _col2
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(2)~CURRENT
                  Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: string), _col1 (type: string), _col2 (type: double), round(sum_window_0, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3
                    Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 13 Data size: 8021 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3
                            columns.types string:string:double:double
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_brand, s, 
round(sum(s) over w1,2) as s1
from noop(on mfgr_price_view 
partition by p_mfgr 
order by p_mfgr)  
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row)
PREHOOK: type: QUERY
PREHOOK: Input: default@mfgr_price_view
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_brand, s, 
round(sum(s) over w1,2) as s1
from noop(on mfgr_price_view 
partition by p_mfgr 
order by p_mfgr)  
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mfgr_price_view
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	Brand#12	4800.84	4800.84
Manufacturer#1	Brand#14	2346.3	7147.14
Manufacturer#1	Brand#15	1602.59	8749.73
Manufacturer#2	Brand#22	3491.38	3491.38
Manufacturer#2	Brand#23	2031.98	5523.36
Manufacturer#2	Brand#24	1698.66	7222.02
Manufacturer#2	Brand#25	1701.6	5432.24
Manufacturer#3	Brand#31	1671.68	1671.68
Manufacturer#3	Brand#32	3333.37	5005.05
Manufacturer#3	Brand#34	1337.29	6342.34
Manufacturer#3	Brand#35	1190.27	5860.93
Manufacturer#4	Brand#41	4755.94	4755.94
Manufacturer#4	Brand#42	2581.68	7337.62
Manufacturer#5	Brand#51	1611.66	1611.66
Manufacturer#5	Brand#52	3254.17	4865.83
Manufacturer#5	Brand#53	2806.83	7672.66
PREHOOK: query: CREATE TABLE part_4( 
p_mfgr STRING, 
p_name STRING, 
p_size INT, 
r INT, 
dr INT, 
s DOUBLE)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_4
POSTHOOK: query: CREATE TABLE part_4( 
p_mfgr STRING, 
p_name STRING, 
p_size INT, 
r INT, 
dr INT, 
s DOUBLE)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_4
PREHOOK: query: CREATE TABLE part_5( 
p_mfgr STRING, 
p_name STRING, 
p_size INT, 
s2 INT, 
r INT, 
dr INT, 
cud DOUBLE, 
fv1 INT)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@part_5
POSTHOOK: query: CREATE TABLE part_5( 
p_mfgr STRING, 
p_name STRING, 
p_size INT, 
s2 INT, 
r INT, 
dr INT, 
cud DOUBLE, 
fv1 INT)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@part_5
PREHOOK: query: explain vectorization extended
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size, 
rank() over (distribute by p_mfgr sort by p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_name) as dr, 
round(sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row),2) as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,  
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr, 
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud, 
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size, 
rank() over (distribute by p_mfgr sort by p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_name) as dr, 
round(sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row),2) as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,  
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr, 
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud, 
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-0
  Stage-1 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int), p_retailprice (type: double)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int), _col7 (type: double)
                    auto parallelism: true
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col5 (type: int)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col1 (type: string)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int), VALUE._col5 (type: double)
                outputColumnNames: _col1, _col2, _col5, _col7
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int, _col7: double
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col7
                              name: sum
                              window function: GenericUDAFSumDouble
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), _col5 (type: int), rank_window_0 (type: int), dense_rank_window_1 (type: int), round(sum_window_2, 2) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 1
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                            bucket_count -1
                            column.name.delimiter ,
                            columns p_mfgr,p_name,p_size,r,dr,s
                            columns.comments 
                            columns.types string:string:int:int:int:double
#### A masked pattern was here ####
                            name default.part_4
                            numFiles 0
                            numRows 0
                            rawDataSize 0
                            serialization.ddl struct part_4 { string p_mfgr, string p_name, i32 p_size, i32 r, i32 dr, double s}
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                            totalSize 0
#### A masked pattern was here ####
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.part_4
                      TotalFiles: 1
                      GatherStats: true
                      MultiFileSpray: false
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col5 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: sum_window_0
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(5)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col1 (type: string), _col2 (type: string), _col5 (type: int), sum_window_0 (type: bigint)
                    outputColumnNames: _col1, _col2, _col5, sum_window_0
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col2 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col2 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: sum_window_0 (type: bigint), _col5 (type: int)
                      auto parallelism: true
        Reducer 5 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: bigint), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col0, _col2, _col3, _col6
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col0: bigint, _col2: string, _col3: string, _col6: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col3 ASC NULLS FIRST, _col2 ASC NULLS FIRST
                        partition by: _col3
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_1
                              arguments: _col3, _col2
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_2
                              arguments: _col3, _col2
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: cume_dist_window_3
                              arguments: _col3, _col2
                              name: cume_dist
                              window function: GenericUDAFCumeDistEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: first_value_window_4
                              arguments: _col6, true
                              name: first_value
                              window function: GenericUDAFFirstValueEvaluator
                              window frame: PRECEDING(2)~FOLLOWING(2)
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col3 (type: string), _col2 (type: string), _col6 (type: int), UDFToInteger(round(_col0, 1)) (type: int), rank_window_1 (type: int), dense_rank_window_2 (type: int), cume_dist_window_3 (type: double), first_value_window_4 (type: int)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 2
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                            bucket_count -1
                            column.name.delimiter ,
                            columns p_mfgr,p_name,p_size,s2,r,dr,cud,fv1
                            columns.comments 
                            columns.types string:string:int:int:int:int:double:int
#### A masked pattern was here ####
                            name default.part_5
                            numFiles 0
                            numRows 0
                            rawDataSize 0
                            serialization.ddl struct part_5 { string p_mfgr, string p_name, i32 p_size, i32 s2, i32 r, i32 dr, double cud, i32 fv1}
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                            totalSize 0
#### A masked pattern was here ####
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          name: default.part_5
                      TotalFiles: 1
                      GatherStats: true
                      MultiFileSpray: false

  Stage: Stage-3
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
#### A masked pattern was here ####
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns p_mfgr,p_name,p_size,r,dr,s
                columns.comments 
                columns.types string:string:int:int:int:double
#### A masked pattern was here ####
                name default.part_4
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct part_4 { string p_mfgr, string p_name, i32 p_size, i32 r, i32 dr, double s}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.part_4

  Stage: Stage-4
    Stats-Aggr Operator
#### A masked pattern was here ####

  Stage: Stage-1
    Move Operator
      tables:
          replace: true
#### A masked pattern was here ####
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                bucket_count -1
                column.name.delimiter ,
                columns p_mfgr,p_name,p_size,s2,r,dr,cud,fv1
                columns.comments 
                columns.types string:string:int:int:int:int:double:int
#### A masked pattern was here ####
                name default.part_5
                numFiles 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct part_5 { string p_mfgr, string p_name, i32 p_size, i32 s2, i32 r, i32 dr, double cud, i32 fv1}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 0
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.part_5

  Stage: Stage-5
    Stats-Aggr Operator
#### A masked pattern was here ####

PREHOOK: query: from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size, 
rank() over (distribute by p_mfgr sort by p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_name) as dr, 
round(sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row),2) as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,  
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr, 
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud, 
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
PREHOOK: Output: default@part_4
PREHOOK: Output: default@part_5
POSTHOOK: query: from noop(on part_orc 
partition by p_mfgr 
order by p_name) 
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size, 
rank() over (distribute by p_mfgr sort by p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_name) as dr, 
round(sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row),2) as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,  
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r, 
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr, 
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud, 
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
POSTHOOK: Output: default@part_4
POSTHOOK: Output: default@part_5
POSTHOOK: Lineage: part_4.dr SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_4.p_mfgr SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_4.p_name SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_4.p_size SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_4.r SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_4.s SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.cud SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.dr SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.fv1 SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.p_mfgr SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.p_name SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.p_size SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.r SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
POSTHOOK: Lineage: part_5.s2 SCRIPT [(part_orc)part_orc.FieldSchema(name:p_partkey, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_name, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_mfgr, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_brand, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_type, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_size, type:int, comment:null), (part_orc)part_orc.FieldSchema(name:p_container, type:string, comment:null), (part_orc)part_orc.FieldSchema(name:p_retailprice, type:double, comment:null), (part_orc)part_orc.FieldSchema(name:p_comment, type:string, comment:null), ]
PREHOOK: query: select * from part_4
PREHOOK: type: QUERY
PREHOOK: Input: default@part_4
#### A masked pattern was here ####
POSTHOOK: query: select * from part_4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_4
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	1	1	1173.15
Manufacturer#1	almond antique burnished rose metallic	2	1	1	2346.3
Manufacturer#1	almond antique chartreuse lavender yellow	34	3	2	4100.06
Manufacturer#1	almond antique salmon chartreuse burlywood	6	4	3	5702.65
Manufacturer#1	almond aquamarine burnished black steel	28	5	4	7117.07
Manufacturer#1	almond aquamarine pink moccasin thistle	42	6	5	8749.73
Manufacturer#2	almond antique violet chocolate turquoise	14	1	1	1690.68
Manufacturer#2	almond antique violet turquoise frosted	40	2	2	3491.38
Manufacturer#2	almond aquamarine midnight light salmon	2	3	3	5523.36
Manufacturer#2	almond aquamarine rose maroon antique	25	4	4	7222.02
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	5	5	8923.62
Manufacturer#3	almond antique chartreuse khaki white	17	1	1	1671.68
Manufacturer#3	almond antique forest lavender goldenrod	14	2	2	2861.95
Manufacturer#3	almond antique metallic orange dim	19	3	3	4272.34
Manufacturer#3	almond antique misty red olive	1	4	4	6195.32
Manufacturer#3	almond antique olive coral navajo	45	5	5	7532.61
Manufacturer#4	almond antique gainsboro frosted violet	10	1	1	1620.67
Manufacturer#4	almond antique violet mint lemon	39	2	2	2996.09
Manufacturer#4	almond aquamarine floral ivory bisque	27	3	3	4202.35
Manufacturer#4	almond aquamarine yellow dodger mint	7	4	4	6047.27
Manufacturer#4	almond azure aquamarine papaya violet	12	5	5	7337.62
Manufacturer#5	almond antique blue firebrick mint	31	1	1	1789.69
Manufacturer#5	almond antique medium spring khaki	6	2	2	3401.35
Manufacturer#5	almond antique sky peru orange	2	3	3	5190.08
Manufacturer#5	almond aquamarine dodger light gainsboro	46	4	4	6208.18
Manufacturer#5	almond azure blanched chiffon midnight	23	5	5	7672.66
PREHOOK: query: select * from part_5
PREHOOK: type: QUERY
PREHOOK: Input: default@part_5
#### A masked pattern was here ####
POSTHOOK: query: select * from part_5
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_5
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	2	4	1	1	0.3333333333333333	2
Manufacturer#1	almond antique burnished rose metallic	2	4	1	1	0.3333333333333333	2
Manufacturer#1	almond antique chartreuse lavender yellow	34	34	3	2	0.5	2
Manufacturer#1	almond antique salmon chartreuse burlywood	6	10	4	3	0.6666666666666666	2
Manufacturer#1	almond aquamarine burnished black steel	28	28	5	4	0.8333333333333334	34
Manufacturer#1	almond aquamarine pink moccasin thistle	42	42	6	5	1.0	6
Manufacturer#2	almond antique violet chocolate turquoise	14	14	1	1	0.2	14
Manufacturer#2	almond antique violet turquoise frosted	40	40	2	2	0.4	14
Manufacturer#2	almond aquamarine midnight light salmon	2	2	3	3	0.6	14
Manufacturer#2	almond aquamarine rose maroon antique	25	25	4	4	0.8	40
Manufacturer#2	almond aquamarine sandy cyan gainsboro	18	32	5	5	1.0	2
Manufacturer#3	almond antique chartreuse khaki white	17	31	1	1	0.2	17
Manufacturer#3	almond antique forest lavender goldenrod	14	14	2	2	0.4	17
Manufacturer#3	almond antique metallic orange dim	19	50	3	3	0.6	17
Manufacturer#3	almond antique misty red olive	1	1	4	4	0.8	14
Manufacturer#3	almond antique olive coral navajo	45	45	5	5	1.0	19
Manufacturer#4	almond antique gainsboro frosted violet	10	17	1	1	0.2	10
Manufacturer#4	almond antique violet mint lemon	39	39	2	2	0.4	10
Manufacturer#4	almond aquamarine floral ivory bisque	27	27	3	3	0.6	10
Manufacturer#4	almond aquamarine yellow dodger mint	7	7	4	4	0.8	39
Manufacturer#4	almond azure aquamarine papaya violet	12	29	5	5	1.0	27
Manufacturer#5	almond antique blue firebrick mint	31	31	1	1	0.2	31
Manufacturer#5	almond antique medium spring khaki	6	8	2	2	0.4	31
Manufacturer#5	almond antique sky peru orange	2	2	3	3	0.6	31
Manufacturer#5	almond aquamarine dodger light gainsboro	46	46	4	4	0.8	6
Manufacturer#5	almond azure blanched chiffon midnight	23	23	5	5	1.0	2
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noopwithmap(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr,p_name  
        order by p_mfgr,p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noopwithmap(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr,p_name  
        order by p_mfgr,p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_name (type: string), p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: ptf_0
                          output shape: _col1: string, _col2: string, _col5: int
                          type: PTFCOMPONENT
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2, _col1
                          raw input shape:
                          transforms raw input: true
                        Partition table definition
                          input alias: ptf_2
                          name: noop
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2, _col1
                          raw input shape:
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: _col2 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col5 (type: int)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                        transforms raw input: true
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        partition by: _col2, _col1
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col2, _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col2, _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noopwithmap(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr,p_name  
        order by p_mfgr,p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noopwithmap(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr,p_name  
        order by p_mfgr,p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	2
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4
Manufacturer#1	almond antique chartreuse lavender yellow	1	1	34	34
Manufacturer#1	almond antique salmon chartreuse burlywood	1	1	6	6
Manufacturer#1	almond aquamarine burnished black steel	1	1	28	28
Manufacturer#1	almond aquamarine pink moccasin thistle	1	1	42	42
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14
Manufacturer#2	almond antique violet turquoise frosted	1	1	40	40
Manufacturer#2	almond aquamarine midnight light salmon	1	1	2	2
Manufacturer#2	almond aquamarine rose maroon antique	1	1	25	25
Manufacturer#2	almond aquamarine sandy cyan gainsboro	1	1	18	18
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17
Manufacturer#3	almond antique forest lavender goldenrod	1	1	14	14
Manufacturer#3	almond antique metallic orange dim	1	1	19	19
Manufacturer#3	almond antique misty red olive	1	1	1	1
Manufacturer#3	almond antique olive coral navajo	1	1	45	45
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10
Manufacturer#4	almond antique violet mint lemon	1	1	39	39
Manufacturer#4	almond aquamarine floral ivory bisque	1	1	27	27
Manufacturer#4	almond aquamarine yellow dodger mint	1	1	7	7
Manufacturer#4	almond azure aquamarine papaya violet	1	1	12	12
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31
Manufacturer#5	almond antique medium spring khaki	1	1	6	6
Manufacturer#5	almond antique sky peru orange	1	1	2	2
Manufacturer#5	almond aquamarine dodger light gainsboro	1	1	46	46
Manufacturer#5	almond azure blanched chiffon midnight	1	1	23	23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr  
        order by p_mfgr )
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr  
        order by p_mfgr )
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: p_mfgr (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_name (type: string), p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col1 (type: string), _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 5 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr  
        order by p_mfgr )
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr 
              order by p_mfgr) 
            ) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name) 
        partition by p_mfgr  
        order by p_mfgr )
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	2
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4
Manufacturer#1	almond antique chartreuse lavender yellow	3	2	34	38
Manufacturer#1	almond antique salmon chartreuse burlywood	4	3	6	44
Manufacturer#1	almond aquamarine burnished black steel	5	4	28	72
Manufacturer#1	almond aquamarine pink moccasin thistle	6	5	42	114
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14
Manufacturer#2	almond antique violet turquoise frosted	2	2	40	54
Manufacturer#2	almond aquamarine midnight light salmon	3	3	2	56
Manufacturer#2	almond aquamarine rose maroon antique	4	4	25	81
Manufacturer#2	almond aquamarine sandy cyan gainsboro	5	5	18	99
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17
Manufacturer#3	almond antique forest lavender goldenrod	2	2	14	31
Manufacturer#3	almond antique metallic orange dim	3	3	19	50
Manufacturer#3	almond antique misty red olive	4	4	1	51
Manufacturer#3	almond antique olive coral navajo	5	5	45	96
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10
Manufacturer#4	almond antique violet mint lemon	2	2	39	49
Manufacturer#4	almond aquamarine floral ivory bisque	3	3	27	76
Manufacturer#4	almond aquamarine yellow dodger mint	4	4	7	83
Manufacturer#4	almond azure aquamarine papaya violet	5	5	12	95
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31
Manufacturer#5	almond antique medium spring khaki	2	2	6	37
Manufacturer#5	almond antique sky peru orange	3	3	2	39
Manufacturer#5	almond aquamarine dodger light gainsboro	4	4	46	85
Manufacturer#5	almond azure blanched chiffon midnight	5	5	23	108
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1 
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr))
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1 
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr))
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string), p_name (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col1 (type: string), _col5 (type: int)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1 
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr))
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1 
from noop(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr))
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4
Manufacturer#1	almond antique chartreuse lavender yellow	3	2	34	38
Manufacturer#1	almond antique salmon chartreuse burlywood	4	3	6	44
Manufacturer#1	almond aquamarine burnished black steel	5	4	28	72
Manufacturer#1	almond aquamarine pink moccasin thistle	6	5	42	114
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14
Manufacturer#2	almond antique violet turquoise frosted	2	2	40	54
Manufacturer#2	almond aquamarine midnight light salmon	3	3	2	56
Manufacturer#2	almond aquamarine rose maroon antique	4	4	25	81
Manufacturer#2	almond aquamarine sandy cyan gainsboro	5	5	18	99
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17
Manufacturer#3	almond antique forest lavender goldenrod	2	2	14	31
Manufacturer#3	almond antique metallic orange dim	3	3	19	50
Manufacturer#3	almond antique misty red olive	4	4	1	51
Manufacturer#3	almond antique olive coral navajo	5	5	45	96
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10
Manufacturer#4	almond antique violet mint lemon	2	2	39	49
Manufacturer#4	almond aquamarine floral ivory bisque	3	3	27	76
Manufacturer#4	almond aquamarine yellow dodger mint	4	4	7	83
Manufacturer#4	almond azure aquamarine papaya violet	5	5	12	95
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31
Manufacturer#5	almond antique medium spring khaki	2	2	6	37
Manufacturer#5	almond antique sky peru orange	3	3	2	39
Manufacturer#5	almond aquamarine dodger light gainsboro	4	4	46	85
Manufacturer#5	almond azure blanched chiffon midnight	5	5	23	108
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1 
from noopwithmap(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name)
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1 
from noopwithmap(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name)
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
        Reducer 5 <- Reducer 4 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string), p_name (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string)
                    null sort order: a
                    sort order: +
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col1 (type: string), _col5 (type: int)
                    auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: ptf_0
                          output shape: _col1: string, _col2: string, _col5: int
                          type: PTFCOMPONENT
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2, _col1
                          raw input shape:
                          transforms raw input: true
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: _col2 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col5 (type: int)
                      auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                        transforms raw input: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 5 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        partition by: _col2, _col1
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col2, _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col2, _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5
                            columns.types string:string:int:int:int:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1 
from noopwithmap(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name)
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name) as dr, 
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1 
from noopwithmap(on 
        noop(on 
          noop(on 
              noop(on part_orc 
              partition by p_mfgr,p_name 
              order by p_mfgr,p_name) 
            ) 
          partition by p_mfgr 
          order by p_mfgr) 
          partition by p_mfgr,p_name 
          order by p_mfgr,p_name)
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	2
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4
Manufacturer#1	almond antique chartreuse lavender yellow	1	1	34	34
Manufacturer#1	almond antique salmon chartreuse burlywood	1	1	6	6
Manufacturer#1	almond aquamarine burnished black steel	1	1	28	28
Manufacturer#1	almond aquamarine pink moccasin thistle	1	1	42	42
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14
Manufacturer#2	almond antique violet turquoise frosted	1	1	40	40
Manufacturer#2	almond aquamarine midnight light salmon	1	1	2	2
Manufacturer#2	almond aquamarine rose maroon antique	1	1	25	25
Manufacturer#2	almond aquamarine sandy cyan gainsboro	1	1	18	18
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17
Manufacturer#3	almond antique forest lavender goldenrod	1	1	14	14
Manufacturer#3	almond antique metallic orange dim	1	1	19	19
Manufacturer#3	almond antique misty red olive	1	1	1	1
Manufacturer#3	almond antique olive coral navajo	1	1	45	45
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10
Manufacturer#4	almond antique violet mint lemon	1	1	39	39
Manufacturer#4	almond aquamarine floral ivory bisque	1	1	27	27
Manufacturer#4	almond aquamarine yellow dodger mint	1	1	7	7
Manufacturer#4	almond azure aquamarine papaya violet	1	1	12	12
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31
Manufacturer#5	almond antique medium spring khaki	1	1	6	6
Manufacturer#5	almond antique sky peru orange	1	1	2	2
Manufacturer#5	almond aquamarine dodger light gainsboro	1	1	46	46
Manufacturer#5	almond azure blanched chiffon midnight	1	1	23	23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on 
        noopwithmap(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          partition by p_mfgr 
          order by p_mfgr
          ))
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on 
        noopwithmap(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          partition by p_mfgr 
          order by p_mfgr
          ))
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string), p_name (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: ptf_0
                          output shape: _col1: string, _col2: string, _col5: int
                          type: PTFCOMPONENT
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: _col2 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2
                          raw input shape:
                          transforms raw input: true
                        Partition table definition
                          input alias: ptf_2
                          name: noop
                          order by: _col2 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2
                          raw input shape:
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: _col2 (type: string)
                      null sort order: a
                      sort order: +
                      Map-reduce partition columns: _col2 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col1 (type: string), _col5 (type: int)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col4 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                        transforms raw input: true
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        partition by: _col2, _col1
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col2, _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col2, _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                            columns.types string:string:int:int:int:bigint:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on 
        noopwithmap(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          partition by p_mfgr 
          order by p_mfgr
          ))
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r, 
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on 
        noopwithmap(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          partition by p_mfgr 
          order by p_mfgr
          ))
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	2	2
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4	4
Manufacturer#1	almond antique chartreuse lavender yellow	1	1	34	34	34
Manufacturer#1	almond antique salmon chartreuse burlywood	1	1	6	6	6
Manufacturer#1	almond aquamarine burnished black steel	1	1	28	28	28
Manufacturer#1	almond aquamarine pink moccasin thistle	1	1	42	42	42
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14	14
Manufacturer#2	almond antique violet turquoise frosted	1	1	40	40	40
Manufacturer#2	almond aquamarine midnight light salmon	1	1	2	2	2
Manufacturer#2	almond aquamarine rose maroon antique	1	1	25	25	25
Manufacturer#2	almond aquamarine sandy cyan gainsboro	1	1	18	18	18
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17	17
Manufacturer#3	almond antique forest lavender goldenrod	1	1	14	14	14
Manufacturer#3	almond antique metallic orange dim	1	1	19	19	19
Manufacturer#3	almond antique misty red olive	1	1	1	1	1
Manufacturer#3	almond antique olive coral navajo	1	1	45	45	45
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10	10
Manufacturer#4	almond antique violet mint lemon	1	1	39	39	39
Manufacturer#4	almond aquamarine floral ivory bisque	1	1	27	27	27
Manufacturer#4	almond aquamarine yellow dodger mint	1	1	7	7	7
Manufacturer#4	almond azure aquamarine papaya violet	1	1	12	12	12
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31	31
Manufacturer#5	almond antique medium spring khaki	1	1	6	6	6
Manufacturer#5	almond antique sky peru orange	1	1	2	2	2
Manufacturer#5	almond aquamarine dodger light gainsboro	1	1	46	46	46
Manufacturer#5	almond azure blanched chiffon midnight	1	1	23	23	23
PREHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on 
        noop(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          ))
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization extended
select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on 
        noop(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          ))
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: part_orc
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  GatherStats: false
                  Reduce Output Operator
                    key expressions: p_mfgr (type: string), p_name (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: p_mfgr (type: string), p_name (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: p_size (type: int)
                    auto parallelism: true
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                groupByVectorOutput: true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: part_orc
                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                  properties:
                    COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                    bucket_count -1
                    column.name.delimiter ,
                    columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                    columns.comments 
                    columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                    name default.part_orc
                    numFiles 1
                    numRows 26
                    rawDataSize 16042
                    serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    totalSize 2689
#### A masked pattern was here ####
                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    properties:
                      COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
                      bucket_count -1
                      column.name.delimiter ,
                      columns p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment
                      columns.comments 
                      columns.types int:string:string:string:string:int:string:double:string
#### A masked pattern was here ####
                      name default.part_orc
                      numFiles 1
                      numRows 26
                      rawDataSize 16042
                      serialization.ddl struct part_orc { i32 p_partkey, string p_name, string p_mfgr, string p_brand, string p_type, i32 p_size, string p_container, double p_retailprice, string p_comment}
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      totalSize 2689
#### A masked pattern was here ####
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.part_orc
                  name: default.part_orc
            Truncated Path -> Alias:
              /part_orc [part_orc]
        Reducer 2 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: part_orc
                        output shape: _col1: string, _col2: string, _col5: int
                        type: TABLE
                      Partition table definition
                        input alias: ptf_1
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                      Partition table definition
                        input alias: ptf_2
                        name: noop
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  PTF Operator
                    Function definitions:
                        Input definition
                          input alias: ptf_0
                          output shape: _col1: string, _col2: string, _col5: int
                          type: PTFCOMPONENT
                        Partition table definition
                          input alias: ptf_1
                          name: noopwithmap
                          order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                          output shape: _col1: string, _col2: string, _col5: int
                          partition by: _col2, _col1
                          raw input shape:
                          transforms raw input: true
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    Map-side function: true
                    Reduce Output Operator
                      key expressions: _col2 (type: string), _col1 (type: string)
                      null sort order: aa
                      sort order: ++
                      Map-reduce partition columns: _col2 (type: string), _col1 (type: string)
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                      tag: -1
                      value expressions: _col5 (type: int)
                      auto parallelism: true
        Reducer 3 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: PTFCOMPONENT
                      Partition table definition
                        input alias: ptf_1
                        name: noopwithmap
                        order by: _col2 ASC NULLS FIRST, _col1 ASC NULLS FIRST
                        output shape: _col1: string, _col2: string, _col5: int
                        partition by: _col2, _col1
                        raw input shape:
                        transforms raw input: true
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col2 (type: string), _col1 (type: string)
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col2 (type: string)
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    tag: -1
                    value expressions: _col5 (type: int)
                    auto parallelism: true
        Reducer 4 
            Execution mode: llap
            Needs Tagging: false
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: PTF Operator (PTF) not supported
                vectorized: false
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey1 (type: string), KEY.reducesinkkey0 (type: string), VALUE._col3 (type: int)
                outputColumnNames: _col1, _col2, _col5
                Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                PTF Operator
                  Function definitions:
                      Input definition
                        input alias: ptf_0
                        output shape: _col1: string, _col2: string, _col5: int
                        type: WINDOWING
                      Windowing table definition
                        input alias: ptf_1
                        name: windowingtablefunction
                        order by: _col1 ASC NULLS FIRST
                        partition by: _col2
                        raw input shape:
                        window functions:
                            window function definition
                              alias: rank_window_0
                              arguments: _col1
                              name: rank
                              window function: GenericUDAFRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: dense_rank_window_1
                              arguments: _col1
                              name: dense_rank
                              window function: GenericUDAFDenseRankEvaluator
                              window frame: PRECEDING(MAX)~FOLLOWING(MAX)
                              isPivotResult: true
                            window function definition
                              alias: sum_window_2
                              arguments: _col5
                              name: sum
                              window function: GenericUDAFSumLong
                              window frame: PRECEDING(MAX)~CURRENT
                  Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col2 (type: string), _col1 (type: string), rank_window_0 (type: int), dense_rank_window_1 (type: int), _col5 (type: int), sum_window_2 (type: bigint), sum_window_2 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
#### A masked pattern was here ####
                      NumFilesPerFileSink: 1
                      Statistics: Num rows: 26 Data size: 16042 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          properties:
                            columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                            columns.types string:string:int:int:int:bigint:bigint
                            escape.delim \
                            hive.serialization.extend.additional.nesting.levels true
                            serialization.escape.crlf true
                            serialization.format 1
                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      TotalFiles: 1
                      GatherStats: false
                      MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on 
        noop(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          ))
PREHOOK: type: QUERY
PREHOOK: Input: default@part_orc
#### A masked pattern was here ####
POSTHOOK: query: select p_mfgr, p_name,  
rank() over (partition by p_mfgr order by p_name) as r, 
dense_rank() over (partition by p_mfgr order by p_name) as dr, 
p_size, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1, 
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on 
        noop(on 
              noop(on part_orc 
              partition by p_mfgr, p_name 
              order by p_mfgr, p_name) 
          ))
POSTHOOK: type: QUERY
POSTHOOK: Input: default@part_orc
#### A masked pattern was here ####
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4	4
Manufacturer#1	almond antique burnished rose metallic	1	1	2	4	4
Manufacturer#1	almond antique chartreuse lavender yellow	3	2	34	38	38
Manufacturer#1	almond antique salmon chartreuse burlywood	4	3	6	44	44
Manufacturer#1	almond aquamarine burnished black steel	5	4	28	72	72
Manufacturer#1	almond aquamarine pink moccasin thistle	6	5	42	114	114
Manufacturer#2	almond antique violet chocolate turquoise	1	1	14	14	14
Manufacturer#2	almond antique violet turquoise frosted	2	2	40	54	54
Manufacturer#2	almond aquamarine midnight light salmon	3	3	2	56	56
Manufacturer#2	almond aquamarine rose maroon antique	4	4	25	81	81
Manufacturer#2	almond aquamarine sandy cyan gainsboro	5	5	18	99	99
Manufacturer#3	almond antique chartreuse khaki white	1	1	17	17	17
Manufacturer#3	almond antique forest lavender goldenrod	2	2	14	31	31
Manufacturer#3	almond antique metallic orange dim	3	3	19	50	50
Manufacturer#3	almond antique misty red olive	4	4	1	51	51
Manufacturer#3	almond antique olive coral navajo	5	5	45	96	96
Manufacturer#4	almond antique gainsboro frosted violet	1	1	10	10	10
Manufacturer#4	almond antique violet mint lemon	2	2	39	49	49
Manufacturer#4	almond aquamarine floral ivory bisque	3	3	27	76	76
Manufacturer#4	almond aquamarine yellow dodger mint	4	4	7	83	83
Manufacturer#4	almond azure aquamarine papaya violet	5	5	12	95	95
Manufacturer#5	almond antique blue firebrick mint	1	1	31	31	31
Manufacturer#5	almond antique medium spring khaki	2	2	6	37	37
Manufacturer#5	almond antique sky peru orange	3	3	2	39	39
Manufacturer#5	almond aquamarine dodger light gainsboro	4	4	46	85	85
Manufacturer#5	almond azure blanched chiffon midnight	5	5	23	108	108
