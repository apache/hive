PREHOOK: query: DROP TABLE Employee_Part
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE Employee_Part
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double)
row format delimited fields terminated by '|'  stored as textfile
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@Employee_Part
POSTHOOK: query: CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double)
row format delimited fields terminated by '|'  stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@Employee_Part
PREHOOK: query: LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=2000.0)
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@employee_part
POSTHOOK: query: LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=2000.0)
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: query: LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=4000.0)
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@employee_part
POSTHOOK: query: LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=4000.0)
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
PREHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  filterExpr: (employeesalary = 2000.0D) (type: boolean)
                  Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                  Select Operator
                    expressions: employeeid (type: int)
                    outputColumnNames: employeeid
                    Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid)
                      keys: 2000.0D (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                      Reduce Output Operator
                        key expressions: 2000.0D (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: 2000.0D (type: double)
                        Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary)
            Execution mode: llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4)
                keys: 2000.0D (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 2000.0D (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: employeeID
          Column Types: int
          Table: default.employee_part

PREHOOK: query: explain extended
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: explain extended
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  filterExpr: (employeesalary = 2000.0D) (type: boolean)
                  Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                  Statistics Aggregation Key Prefix: default.employee_part/
                  GatherStats: true
                  Select Operator
                    expressions: employeeid (type: int)
                    outputColumnNames: employeeid
                    Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid)
                      keys: 2000.0D (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: 2000.0D (type: double)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: 2000.0D (type: double)
                        Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                        tag: -1
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary)
                        auto parallelism: true
            Execution mode: llap
            LLAP IO: all inputs
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: employeesalary=2000.0
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  partition values:
                    employeesalary 2000.0
                  properties:
                    column.name.delimiter ,
                    columns employeeid,employeename
                    columns.types int:string
                    field.delim |
#### A masked pattern was here ####
                    name default.employee_part
                    partition_columns employeesalary
                    partition_columns.types double
                    serialization.format |
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns employeeid,employeename
                      columns.comments 
                      columns.types int:string
                      field.delim |
#### A masked pattern was here ####
                      name default.employee_part
                      partition_columns employeesalary
                      partition_columns.types double
                      serialization.format |
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.employee_part
                  name: default.employee_part
            Truncated Path -> Alias:
              /employee_part/employeesalary=2000.0 [employee_part]
        Reducer 2 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4)
                keys: 2000.0D (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 2000.0D (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                  File Output Operator
                    bucketingVersion: 2
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          bucketing_version -1
                          columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                          columns.types string:bigint:bigint:bigint:bigint:binary:double
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: default.employee_part/
      Column Stats Desc:
          Columns: employeeID
          Column Types: int
          Table: default.employee_part
          Is Table Level Stats: false

PREHOOK: query: analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
PREHOOK: query: describe formatted Employee_Part partition(employeeSalary=2000.0)
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part partition(employeeSalary=2000.0)
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
# col_name            	data_type           	comment             
employeeid          	int                 	                    
employeename        	string              	                    
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
employeesalary      	double              	                    
	 	 
# Detailed Partition Information	 	 
Partition Value:    	[2000.0]            	 
Database:           	default             	 
Table:              	employee_part       	 
#### A masked pattern was here ####
Partition Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"employeeid\":\"true\"}}
	numFiles            	1                   
	numRows             	13                  
	rawDataSize         	92                  
	totalSize           	105                 
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	|                   
	serialization.format	|                   
PREHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  filterExpr: (employeesalary = 4000.0D) (type: boolean)
                  Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                  Select Operator
                    expressions: employeeid (type: int)
                    outputColumnNames: employeeid
                    Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid)
                      keys: 4000.0D (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                      Reduce Output Operator
                        key expressions: 4000.0D (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: 4000.0D (type: double)
                        Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary)
            Execution mode: llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4)
                keys: 4000.0D (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 4000.0D (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: employeeID
          Column Types: int
          Table: default.employee_part

PREHOOK: query: explain extended
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: explain extended
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  filterExpr: (employeesalary = 4000.0D) (type: boolean)
                  Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                  Statistics Aggregation Key Prefix: default.employee_part/
                  GatherStats: true
                  Select Operator
                    expressions: employeeid (type: int)
                    outputColumnNames: employeeid
                    Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid)
                      keys: 4000.0D (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                      Reduce Output Operator
                        bucketingVersion: 2
                        key expressions: 4000.0D (type: double)
                        null sort order: z
                        numBuckets: -1
                        sort order: +
                        Map-reduce partition columns: 4000.0D (type: double)
                        Statistics: Num rows: 3 Data size: 1062 Basic stats: PARTIAL Column stats: NONE
                        tag: -1
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary)
                        auto parallelism: true
            Execution mode: llap
            LLAP IO: all inputs
            Path -> Alias:
#### A masked pattern was here ####
            Path -> Partition:
#### A masked pattern was here ####
                Partition
                  base file name: employeesalary=4000.0
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  partition values:
                    employeesalary 4000.0
                  properties:
                    column.name.delimiter ,
                    columns employeeid,employeename
                    columns.types int:string
                    field.delim |
#### A masked pattern was here ####
                    name default.employee_part
                    partition_columns employeesalary
                    partition_columns.types double
                    serialization.format |
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      bucketing_version 2
                      column.name.delimiter ,
                      columns employeeid,employeename
                      columns.comments 
                      columns.types int:string
                      field.delim |
#### A masked pattern was here ####
                      name default.employee_part
                      partition_columns employeesalary
                      partition_columns.types double
                      serialization.format |
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    name: default.employee_part
                  name: default.employee_part
            Truncated Path -> Alias:
              /employee_part/employeesalary=4000.0 [employee_part]
        Reducer 2 
            Execution mode: vectorized, llap
            Needs Tagging: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4)
                keys: 4000.0D (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 4000.0D (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
                  File Output Operator
                    bucketingVersion: 2
                    compressed: false
                    GlobalTableId: 0
#### A masked pattern was here ####
                    NumFilesPerFileSink: 1
                    Statistics: Num rows: 1 Data size: 354 Basic stats: PARTIAL Column stats: NONE
#### A masked pattern was here ####
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        properties:
                          bucketing_version -1
                          columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                          columns.types string:bigint:bigint:bigint:bigint:binary:double
                          escape.delim \
                          hive.serialization.extend.additional.nesting.levels true
                          serialization.escape.crlf true
                          serialization.format 1
                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    TotalFiles: 1
                    GatherStats: false
                    MultiFileSpray: false

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
          Stats Aggregation Key Prefix: default.employee_part/
      Column Stats Desc:
          Columns: employeeID
          Column Types: int
          Table: default.employee_part
          Is Table Level Stats: false

PREHOOK: query: analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
PREHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: explain 
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  filterExpr: (employeesalary = 2000.0D) (type: boolean)
                  Statistics: Num rows: 13 Data size: 52 Basic stats: COMPLETE Column stats: PARTIAL
                  Select Operator
                    expressions: employeeid (type: int), employeename (type: string)
                    outputColumnNames: employeeid, employeename
                    Statistics: Num rows: 13 Data size: 52 Basic stats: COMPLETE Column stats: PARTIAL
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid), max(length(employeename)), avg(COALESCE(length(employeename),0)), count(employeename), compute_bit_vector_hll(employeename)
                      keys: 2000.0D (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      Statistics: Num rows: 1 Data size: 408 Basic stats: COMPLETE Column stats: PARTIAL
                      Reduce Output Operator
                        key expressions: 2000.0D (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: 2000.0D (type: double)
                        Statistics: Num rows: 1 Data size: 408 Basic stats: COMPLETE Column stats: PARTIAL
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary), _col6 (type: int), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: bigint), _col9 (type: binary)
            Execution mode: llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                keys: 2000.0D (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                Statistics: Num rows: 1 Data size: 340 Basic stats: COMPLETE Column stats: PARTIAL
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col6,0)) (type: bigint), COALESCE(_col7,0) (type: double), (_col3 - _col8) (type: bigint), COALESCE(ndv_compute_bit_vector(_col9),0) (type: bigint), _col9 (type: binary), 2000.0D (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                  Statistics: Num rows: 1 Data size: 538 Basic stats: COMPLETE Column stats: PARTIAL
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 538 Basic stats: COMPLETE Column stats: PARTIAL
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: employeeid, employeename
          Column Types: int, string
          Table: default.employee_part

PREHOOK: query: analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
PREHOOK: query: describe formatted Employee_Part partition (employeeSalary=2000.0) employeeID
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part partition (employeeSalary=2000.0) employeeID
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeID          
data_type           	int                 
min                 	16                  
max                 	34                  
num_nulls           	1                   
distinct_count      	12                  
avg_col_len         	                    
max_col_len         	                    
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
PREHOOK: query: describe formatted Employee_Part partition (employeeSalary=2000.0) employeeName
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part partition (employeeSalary=2000.0) employeeName
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeName        
data_type           	string              
min                 	                    
max                 	                    
num_nulls           	1                   
distinct_count      	12                  
avg_col_len         	4.3076923076923075  
max_col_len         	6                   
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
PREHOOK: query: explain 
analyze table Employee_Part  compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: explain 
analyze table Employee_Part  compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  Statistics: Num rows: 26 Data size: 2596 Basic stats: COMPLETE Column stats: PARTIAL
                  Select Operator
                    expressions: employeeid (type: int), employeename (type: string), employeesalary (type: double)
                    outputColumnNames: employeeid, employeename, employeesalary
                    Statistics: Num rows: 26 Data size: 2596 Basic stats: COMPLETE Column stats: PARTIAL
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid), max(length(employeename)), avg(COALESCE(length(employeename),0)), count(employeename), compute_bit_vector_hll(employeename)
                      keys: employeesalary (type: double)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                      Statistics: Num rows: 2 Data size: 816 Basic stats: COMPLETE Column stats: PARTIAL
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        null sort order: z
                        sort order: +
                        Map-reduce partition columns: _col0 (type: double)
                        Statistics: Num rows: 2 Data size: 816 Basic stats: COMPLETE Column stats: PARTIAL
                        value expressions: _col1 (type: int), _col2 (type: int), _col3 (type: bigint), _col4 (type: bigint), _col5 (type: binary), _col6 (type: int), _col7 (type: struct<count:bigint,sum:double,input:int>), _col8 (type: bigint), _col9 (type: binary)
            Execution mode: llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                keys: KEY._col0 (type: double)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                Statistics: Num rows: 2 Data size: 680 Basic stats: COMPLETE Column stats: PARTIAL
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col1) (type: bigint), UDFToLong(_col2) (type: bigint), (_col3 - _col4) (type: bigint), COALESCE(ndv_compute_bit_vector(_col5),0) (type: bigint), _col5 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col6,0)) (type: bigint), COALESCE(_col7,0) (type: double), (_col3 - _col8) (type: bigint), COALESCE(ndv_compute_bit_vector(_col9),0) (type: bigint), _col9 (type: binary), _col0 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12
                  Statistics: Num rows: 2 Data size: 1076 Basic stats: COMPLETE Column stats: PARTIAL
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 2 Data size: 1076 Basic stats: COMPLETE Column stats: PARTIAL
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: employeeid, employeename
          Column Types: int, string
          Table: default.employee_part

PREHOOK: query: analyze table Employee_Part  compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table Employee_Part  compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
PREHOOK: query: describe formatted Employee_Part partition(employeeSalary=2000.0) employeeID
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part partition(employeeSalary=2000.0) employeeID
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeID          
data_type           	int                 
min                 	16                  
max                 	34                  
num_nulls           	1                   
distinct_count      	12                  
avg_col_len         	                    
max_col_len         	                    
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
PREHOOK: query: describe formatted Employee_Part partition(employeeSalary=4000.0) employeeID
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part partition(employeeSalary=4000.0) employeeID
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeID          
data_type           	int                 
min                 	16                  
max                 	34                  
num_nulls           	1                   
distinct_count      	12                  
avg_col_len         	                    
max_col_len         	                    
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
PREHOOK: query: explain 
analyze table Employee_Part  compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: explain 
analyze table Employee_Part  compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-2 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-0
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: employee_part
                  Statistics: Num rows: 26 Data size: 2300 Basic stats: COMPLETE Column stats: PARTIAL
                  Select Operator
                    expressions: employeeid (type: int), employeename (type: string)
                    outputColumnNames: employeeid, employeename
                    Statistics: Num rows: 26 Data size: 2300 Basic stats: COMPLETE Column stats: PARTIAL
                    Group By Operator
                      aggregations: min(employeeid), max(employeeid), count(1), count(employeeid), compute_bit_vector_hll(employeeid), max(length(employeename)), avg(COALESCE(length(employeename),0)), count(employeename), compute_bit_vector_hll(employeename)
                      minReductionHashAggr: 0.99
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                      Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: PARTIAL
                      Reduce Output Operator
                        null sort order: 
                        sort order: 
                        Statistics: Num rows: 1 Data size: 400 Basic stats: COMPLETE Column stats: PARTIAL
                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: bigint), _col3 (type: bigint), _col4 (type: binary), _col5 (type: int), _col6 (type: struct<count:bigint,sum:double,input:int>), _col7 (type: bigint), _col8 (type: binary)
            Execution mode: llap
            LLAP IO: all inputs
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), count(VALUE._col2), count(VALUE._col3), compute_bit_vector_hll(VALUE._col4), max(VALUE._col5), avg(VALUE._col6), count(VALUE._col7), compute_bit_vector_hll(VALUE._col8)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
                Statistics: Num rows: 1 Data size: 332 Basic stats: COMPLETE Column stats: PARTIAL
                Select Operator
                  expressions: 'LONG' (type: string), UDFToLong(_col0) (type: bigint), UDFToLong(_col1) (type: bigint), (_col2 - _col3) (type: bigint), COALESCE(ndv_compute_bit_vector(_col4),0) (type: bigint), _col4 (type: binary), 'STRING' (type: string), UDFToLong(COALESCE(_col5,0)) (type: bigint), COALESCE(_col6,0) (type: double), (_col2 - _col7) (type: bigint), COALESCE(ndv_compute_bit_vector(_col8),0) (type: bigint), _col8 (type: binary)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                  Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: PARTIAL
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 530 Basic stats: COMPLETE Column stats: PARTIAL
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-2
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: employeeid, employeename
          Column Types: int, string
          Table: default.employee_part

PREHOOK: query: analyze table Employee_Part  compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table Employee_Part  compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
PREHOOK: query: describe formatted Employee_Part employeeID
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted Employee_Part employeeID
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeID          
data_type           	int                 
min                 	16                  
max                 	34                  
num_nulls           	2                   
distinct_count      	12                  
avg_col_len         	                    
max_col_len         	                    
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
COLUMN_STATS_ACCURATE	{\"COLUMN_STATS\":{\"employeeid\":\"true\",\"employeename\":\"true\"}}
PREHOOK: query: create database if not exists dummydb
PREHOOK: type: CREATEDATABASE
PREHOOK: Output: database:dummydb
POSTHOOK: query: create database if not exists dummydb
POSTHOOK: type: CREATEDATABASE
POSTHOOK: Output: database:dummydb
PREHOOK: query: use dummydb
PREHOOK: type: SWITCHDATABASE
PREHOOK: Input: database:dummydb
POSTHOOK: query: use dummydb
POSTHOOK: type: SWITCHDATABASE
POSTHOOK: Input: database:dummydb
PREHOOK: query: analyze table default.Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table default.Employee_Part partition (employeeSalary=2000.0) compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
#### A masked pattern was here ####
PREHOOK: query: describe formatted default.Employee_Part partition (employeeSalary=2000.0) employeeID
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@employee_part
POSTHOOK: query: describe formatted default.Employee_Part partition (employeeSalary=2000.0) employeeID
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@employee_part
col_name            	employeeID          
data_type           	int                 
min                 	16                  
max                 	34                  
num_nulls           	1                   
distinct_count      	12                  
avg_col_len         	                    
max_col_len         	                    
num_trues           	                    
num_falses          	                    
bit_vector          	HL                  
comment             	from deserializer   
COLUMN_STATS_ACCURATE	{\"COLUMN_STATS\":{\"employeeid\":\"true\",\"employeename\":\"true\"}}
PREHOOK: query: analyze table default.Employee_Part  compute statistics for columns
PREHOOK: type: ANALYZE_TABLE
PREHOOK: Input: default@employee_part
PREHOOK: Input: default@employee_part@employeesalary=2000.0
PREHOOK: Input: default@employee_part@employeesalary=4000.0
PREHOOK: Output: default@employee_part
PREHOOK: Output: default@employee_part@employeesalary=2000.0
PREHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
POSTHOOK: query: analyze table default.Employee_Part  compute statistics for columns
POSTHOOK: type: ANALYZE_TABLE
POSTHOOK: Input: default@employee_part
POSTHOOK: Input: default@employee_part@employeesalary=2000.0
POSTHOOK: Input: default@employee_part@employeesalary=4000.0
POSTHOOK: Output: default@employee_part
POSTHOOK: Output: default@employee_part@employeesalary=2000.0
POSTHOOK: Output: default@employee_part@employeesalary=4000.0
#### A masked pattern was here ####
PREHOOK: query: use default
PREHOOK: type: SWITCHDATABASE
PREHOOK: Input: database:default
POSTHOOK: query: use default
POSTHOOK: type: SWITCHDATABASE
POSTHOOK: Input: database:default
PREHOOK: query: drop database dummydb
PREHOOK: type: DROPDATABASE
PREHOOK: Input: database:dummydb
PREHOOK: Output: database:dummydb
POSTHOOK: query: drop database dummydb
POSTHOOK: type: DROPDATABASE
POSTHOOK: Input: database:dummydb
POSTHOOK: Output: database:dummydb
