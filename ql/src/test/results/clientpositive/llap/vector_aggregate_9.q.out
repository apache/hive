PREHOOK: query: create table vectortab2k_n4(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@vectortab2k_n4
POSTHOOK: query: create table vectortab2k_n4(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@vectortab2k_n4
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k_n4
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@vectortab2k_n4
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k_n4
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@vectortab2k_n4
PREHOOK: query: create table vectortab2korc_n4(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@vectortab2korc_n4
POSTHOOK: query: create table vectortab2korc_n4(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@vectortab2korc_n4
PREHOOK: query: INSERT INTO TABLE vectortab2korc_n4 SELECT * FROM vectortab2k_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2k_n4
PREHOOK: Output: default@vectortab2korc_n4
POSTHOOK: query: INSERT INTO TABLE vectortab2korc_n4 SELECT * FROM vectortab2k_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2k_n4
POSTHOOK: Output: default@vectortab2korc_n4
POSTHOOK: Lineage: vectortab2korc_n4.b SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:b, type:bigint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.bo SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:bo, type:boolean, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.d SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:d, type:double, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.dc SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:dc, type:decimal(38,18), comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.dt SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:dt, type:date, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.f SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:f, type:float, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.i SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:i, type:int, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.s SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:s, type:string, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.s2 SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:s2, type:string, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.si SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:si, type:smallint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.t SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:t, type:tinyint, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.ts SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:ts, type:timestamp, comment:null), ]
POSTHOOK: Lineage: vectortab2korc_n4.ts2 SIMPLE [(vectortab2k_n4)vectortab2k_n4.FieldSchema(name:ts2, type:timestamp, comment:null), ]
PREHOOK: query: explain vectorization detail
select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc_n4
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc_n4
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: vectortab2korc_n4
                  Statistics: Num rows: 2000 Data size: 212912 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:t:tinyint, 1:si:smallint, 2:i:int, 3:b:bigint, 4:f:float, 5:d:double, 6:dc:decimal(38,18), 7:bo:boolean, 8:s:string, 9:s2:string, 10:ts:timestamp, 11:ts2:timestamp, 12:dt:date, 13:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: dc (type: decimal(38,18))
                    outputColumnNames: dc
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [6]
                    Statistics: Num rows: 2000 Data size: 212912 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(dc), max(dc), sum(dc), count(dc)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDecimal(col 6:decimal(38,18)) -> decimal(38,18), VectorUDAFMaxDecimal(col 6:decimal(38,18)) -> decimal(38,18), VectorUDAFSumDecimal(col 6:decimal(38,18)) -> decimal(38,18), VectorUDAFCount(col 6:decimal(38,18)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2, 3]
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 456 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0, 1, 2, 3]
                        Statistics: Num rows: 1 Data size: 456 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(38,18)), _col1 (type: decimal(38,18)), _col2 (type: decimal(38,18)), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 13
                    includeColumns: [6]
                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: VALUE._col0:decimal(38,18), VALUE._col1:decimal(38,18), VALUE._col2:decimal(38,18), VALUE._col3:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), count(VALUE._col3)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 0:decimal(38,18)) -> decimal(38,18), VectorUDAFMaxDecimal(col 1:decimal(38,18)) -> decimal(38,18), VectorUDAFSumDecimal(col 2:decimal(38,18)) -> decimal(38,18), VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0, 1, 2, 3]
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 456 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: decimal(38,18)), _col1 (type: decimal(38,18)), _col2 (type: decimal(38,18)), (_col2 / _col3) (type: decimal(38,18))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 1, 2, 5]
                      selectExpressions: DecimalColDivideDecimalColumn(col 2:decimal(38,18), col 4:decimal(19,0))(children: CastLongToDecimal(col 3:bigint) -> 4:decimal(19,0)) -> 5:decimal(38,18)
                  Statistics: Num rows: 1 Data size: 456 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 1 Data size: 456 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
POSTHOOK: query: select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
-4997414117561.546875000000000000	4994550248722.298828000000000000	-10252745435816.024410000000000000	-5399023399.587163986308583465
PREHOOK: query: explain vectorization detail
select min(d), max(d), sum(d), avg(d) from vectortab2korc_n4
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(d), max(d), sum(d), avg(d) from vectortab2korc_n4
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: vectortab2korc_n4
                  Statistics: Num rows: 2000 Data size: 15208 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:t:tinyint, 1:si:smallint, 2:i:int, 3:b:bigint, 4:f:float, 5:d:double, 6:dc:decimal(38,18), 7:bo:boolean, 8:s:string, 9:s2:string, 10:ts:timestamp, 11:ts2:timestamp, 12:dt:date, 13:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: d (type: double)
                    outputColumnNames: d
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [5]
                    Statistics: Num rows: 2000 Data size: 15208 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(d), max(d), sum(d), count(d)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDouble(col 5:double) -> double, VectorUDAFMaxDouble(col 5:double) -> double, VectorUDAFSumDouble(col 5:double) -> double, VectorUDAFCount(col 5:double) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2, 3]
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0, 1, 2, 3]
                        Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: double), _col1 (type: double), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 13
                    includeColumns: [5]
                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: VALUE._col0:double, VALUE._col1:double, VALUE._col2:double, VALUE._col3:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), count(VALUE._col3)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDouble(col 0:double) -> double, VectorUDAFMaxDouble(col 1:double) -> double, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0, 1, 2, 3]
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: double), _col1 (type: double), _col2 (type: double), (_col2 / _col3) (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 1, 2, 4]
                      selectExpressions: DoubleColDivideLongColumn(col 2:double, col 3:bigint) -> 4:double
                  Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 1 Data size: 40 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
POSTHOOK: query: select min(d), max(d), sum(d), avg(d) from vectortab2korc_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
-4999829.07	4997627.14	-1.7516847286999977E8	-92193.93308947356
PREHOOK: query: explain vectorization detail
select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc_n4
PREHOOK: type: QUERY
POSTHOOK: query: explain vectorization detail
select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc_n4
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: vectortab2korc_n4
                  Statistics: Num rows: 2000 Data size: 76040 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:t:tinyint, 1:si:smallint, 2:i:int, 3:b:bigint, 4:f:float, 5:d:double, 6:dc:decimal(38,18), 7:bo:boolean, 8:s:string, 9:s2:string, 10:ts:timestamp, 11:ts2:timestamp, 12:dt:date, 13:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: ts (type: timestamp)
                    outputColumnNames: ts
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [10]
                    Statistics: Num rows: 2000 Data size: 76040 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(ts), max(ts), sum(ts), count(ts)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinTimestamp(col 10:timestamp) -> timestamp, VectorUDAFMaxTimestamp(col 10:timestamp) -> timestamp, VectorUDAFSumTimestamp(col 10:timestamp) -> double, VectorUDAFCount(col 10:timestamp) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2, 3]
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0, 1, 2, 3]
                        Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: timestamp), _col1 (type: timestamp), _col2 (type: double), _col3 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 13
                    includeColumns: [10]
                    dataColumns: t:tinyint, si:smallint, i:int, b:bigint, f:float, d:double, dc:decimal(38,18), bo:boolean, s:string, s2:string, ts:timestamp, ts2:timestamp, dt:date
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: VALUE._col0:timestamp, VALUE._col1:timestamp, VALUE._col2:double, VALUE._col3:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), sum(VALUE._col2), count(VALUE._col3)
                Group By Vectorization:
                    aggregators: VectorUDAFMinTimestamp(col 0:timestamp) -> timestamp, VectorUDAFMaxTimestamp(col 1:timestamp) -> timestamp, VectorUDAFSumDouble(col 2:double) -> double, VectorUDAFCountMerge(col 3:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0, 1, 2, 3]
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: timestamp), _col1 (type: timestamp), _col2 (type: double), (_col2 / _col3) (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 1, 2, 4]
                      selectExpressions: DoubleColDivideLongColumn(col 2:double, col 3:bigint) -> 4:double
                  Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    File Sink Vectorization:
                        className: VectorFileSinkOperator
                        native: false
                    Statistics: Num rows: 1 Data size: 136 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc_n4
PREHOOK: type: QUERY
PREHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
POSTHOOK: query: select min(ts), max(ts), sum(ts), avg(ts) from vectortab2korc_n4
POSTHOOK: type: QUERY
POSTHOOK: Input: default@vectortab2korc_n4
#### A masked pattern was here ####
2013-02-18 21:06:48	2081-02-22 01:21:53	4.591334884281E12	2.4254278311045957E9
