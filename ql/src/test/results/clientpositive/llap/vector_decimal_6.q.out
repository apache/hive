PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_1_txt
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_1_txt
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_1
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_1
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_2_txt
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_2_txt
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_2
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_2
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_3_txt
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_3_txt
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_3
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_6_3
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE DECIMAL_6_1_txt(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_6_1_txt
POSTHOOK: query: CREATE TABLE DECIMAL_6_1_txt(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_6_1_txt
PREHOOK: query: CREATE TABLE DECIMAL_6_2_txt(key decimal(17,4), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_6_2_txt
POSTHOOK: query: CREATE TABLE DECIMAL_6_2_txt(key decimal(17,4), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_6_2_txt
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_1_txt
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_6_1_txt
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_1_txt
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_6_1_txt
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_2_txt
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_6_2_txt
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_2_txt
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_6_2_txt
PREHOOK: query: CREATE TABLE DECIMAL_6_1(key decimal(10,5), value int)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_6_1
POSTHOOK: query: CREATE TABLE DECIMAL_6_1(key decimal(10,5), value int)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_6_1
PREHOOK: query: CREATE TABLE DECIMAL_6_2(key decimal(17,4), value int)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_6_2
POSTHOOK: query: CREATE TABLE DECIMAL_6_2(key decimal(17,4), value int)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_6_2
PREHOOK: query: INSERT OVERWRITE TABLE DECIMAL_6_1 SELECT * FROM DECIMAL_6_1_txt
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_1_txt
PREHOOK: Output: default@decimal_6_1
POSTHOOK: query: INSERT OVERWRITE TABLE DECIMAL_6_1 SELECT * FROM DECIMAL_6_1_txt
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_1_txt
POSTHOOK: Output: default@decimal_6_1
POSTHOOK: Lineage: decimal_6_1.key SIMPLE [(decimal_6_1_txt)decimal_6_1_txt.FieldSchema(name:key, type:decimal(10,5), comment:null), ]
POSTHOOK: Lineage: decimal_6_1.value SIMPLE [(decimal_6_1_txt)decimal_6_1_txt.FieldSchema(name:value, type:int, comment:null), ]
PREHOOK: query: INSERT OVERWRITE TABLE DECIMAL_6_2 SELECT * FROM DECIMAL_6_2_txt
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_2_txt
PREHOOK: Output: default@decimal_6_2
POSTHOOK: query: INSERT OVERWRITE TABLE DECIMAL_6_2 SELECT * FROM DECIMAL_6_2_txt
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_2_txt
POSTHOOK: Output: default@decimal_6_2
POSTHOOK: Lineage: decimal_6_2.key SIMPLE [(decimal_6_2_txt)decimal_6_2_txt.FieldSchema(name:key, type:decimal(17,4), comment:null), ]
POSTHOOK: Lineage: decimal_6_2.value SIMPLE [(decimal_6_2_txt)decimal_6_2_txt.FieldSchema(name:value, type:int, comment:null), ]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT * FROM DECIMAL_6_1 ORDER BY key, value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT * FROM DECIMAL_6_1 ORDER BY key, value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_6_1
                  Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(10,5)/DECIMAL_64, 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(10,5)), value (type: int)
                    outputColumnNames: _col0, _col1
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: decimal(10,5)), _col1 (type: int)
                      sort order: ++
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkObjectHashOperator
                          keyColumnNums: [0, 1]
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumnNums: []
                      Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(10,5)/DECIMAL_64, value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: aa
                reduceColumnSortOrder: ++
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    dataColumns: KEY.reducesinkkey0:decimal(10,5), KEY.reducesinkkey1:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: decimal(10,5)), KEY.reducesinkkey1 (type: int)
                outputColumnNames: _col0, _col1
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1]
                Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM DECIMAL_6_1 ORDER BY key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM DECIMAL_6_1 ORDER BY key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_1
#### A masked pattern was here ####
NULL	-1234567890
NULL	0
NULL	3
NULL	4
NULL	1234567890
-4400.00000	4400
-1255.49000	-1255
-1.12200	-11
-1.12000	-1
-0.33300	0
-0.30000	0
0.00000	0
0.00000	0
0.33300	0
1.00000	1
1.00000	1
1.12000	1
1.12200	1
2.00000	2
3.14000	3
3.14000	3
3.14000	4
10.00000	10
10.73433	5
124.00000	124
125.20000	125
23232.23435	2
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT * FROM DECIMAL_6_2 ORDER BY key, value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT * FROM DECIMAL_6_2 ORDER BY key, value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_6_2
                  Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(17,4)/DECIMAL_64, 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(17,4)), value (type: int)
                    outputColumnNames: _col0, _col1
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: decimal(17,4)), _col1 (type: int)
                      sort order: ++
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkObjectHashOperator
                          keyColumnNums: [0, 1]
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumnNums: []
                      Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(17,4)/DECIMAL_64, value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: aa
                reduceColumnSortOrder: ++
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    dataColumns: KEY.reducesinkkey0:decimal(17,4), KEY.reducesinkkey1:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: decimal(17,4)), KEY.reducesinkkey1 (type: int)
                outputColumnNames: _col0, _col1
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1]
                Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM DECIMAL_6_2 ORDER BY key, value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_2
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM DECIMAL_6_2 ORDER BY key, value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_2
#### A masked pattern was here ####
NULL	0
-1234567890.1235	-1234567890
-4400.0000	4400
-1255.4900	-1255
-1.1220	-11
-1.1200	-1
-0.3330	0
-0.3000	0
0.0000	0
0.0000	0
0.3330	0
1.0000	1
1.0000	1
1.1200	1
1.1220	1
2.0000	2
3.1400	3
3.1400	3
3.1400	4
10.0000	10
10.7343	5
124.0000	124
125.2000	125
23232.2344	2
2389432.2375	3
2389432.2375	4
1234567890.1235	1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Map 1 <- Union 2 (CONTAINS)
        Map 4 <- Union 2 (CONTAINS)
        Reducer 3 <- Union 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_6_1
                  Statistics: Num rows: 27 Data size: 2576 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(10,5)/DECIMAL_64, 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: CAST( key AS decimal(18,5)) (type: decimal(18,5))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: ConvertDecimal64ToDecimal(col 0:decimal(18,5)/DECIMAL_64) -> 3:decimal(18,5)
                    Statistics: Num rows: 27 Data size: 2576 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: decimal(18,5))
                      sort order: +
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkObjectHashOperator
                          keyColumnNums: [3]
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumnNums: []
                      Statistics: Num rows: 54 Data size: 5600 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(10,5)/DECIMAL_64, value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(18,5)]
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: decimal_6_2
                  Statistics: Num rows: 27 Data size: 3024 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(17,4)/DECIMAL_64, 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: CAST( key AS decimal(18,5)) (type: decimal(18,5))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: ConvertDecimal64ToDecimal(col 0:decimal(18,5)/DECIMAL_64) -> 3:decimal(18,5)
                    Statistics: Num rows: 27 Data size: 3024 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col0 (type: decimal(18,5))
                      sort order: +
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkObjectHashOperator
                          keyColumnNums: [3]
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumnNums: []
                      Statistics: Num rows: 54 Data size: 5600 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(17,4)/DECIMAL_64, value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(18,5)]
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: KEY.reducesinkkey0:decimal(18,5)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: decimal(18,5))
                outputColumnNames: _col0
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0]
                Statistics: Num rows: 54 Data size: 4928 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 54 Data size: 4928 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Union 2 
            Vertex: Union 2

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_1
PREHOOK: Input: default@decimal_6_2
#### A masked pattern was here ####
POSTHOOK: query: SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_1
POSTHOOK: Input: default@decimal_6_2
#### A masked pattern was here ####
NULL
NULL
NULL
NULL
NULL
NULL
-123456789.01235
-4400.00000
-1255.49000
-440.00000
-125.54900
-1.12200
-1.12000
-0.33300
-0.30000
-0.11220
-0.11200
-0.03330
-0.03000
0.00000
0.00000
0.00000
0.00000
0.03330
0.10000
0.10000
0.11200
0.11220
0.20000
0.31400
0.31400
0.31400
0.33300
1.00000
1.00000
1.00000
1.07343
1.12000
1.12200
2.00000
3.14000
3.14000
3.14000
10.00000
10.73433
12.40000
12.52000
124.00000
125.20000
2323.22344
23232.23435
238943.22375
238943.22375
123456789.01235
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
CREATE TABLE DECIMAL_6_3 STORED AS ORC AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v
PREHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
CREATE TABLE DECIMAL_6_3 STORED AS ORC AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v
POSTHOOK: type: CREATETABLE_AS_SELECT
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-4 depends on stages: Stage-2, Stage-0
  Stage-3 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_6_1
                  Statistics: Num rows: 27 Data size: 2684 Basic stats: COMPLETE Column stats: COMPLETE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(10,5)/DECIMAL_64, 1:value:int, 2:ROW__ID:struct<writeid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + 5.5) (type: decimal(11,5)), (value * 11) (type: int)
                    outputColumnNames: _col0, _col1
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3, 4]
                        selectExpressions: Decimal64ColAddDecimal64Scalar(col 0:decimal(10,5)/DECIMAL_64, decimal64Val 550000, decimalVal 5.5) -> 3:decimal(11,5)/DECIMAL_64, LongColMultiplyLongScalar(col 1:int, val 11) -> 4:int
                    Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Reduce Sink Vectorization:
                          className: VectorReduceSinkObjectHashOperator
                          keyColumnNums: [4]
                          native: true
                          nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                          valueColumnNums: [3]
                      Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                      value expressions: _col0 (type: decimal(11,5))
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                featureSupportInUse: [DECIMAL_64]
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: true
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(10,5)/DECIMAL_64, value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(11,5)/DECIMAL_64, bigint]
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    dataColumns: KEY.reducesinkkey0:int, VALUE._col0:decimal(11,5)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: decimal(11,5)), KEY.reducesinkkey0 (type: int)
                outputColumnNames: _col0, _col1
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [1, 0]
                Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 27 Data size: 3132 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                      name: default.DECIMAL_6_3

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-4
      Create Table Operator:
        Create Table
          columns: k decimal(11,5), v int
          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
          serde name: org.apache.hadoop.hive.ql.io.orc.OrcSerde
          name: default.DECIMAL_6_3

  Stage: Stage-3
    Stats Work
      Basic Stats Work:

  Stage: Stage-0
    Move Operator
      files:
          hdfs directory: true
#### A masked pattern was here ####

PREHOOK: query: CREATE TABLE DECIMAL_6_3 STORED AS ORC AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v
PREHOOK: type: CREATETABLE_AS_SELECT
PREHOOK: Input: default@decimal_6_1
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_6_3
POSTHOOK: query: CREATE TABLE DECIMAL_6_3 STORED AS ORC AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v
POSTHOOK: type: CREATETABLE_AS_SELECT
POSTHOOK: Input: default@decimal_6_1
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_6_3
POSTHOOK: Lineage: decimal_6_3.k EXPRESSION [(decimal_6_1)decimal_6_1.FieldSchema(name:key, type:decimal(10,5), comment:null), ]
POSTHOOK: Lineage: decimal_6_3.v EXPRESSION [(decimal_6_1)decimal_6_1.FieldSchema(name:value, type:int, comment:null), ]
PREHOOK: query: desc DECIMAL_6_3
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@decimal_6_3
POSTHOOK: query: desc DECIMAL_6_3
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@decimal_6_3
k                   	decimal(11,5)       	                    
v                   	int                 	                    
PREHOOK: query: SELECT * FROM DECIMAL_6_3 ORDER BY k, v
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_6_3
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM DECIMAL_6_3 ORDER BY k, v
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_6_3
#### A masked pattern was here ####
NULL	-695344902
NULL	0
NULL	33
NULL	44
NULL	695344902
-4394.50000	48400
-1249.99000	-13805
4.37800	-121
4.38000	-11
5.16700	0
5.20000	0
5.50000	0
5.50000	0
5.83300	0
6.50000	11
6.50000	11
6.62000	11
6.62200	11
7.50000	22
8.64000	33
8.64000	33
8.64000	44
15.50000	110
16.23433	55
129.50000	1364
130.70000	1375
23237.73435	22
