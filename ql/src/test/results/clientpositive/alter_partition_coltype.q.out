PREHOOK: query: -- create testing table.
create table alter_coltype(key string, value string) partitioned by (dt string, ts string)
PREHOOK: type: CREATETABLE
POSTHOOK: query: -- create testing table.
create table alter_coltype(key string, value string) partitioned by (dt string, ts string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@alter_coltype
PREHOOK: query: -- insert and create a partition.
insert overwrite table alter_coltype partition(dt='100x', ts='6:30pm') select * from src1
PREHOOK: type: QUERY
PREHOOK: Input: default@src1
PREHOOK: Output: default@alter_coltype@dt=100x/ts=6%3A30pm
POSTHOOK: query: -- insert and create a partition.
insert overwrite table alter_coltype partition(dt='100x', ts='6:30pm') select * from src1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src1
POSTHOOK: Output: default@alter_coltype@dt=100x/ts=6%3A30pm
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: desc alter_coltype
PREHOOK: type: DESCTABLE
POSTHOOK: query: desc alter_coltype
POSTHOOK: type: DESCTABLE
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
key                 	string              	None                
value               	string              	None                
dt                  	string              	None                
ts                  	string              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	None                
ts                  	string              	None                
PREHOOK: query: -- select with paritition predicate.
select count(*) from alter_coltype where dt = '100x'
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: -- select with paritition predicate.
select count(*) from alter_coltype where dt = '100x'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
25
PREHOOK: query: -- alter partition key column data type for dt column.
alter table alter_coltype partition column (dt int)
PREHOOK: type: null
PREHOOK: Input: default@alter_coltype
POSTHOOK: query: -- alter partition key column data type for dt column.
alter table alter_coltype partition column (dt int)
POSTHOOK: type: null
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Output: default@alter_coltype
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: -- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt=10, ts='3.0') select * from src1
PREHOOK: type: QUERY
PREHOOK: Input: default@src1
PREHOOK: Output: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: query: -- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt=10, ts='3.0') select * from src1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src1
POSTHOOK: Output: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: -- make sure the partition predicate still works. 
select count(*) from alter_coltype where dt = '100x'
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: -- make sure the partition predicate still works. 
select count(*) from alter_coltype where dt = '100x'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
25
PREHOOK: query: explain extended select count(*) from alter_coltype where dt = '100x'
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select count(*) from alter_coltype where dt = '100x'
POSTHOOK: type: QUERY
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME alter_coltype))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (= (TOK_TABLE_OR_COL dt) '100x'))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alter_coltype 
          TableScan
            alias: alter_coltype
            Statistics:
                numRows: 25 dataSize: 191 basicStatsState: COMPLETE colStatsState: COMPLETE
            GatherStats: false
            Select Operator
              Statistics:
                  numRows: 25 dataSize: 191 basicStatsState: COMPLETE colStatsState: COMPLETE
              Group By Operator
                aggregations:
                      expr: count()
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0
                Statistics:
                    numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                Reduce Output Operator
                  sort order: 
                  Statistics:
                      numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: bigint
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ts=6%3A30pm
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 6:30pm
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
      Truncated Path -> Alias:
        /alter_coltype/dt=100x/ts=6%3A30pm [alter_coltype]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(VALUE._col0)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0
          Statistics:
              numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
          Select Operator
            expressions:
                  expr: _col0
                  type: bigint
            outputColumnNames: _col0
            Statistics:
                numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics:
                  numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0
                    columns.types bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1

PREHOOK: query: select count(*) from alter_coltype where dt = '100'
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from alter_coltype where dt = '100'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
0
PREHOOK: query: -- alter partition key column data type for ts column.
alter table alter_coltype partition column (ts double)
PREHOOK: type: null
PREHOOK: Input: default@alter_coltype
POSTHOOK: query: -- alter partition key column data type for ts column.
alter table alter_coltype partition column (ts double)
POSTHOOK: type: null
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Output: default@alter_coltype
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: alter table alter_coltype partition column (dt string)
PREHOOK: type: null
PREHOOK: Input: default@alter_coltype
POSTHOOK: query: alter table alter_coltype partition column (dt string)
POSTHOOK: type: null
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Output: default@alter_coltype
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: -- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt='100x', ts=3.0) select * from src1
PREHOOK: type: QUERY
PREHOOK: Input: default@src1
PREHOOK: Output: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: query: -- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt='100x', ts=3.0) select * from src1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src1
POSTHOOK: Output: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: --  validate partition key column predicate can still work.
select count(*) from alter_coltype where ts = '6:30pm'
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: --  validate partition key column predicate can still work.
select count(*) from alter_coltype where ts = '6:30pm'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
25
PREHOOK: query: explain extended select count(*) from alter_coltype where ts = '6:30pm'
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select count(*) from alter_coltype where ts = '6:30pm'
POSTHOOK: type: QUERY
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME alter_coltype))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (= (TOK_TABLE_OR_COL ts) '6:30pm'))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alter_coltype 
          TableScan
            alias: alter_coltype
            Statistics:
                numRows: 25 dataSize: 191 basicStatsState: COMPLETE colStatsState: COMPLETE
            GatherStats: false
            Select Operator
              Statistics:
                  numRows: 25 dataSize: 191 basicStatsState: COMPLETE colStatsState: COMPLETE
              Group By Operator
                aggregations:
                      expr: count()
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0
                Statistics:
                    numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                Reduce Output Operator
                  sort order: 
                  Statistics:
                      numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: bigint
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ts=6%3A30pm
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 6:30pm
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
      Truncated Path -> Alias:
        /alter_coltype/dt=100x/ts=6%3A30pm [alter_coltype]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(VALUE._col0)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0
          Statistics:
              numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
          Select Operator
            expressions:
                  expr: _col0
                  type: bigint
            outputColumnNames: _col0
            Statistics:
                numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics:
                  numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0
                    columns.types bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1

PREHOOK: query: --  validate partition key column predicate on two different partition column data type 
--  can still work.
select count(*) from alter_coltype where ts = 3.0 and dt=10
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=10/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: --  validate partition key column predicate on two different partition column data type 
--  can still work.
select count(*) from alter_coltype where ts = 3.0 and dt=10
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
25
PREHOOK: query: explain extended select count(*) from alter_coltype where ts = 3.0 and dt=10
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select count(*) from alter_coltype where ts = 3.0 and dt=10
POSTHOOK: type: QUERY
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME alter_coltype))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ts) 3.0) (= (TOK_TABLE_OR_COL dt) 10)))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alter_coltype 
          TableScan
            alias: alter_coltype
            Statistics:
                numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: COMPLETE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: ((ts = 3.0) and (dt = 10))
                  type: boolean
              Statistics:
                  numRows: 75 dataSize: 0 basicStatsState: PARTIAL colStatsState: COMPLETE
              Select Operator
                Statistics:
                    numRows: 75 dataSize: 0 basicStatsState: PARTIAL colStatsState: COMPLETE
                Group By Operator
                  aggregations:
                        expr: count()
                  bucketGroup: false
                  mode: hash
                  outputColumnNames: _col0
                  Statistics:
                      numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                  Reduce Output Operator
                    sort order: 
                    Statistics:
                        numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                    tag: -1
                    value expressions:
                          expr: _col0
                          type: bigint
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 10
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=6%3A30pm
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 6:30pm
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
      Truncated Path -> Alias:
        /alter_coltype/dt=10/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=6%3A30pm [alter_coltype]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(VALUE._col0)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0
          Statistics:
              numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
          Select Operator
            expressions:
                  expr: _col0
                  type: bigint
            outputColumnNames: _col0
            Statistics:
                numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics:
                  numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0
                    columns.types bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1

PREHOOK: query: -- query where multiple partition values (of different datatypes) are being selected 
select key, value, dt, ts from alter_coltype where dt is not null
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=10/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: -- query where multiple partition values (of different datatypes) are being selected 
select key, value, dt, ts from alter_coltype where dt is not null
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
238	val_238	10	3.0
		10	3.0
311	val_311	10	3.0
	val_27	10	3.0
	val_165	10	3.0
	val_409	10	3.0
255	val_255	10	3.0
278	val_278	10	3.0
98	val_98	10	3.0
	val_484	10	3.0
	val_265	10	3.0
	val_193	10	3.0
401	val_401	10	3.0
150	val_150	10	3.0
273	val_273	10	3.0
224		10	3.0
369		10	3.0
66	val_66	10	3.0
128		10	3.0
213	val_213	10	3.0
146	val_146	10	3.0
406	val_406	10	3.0
		10	3.0
		10	3.0
		10	3.0
238	val_238	100x	3.0
		100x	3.0
311	val_311	100x	3.0
	val_27	100x	3.0
	val_165	100x	3.0
	val_409	100x	3.0
255	val_255	100x	3.0
278	val_278	100x	3.0
98	val_98	100x	3.0
	val_484	100x	3.0
	val_265	100x	3.0
	val_193	100x	3.0
401	val_401	100x	3.0
150	val_150	100x	3.0
273	val_273	100x	3.0
224		100x	3.0
369		100x	3.0
66	val_66	100x	3.0
128		100x	3.0
213	val_213	100x	3.0
146	val_146	100x	3.0
406	val_406	100x	3.0
		100x	3.0
		100x	3.0
		100x	3.0
238	val_238	100x	6:30pm
		100x	6:30pm
311	val_311	100x	6:30pm
	val_27	100x	6:30pm
	val_165	100x	6:30pm
	val_409	100x	6:30pm
255	val_255	100x	6:30pm
278	val_278	100x	6:30pm
98	val_98	100x	6:30pm
	val_484	100x	6:30pm
	val_265	100x	6:30pm
	val_193	100x	6:30pm
401	val_401	100x	6:30pm
150	val_150	100x	6:30pm
273	val_273	100x	6:30pm
224		100x	6:30pm
369		100x	6:30pm
66	val_66	100x	6:30pm
128		100x	6:30pm
213	val_213	100x	6:30pm
146	val_146	100x	6:30pm
406	val_406	100x	6:30pm
		100x	6:30pm
		100x	6:30pm
		100x	6:30pm
PREHOOK: query: explain extended select key, value, dt, ts from alter_coltype where dt is not null
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select key, value, dt, ts from alter_coltype where dt is not null
POSTHOOK: type: QUERY
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME alter_coltype))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL dt)) (TOK_SELEXPR (TOK_TABLE_OR_COL ts))) (TOK_WHERE (TOK_FUNCTION TOK_ISNOTNULL (TOK_TABLE_OR_COL dt)))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alter_coltype 
          TableScan
            alias: alter_coltype
            Statistics:
                numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: NONE
            GatherStats: false
            Select Operator
              expressions:
                    expr: key
                    type: string
                    expr: value
                    type: string
                    expr: dt
                    type: string
                    expr: ts
                    type: string
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics:
                  numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: NONE
              File Output Operator
                compressed: false
                GlobalTableId: 0
#### A masked pattern was here ####
                NumFilesPerFileSink: 1
                Statistics:
                    numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: NONE
#### A masked pattern was here ####
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                    properties:
                      columns _col0,_col1,_col2,_col3
                      columns.types string:string:string:string
                      escape.delim \
                      hive.serialization.extend.nesting.levels true
                      serialization.format 1
                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                TotalFiles: 1
                GatherStats: false
                MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 10
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=6%3A30pm
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 6:30pm
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
      Truncated Path -> Alias:
        /alter_coltype/dt=10/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=6%3A30pm [alter_coltype]

  Stage: Stage-0
    Fetch Operator
      limit: -1

PREHOOK: query: select count(*) from alter_coltype where ts = 3.0
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=10/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: select count(*) from alter_coltype where ts = 3.0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
50
PREHOOK: query: -- make sure the partition predicate still works. 
select count(*) from alter_coltype where dt = '100x' or dt = '10'
PREHOOK: type: QUERY
PREHOOK: Input: default@alter_coltype
PREHOOK: Input: default@alter_coltype@dt=10/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
PREHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: query: -- make sure the partition predicate still works. 
select count(*) from alter_coltype where dt = '100x' or dt = '10'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Input: default@alter_coltype@dt=10/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=3.0
POSTHOOK: Input: default@alter_coltype@dt=100x/ts=6%3A30pm
#### A masked pattern was here ####
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
75
PREHOOK: query: explain extended select count(*) from alter_coltype where dt = '100x' or dt = '10'
PREHOOK: type: QUERY
POSTHOOK: query: explain extended select count(*) from alter_coltype where dt = '100x' or dt = '10'
POSTHOOK: type: QUERY
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME alter_coltype))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (or (= (TOK_TABLE_OR_COL dt) '100x') (= (TOK_TABLE_OR_COL dt) '10')))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        alter_coltype 
          TableScan
            alias: alter_coltype
            Statistics:
                numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: COMPLETE
            GatherStats: false
            Select Operator
              Statistics:
                  numRows: 75 dataSize: 573 basicStatsState: COMPLETE colStatsState: COMPLETE
              Group By Operator
                aggregations:
                      expr: count()
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0
                Statistics:
                    numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                Reduce Output Operator
                  sort order: 
                  Statistics:
                      numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: bigint
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 10
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=3.0
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 3.0
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
#### A masked pattern was here ####
          Partition
            base file name: ts=6%3A30pm
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            partition values:
              dt 100x
              ts 6:30pm
            properties:
              COLUMN_STATS_ACCURATE true
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.alter_coltype
              numFiles 1
              numRows 25
              partition_columns dt/ts
              rawDataSize 191
              serialization.ddl struct alter_coltype { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 216
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.alter_coltype
                partition_columns dt/ts
                serialization.ddl struct alter_coltype { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.alter_coltype
            name: default.alter_coltype
      Truncated Path -> Alias:
        /alter_coltype/dt=10/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=3.0 [alter_coltype]
        /alter_coltype/dt=100x/ts=6%3A30pm [alter_coltype]
      Needs Tagging: false
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: count(VALUE._col0)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0
          Statistics:
              numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
          Select Operator
            expressions:
                  expr: _col0
                  type: bigint
            outputColumnNames: _col0
            Statistics:
                numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
              Statistics:
                  numRows: 1 dataSize: 8 basicStatsState: COMPLETE colStatsState: COMPLETE
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0
                    columns.types bigint
                    escape.delim \
                    hive.serialization.extend.nesting.levels true
                    serialization.format 1
                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false

  Stage: Stage-0
    Fetch Operator
      limit: -1

PREHOOK: query: desc alter_coltype
PREHOOK: type: DESCTABLE
POSTHOOK: query: desc alter_coltype
POSTHOOK: type: DESCTABLE
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
key                 	string              	None                
value               	string              	None                
dt                  	string              	None                
ts                  	double              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	None                
ts                  	double              	None                
PREHOOK: query: desc alter_coltype partition (dt='100x', ts='6:30pm')
PREHOOK: type: DESCTABLE
POSTHOOK: query: desc alter_coltype partition (dt='100x', ts='6:30pm')
POSTHOOK: type: DESCTABLE
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
key                 	string              	None                
value               	string              	None                
dt                  	string              	None                
ts                  	double              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	None                
ts                  	double              	None                
PREHOOK: query: desc alter_coltype partition (dt='100x', ts=3.0)
PREHOOK: type: DESCTABLE
POSTHOOK: query: desc alter_coltype partition (dt='100x', ts=3.0)
POSTHOOK: type: DESCTABLE
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
key                 	string              	None                
value               	string              	None                
dt                  	string              	None                
ts                  	double              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	None                
ts                  	double              	None                
PREHOOK: query: desc alter_coltype partition (dt=10, ts=3.0)
PREHOOK: type: DESCTABLE
POSTHOOK: query: desc alter_coltype partition (dt=10, ts=3.0)
POSTHOOK: type: DESCTABLE
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
key                 	string              	None                
value               	string              	None                
dt                  	string              	None                
ts                  	double              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	None                
ts                  	double              	None                
PREHOOK: query: drop table alter_coltype
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@alter_coltype
PREHOOK: Output: default@alter_coltype
POSTHOOK: query: drop table alter_coltype
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@alter_coltype
POSTHOOK: Output: default@alter_coltype
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]
