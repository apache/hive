PREHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@druid_table_1
POSTHOOK: query: CREATE EXTERNAL TABLE druid_table_1
STORED BY 'org.apache.hadoop.hive.druid.QTestDruidStorageHandler'
TBLPROPERTIES ("druid.datasource" = "wikipedia")
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@druid_table_1
PREHOOK: query: DESCRIBE FORMATTED druid_table_1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@druid_table_1
POSTHOOK: query: DESCRIBE FORMATTED druid_table_1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@druid_table_1
# col_name            	data_type           	comment             
	 	 
__time              	timestamp           	from deserializer   
robot               	string              	from deserializer   
namespace           	string              	from deserializer   
anonymous           	string              	from deserializer   
unpatrolled         	string              	from deserializer   
page                	string              	from deserializer   
language            	string              	from deserializer   
newpage             	string              	from deserializer   
user                	string              	from deserializer   
count               	float               	from deserializer   
added               	float               	from deserializer   
delta               	float               	from deserializer   
variation           	float               	from deserializer   
deleted             	float               	from deserializer   
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
#### A masked pattern was here ####
Retention:          	0                   	 
#### A masked pattern was here ####
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	{\"BASIC_STATS\":\"true\"}
	EXTERNAL            	TRUE                
	druid.datasource    	wikipedia           
	numFiles            	0                   
	numRows             	0                   
	rawDataSize         	0                   
	storage_handler     	org.apache.hadoop.hive.druid.QTestDruidStorageHandler
	totalSize           	0                   
#### A masked pattern was here ####
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.druid.QTestDruidSerDe	 
InputFormat:        	null                	 
OutputFormat:       	null                	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
PREHOOK: query: EXPLAIN
SELECT max(added), sum(variation)
FROM druid_table_1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT max(added), sum(variation)
FROM druid_table_1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"all","aggregations":[{"type":"longMax","name":"$f0","fieldName":"added"},{"type":"doubleSum","name":"$f1","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: $f0 (type: bigint), $f1 (type: float)
            outputColumnNames: _col0, _col1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT `__time`, max(added), sum(variation)
FROM druid_table_1
GROUP BY `__time`
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT `__time`, max(added), sum(variation)
FROM druid_table_1
GROUP BY `__time`
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"NONE","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_year(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_year(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_year(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_year(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"YEAR","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_quarter(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_quarter(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_quarter(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_quarter(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"QUARTER","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_month(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_month(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_month(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_month(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"MONTH","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_week(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_week(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_week(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_week(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"WEEK","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_day(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_day(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_day(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_day(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"DAY","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_hour(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_hour(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"HOUR","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_minute(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_minute(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_minute(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_minute(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"MINUTE","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_second(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_second(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_second(`__time`), max(added), sum(variation)
FROM druid_table_1
GROUP BY floor_second(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"SECOND","aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
WHERE robot='1'
GROUP BY floor_hour(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
WHERE robot='1'
GROUP BY floor_hour(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: druid_table_1
          properties:
            druid.query.json {"queryType":"timeseries","dataSource":"wikipedia","descending":false,"granularity":"HOUR","filter":{"type":"selector","dimension":"robot","value":"1"},"aggregations":[{"type":"longMax","name":"$f1","fieldName":"added"},{"type":"doubleSum","name":"$f2","fieldName":"variation"}],"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"]}
            druid.query.type timeseries
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: __time (type: timestamp), $f1 (type: bigint), $f2 (type: float)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
WHERE floor_hour(`__time`)
    BETWEEN CAST('2010-01-01 00:00:00' AS TIMESTAMP)
        AND CAST('2014-01-01 00:00:00' AS TIMESTAMP)
GROUP BY floor_hour(`__time`)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT floor_hour(`__time`), max(added), sum(variation)
FROM druid_table_1
WHERE floor_hour(`__time`)
    BETWEEN CAST('2010-01-01 00:00:00' AS TIMESTAMP)
        AND CAST('2014-01-01 00:00:00' AS TIMESTAMP)
GROUP BY floor_hour(`__time`)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            filterExpr: floor_hour(__time) BETWEEN 2010-01-01 00:00:00.0 AND 2014-01-01 00:00:00.0 (type: boolean)
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"dimensions":["robot","namespace","anonymous","unpatrolled","page","language","newpage","user"],"metrics":["count","added","delta","variation","deleted"],"granularity":"all","pagingSpec":{"threshold":16384},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: floor_hour(__time) BETWEEN 2010-01-01 00:00:00.0 AND 2014-01-01 00:00:00.0 (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: floor_hour(__time) (type: timestamp), added (type: float), variation (type: float)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  aggregations: max(_col1), sum(_col2)
                  keys: _col0 (type: timestamp)
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp)
                    sort order: +
                    Map-reduce partition columns: _col0 (type: timestamp)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                    value expressions: _col1 (type: float), _col2 (type: double)
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), sum(VALUE._col1)
          keys: KEY._col0 (type: timestamp)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: EXPLAIN
SELECT subq.h, subq.m, subq.s
FROM
(
  SELECT floor_hour(`__time`) as h, max(added) as m, sum(variation) as s
  FROM druid_table_1
  GROUP BY floor_hour(`__time`)
) subq
WHERE subq.h BETWEEN CAST('2010-01-01 00:00:00' AS TIMESTAMP)
        AND CAST('2014-01-01 00:00:00' AS TIMESTAMP)
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN
SELECT subq.h, subq.m, subq.s
FROM
(
  SELECT floor_hour(`__time`) as h, max(added) as m, sum(variation) as s
  FROM druid_table_1
  GROUP BY floor_hour(`__time`)
) subq
WHERE subq.h BETWEEN CAST('2010-01-01 00:00:00' AS TIMESTAMP)
        AND CAST('2014-01-01 00:00:00' AS TIMESTAMP)
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: druid_table_1
            filterExpr: floor_hour(__time) BETWEEN 2010-01-01 00:00:00.0 AND 2014-01-01 00:00:00.0 (type: boolean)
            properties:
              druid.query.json {"queryType":"select","dataSource":"wikipedia","descending":false,"intervals":["1900-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"],"dimensions":["robot","namespace","anonymous","unpatrolled","page","language","newpage","user"],"metrics":["count","added","delta","variation","deleted"],"granularity":"all","pagingSpec":{"threshold":16384},"context":{"druid.query.fetch":false}}
              druid.query.type select
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: floor_hour(__time) BETWEEN 2010-01-01 00:00:00.0 AND 2014-01-01 00:00:00.0 (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: floor_hour(__time) (type: timestamp), added (type: float), variation (type: float)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  aggregations: max(_col1), sum(_col2)
                  keys: _col0 (type: timestamp)
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: timestamp)
                    sort order: +
                    Map-reduce partition columns: _col0 (type: timestamp)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                    value expressions: _col1 (type: float), _col2 (type: double)
      Reduce Operator Tree:
        Group By Operator
          aggregations: max(VALUE._col0), sum(VALUE._col1)
          keys: KEY._col0 (type: timestamp)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

