PREHOOK: query: -- Tests truncating a column from a list bucketing table

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE
PREHOOK: type: CREATETABLE
POSTHOOK: query: -- Tests truncating a column from a list bucketing table

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@test_tab
PREHOOK: query: ALTER TABLE test_tab
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES
PREHOOK: type: ALTERTABLE_SKEWED
PREHOOK: Input: default@test_tab
PREHOOK: Output: default@test_tab
POSTHOOK: query: ALTER TABLE test_tab
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES
POSTHOOK: type: ALTERTABLE_SKEWED
POSTHOOK: Input: default@test_tab
POSTHOOK: Output: default@test_tab
PREHOOK: query: INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_tab@part=1
POSTHOOK: query: INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_tab@part=1
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab
PREHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab
POSTHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
0	val_0	1
0	val_0	1
0	val_0	1
PREHOOK: query: TRUNCATE TABLE test_tab PARTITION (part ='1') COLUMNS (value)
PREHOOK: type: TRUNCATETABLE
PREHOOK: Input: default@test_tab
PREHOOK: Output: default@test_tab@part=1
POSTHOOK: query: TRUNCATE TABLE test_tab PARTITION (part ='1') COLUMNS (value)
POSTHOOK: type: TRUNCATETABLE
POSTHOOK: Input: default@test_tab
POSTHOOK: Output: default@test_tab@part=1
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: -- In the following select statements the list bucketing optimization should still be used
-- In both cases value should be null

EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '484'
PREHOOK: type: QUERY
POSTHOOK: query: -- In the following select statements the list bucketing optimization should still be used
-- In both cases value should be null

EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '484'
POSTHOOK: type: QUERY
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME test_tab))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (AND (= (TOK_TABLE_OR_COL part) '1') (= (TOK_TABLE_OR_COL key) '484')))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        test_tab 
          TableScan
            alias: test_tab
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: (key = '484')
                  type: boolean
              Select Operator
                expressions:
                      expr: key
                      type: string
                      expr: value
                      type: string
                      expr: part
                      type: string
                outputColumnNames: _col0, _col1, _col2
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      properties:
                        columns _col0,_col1,_col2
                        columns.types string:string:string
                        escape.delim \
                        hive.serialization.extend.nesting.levels true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: key=484
            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
            partition values:
              part 1
            properties:
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.test_tab
              numFiles 2
              numRows 500
              partition_columns part
              rawDataSize 4812
              serialization.ddl struct test_tab { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              totalSize 1761
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
          
              input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.test_tab
                numFiles 2
                numPartitions 1
                numRows 500
                partition_columns part
                rawDataSize 4812
                serialization.ddl struct test_tab { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
                totalSize 1761
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              name: default.test_tab
            name: default.test_tab
      Truncated Path -> Alias:
        /test_tab/part=1/key=484 [test_tab]

  Stage: Stage-0
    Fetch Operator
      limit: -1


PREHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '484'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab
PREHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '484'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab
POSTHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
484	NULL	1
PREHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME test_tab))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (AND (= (TOK_TABLE_OR_COL part) '1') (= (TOK_TABLE_OR_COL key) '0')))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        test_tab 
          TableScan
            alias: test_tab
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: (key = '0')
                  type: boolean
              Select Operator
                expressions:
                      expr: key
                      type: string
                      expr: value
                      type: string
                      expr: part
                      type: string
                outputColumnNames: _col0, _col1, _col2
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      properties:
                        columns _col0,_col1,_col2
                        columns.types string:string:string
                        escape.delim \
                        hive.serialization.extend.nesting.levels true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME
            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
            partition values:
              part 1
            properties:
              bucket_count -1
              columns key,value
              columns.types string:string
#### A masked pattern was here ####
              name default.test_tab
              numFiles 2
              numRows 500
              partition_columns part
              rawDataSize 4812
              serialization.ddl struct test_tab { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              totalSize 1761
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
          
              input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
              properties:
                bucket_count -1
                columns key,value
                columns.types string:string
#### A masked pattern was here ####
                name default.test_tab
                numFiles 2
                numPartitions 1
                numRows 500
                partition_columns part
                rawDataSize 4812
                serialization.ddl struct test_tab { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
                totalSize 1761
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              name: default.test_tab
            name: default.test_tab
      Truncated Path -> Alias:
        /test_tab/part=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME [test_tab]

  Stage: Stage-0
    Fetch Operator
      limit: -1


PREHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab
PREHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab
POSTHOOK: Input: default@test_tab@part=1
#### A masked pattern was here ####
POSTHOOK: Lineage: test_tab PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
0	NULL	1
0	NULL	1
0	NULL	1
