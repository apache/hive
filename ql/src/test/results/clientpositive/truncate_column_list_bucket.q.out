PREHOOK: query: CREATE TABLE test_tab_n3 (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@test_tab_n3
POSTHOOK: query: CREATE TABLE test_tab_n3 (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@test_tab_n3
PREHOOK: query: ALTER TABLE test_tab_n3
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES
PREHOOK: type: ALTERTABLE_SKEWED
PREHOOK: Input: default@test_tab_n3
PREHOOK: Output: default@test_tab_n3
POSTHOOK: query: ALTER TABLE test_tab_n3
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES
POSTHOOK: type: ALTERTABLE_SKEWED
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Output: default@test_tab_n3
PREHOOK: query: INSERT OVERWRITE TABLE test_tab_n3 PARTITION (part = '1') SELECT * FROM src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@test_tab_n3@part=1
POSTHOOK: query: INSERT OVERWRITE TABLE test_tab_n3 PARTITION (part = '1') SELECT * FROM src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@test_tab_n3@part=1
POSTHOOK: Lineage: test_tab_n3 PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: test_tab_n3 PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab_n3
PREHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
0	val_0	1
0	val_0	1
0	val_0	1
PREHOOK: query: TRUNCATE TABLE test_tab_n3 PARTITION (part ='1') COLUMNS (value)
PREHOOK: type: TRUNCATETABLE
PREHOOK: Input: default@test_tab_n3
PREHOOK: Output: default@test_tab_n3@part=1
POSTHOOK: query: TRUNCATE TABLE test_tab_n3 PARTITION (part ='1') COLUMNS (value)
POSTHOOK: type: TRUNCATETABLE
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Output: default@test_tab_n3@part=1
PREHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '484'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab_n3
PREHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '484'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT CAST('484' AS STRING) AS `key`, `value`, CAST('1' AS STRING) AS `part`
FROM `default`.`test_tab_n3`
WHERE `part` = '1' AND `key` = '484'
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: test_tab_n3
            filterExpr: ((part = '1') and (key = '484')) (type: boolean)
            Statistics: Num rows: 500 Data size: 4812 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (key = '484') (type: boolean)
              Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: '484' (type: string), value (type: string), '1' (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1,_col2
                        columns.types string:string:string
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
      Execution mode: vectorized
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: key=484
            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
            partition values:
              part 1
            properties:
              bucket_count -1
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.test_tab_n3
              numFiles 2
              numRows 500
              partition_columns part
              partition_columns.types string
              rawDataSize 4812
              serialization.ddl struct test_tab_n3 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              totalSize 1761
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
          
              input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
              properties:
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.test_tab_n3
                partition_columns part
                partition_columns.types string
                serialization.ddl struct test_tab_n3 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              name: default.test_tab_n3
            name: default.test_tab_n3
      Truncated Path -> Alias:
        /test_tab_n3/part=1/key=484 [test_tab_n3]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '484'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab_n3
PREHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '484'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
484	NULL	1
PREHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab_n3
PREHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
OPTIMIZED SQL: SELECT CAST('0' AS STRING) AS `key`, `value`, CAST('1' AS STRING) AS `part`
FROM `default`.`test_tab_n3`
WHERE `part` = '1' AND `key` = '0'
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: test_tab_n3
            filterExpr: ((part = '1') and (key = '0')) (type: boolean)
            Statistics: Num rows: 500 Data size: 4812 Basic stats: COMPLETE Column stats: NONE
            GatherStats: false
            Filter Operator
              isSamplingPred: false
              predicate: (key = '0') (type: boolean)
              Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: '0' (type: string), value (type: string), '1' (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
#### A masked pattern was here ####
                  NumFilesPerFileSink: 1
                  Statistics: Num rows: 250 Data size: 2406 Basic stats: COMPLETE Column stats: NONE
#### A masked pattern was here ####
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      properties:
                        columns _col0,_col1,_col2
                        columns.types string:string:string
                        escape.delim \
                        hive.serialization.extend.additional.nesting.levels true
                        serialization.escape.crlf true
                        serialization.format 1
                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                  TotalFiles: 1
                  GatherStats: false
                  MultiFileSpray: false
      Execution mode: vectorized
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME
            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
            partition values:
              part 1
            properties:
              bucket_count -1
              column.name.delimiter ,
              columns key,value
              columns.comments 
              columns.types string:string
#### A masked pattern was here ####
              name default.test_tab_n3
              numFiles 2
              numRows 500
              partition_columns part
              partition_columns.types string
              rawDataSize 4812
              serialization.ddl struct test_tab_n3 { string key, string value}
              serialization.format 1
              serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              totalSize 1761
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
          
              input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
              output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
              properties:
                bucket_count -1
                bucketing_version 2
                column.name.delimiter ,
                columns key,value
                columns.comments 
                columns.types string:string
#### A masked pattern was here ####
                name default.test_tab_n3
                partition_columns part
                partition_columns.types string
                serialization.ddl struct test_tab_n3 { string key, string value}
                serialization.format 1
                serialization.lib org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
              name: default.test_tab_n3
            name: default.test_tab_n3
      Truncated Path -> Alias:
        /test_tab_n3/part=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME [test_tab_n3]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
PREHOOK: type: QUERY
PREHOOK: Input: default@test_tab_n3
PREHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM test_tab_n3 WHERE part = '1' AND key = '0'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@test_tab_n3
POSTHOOK: Input: default@test_tab_n3@part=1
#### A masked pattern was here ####
0	NULL	1
0	NULL	1
0	NULL	1
