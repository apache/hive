PREHOOK: query: create table t1(`x+1` string, `y&y` string, `!@#$%^&*()_q` string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1
POSTHOOK: query: create table t1(`x+1` string, `y&y` string, `!@#$%^&*()_q` string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1
PREHOOK: query: describe t1
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@t1
POSTHOOK: query: describe t1
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@t1
x+1                 	string              	                    
y&y                 	string              	                    
!@#$%^&*()_q        	string              	                    
PREHOOK: query: select `x+1`, `y&y`, `!@#$%^&*()_q` from t1
PREHOOK: type: QUERY
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: select `x+1`, `y&y`, `!@#$%^&*()_q` from t1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
PREHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1
PREHOOK: type: QUERY
POSTHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: t1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: x+1 (type: string), y&y (type: string), !@#$%^&*()_q (type: string)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            ListSink

PREHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1'
PREHOOK: type: QUERY
POSTHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: (!@#$%^&*()_q = '1') (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: x+1 (type: string), y&y (type: string), '1' (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1'
PREHOOK: type: QUERY
POSTHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: (!@#$%^&*()_q = '1') (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: x+1 (type: string), y&y (type: string)
                outputColumnNames: x+1, y&y
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  keys: x+1 (type: string), y&y (type: string)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: string), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Select Operator
            expressions: _col0 (type: string), _col1 (type: string), '1' (type: string)
            outputColumnNames: _col0, _col1, _col2
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)  
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1'
PREHOOK: type: QUERY
POSTHOOK: query: explain select `x+1`, `y&y`, `!@#$%^&*()_q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)  
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: (!@#$%^&*()_q = '1') (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: x+1 (type: string), y&y (type: string)
                outputColumnNames: x+1, y&y
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  keys: x+1 (type: string), y&y (type: string)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: string), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: '1' (type: string), _col1 (type: string)
              sort order: ++
              Map-reduce partition columns: '1' (type: string)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              value expressions: _col0 (type: string)
      Execution mode: vectorized
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), KEY.reducesinkkey1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          PTF Operator
            Function definitions:
                Input definition
                  input alias: ptf_0
                  output shape: _col0: string, _col1: string
                  type: WINDOWING
                Windowing table definition
                  input alias: ptf_1
                  name: windowingtablefunction
                  order by: _col1 ASC NULLS FIRST
                  partition by: '1'
                  raw input shape:
                  window functions:
                      window function definition
                        alias: rank_window_0
                        arguments: _col1
                        name: rank
                        window function: GenericUDAFRankEvaluator
                        window frame: ROWS PRECEDING(MAX)~FOLLOWING(MAX)
                        isPivotResult: true
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: _col0 (type: string), _col1 (type: string), '1' (type: string), rank_window_0 (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              File Output Operator
                compressed: false
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: explain select `X+1`, `Y&y`, `!@#$%^&*()_Q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)  
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&Y`, `!@#$%^&*()_q` having `!@#$%^&*()_Q` = '1'
PREHOOK: type: QUERY
POSTHOOK: query: explain select `X+1`, `Y&y`, `!@#$%^&*()_Q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)  
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&Y`, `!@#$%^&*()_q` having `!@#$%^&*()_Q` = '1'
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t1
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Filter Operator
              predicate: (!@#$%^&*()_q = '1') (type: boolean)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              Select Operator
                expressions: x+1 (type: string), y&y (type: string)
                outputColumnNames: x+1, y&y
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                Group By Operator
                  keys: x+1 (type: string), y&y (type: string)
                  mode: hash
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: string)
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          keys: KEY._col0 (type: string), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: '1' (type: string), _col1 (type: string)
              sort order: ++
              Map-reduce partition columns: '1' (type: string)
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              value expressions: _col0 (type: string)
      Execution mode: vectorized
      Reduce Operator Tree:
        Select Operator
          expressions: VALUE._col0 (type: string), KEY.reducesinkkey1 (type: string)
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          PTF Operator
            Function definitions:
                Input definition
                  input alias: ptf_0
                  output shape: _col0: string, _col1: string
                  type: WINDOWING
                Windowing table definition
                  input alias: ptf_1
                  name: windowingtablefunction
                  order by: _col1 ASC NULLS FIRST
                  partition by: '1'
                  raw input shape:
                  window functions:
                      window function definition
                        alias: rank_window_0
                        arguments: _col1
                        name: rank
                        window function: GenericUDAFRankEvaluator
                        window frame: ROWS PRECEDING(MAX)~FOLLOWING(MAX)
                        isPivotResult: true
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: _col0 (type: string), _col1 (type: string), '1' (type: string), rank_window_0 (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              File Output Operator
                compressed: false
                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: create table t4(`x+1``` string, `y&y` string)
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t4
POSTHOOK: query: create table t4(`x+1``` string, `y&y` string)
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t4
PREHOOK: query: describe t4
PREHOOK: type: DESCTABLE
PREHOOK: Input: default@t4
POSTHOOK: query: describe t4
POSTHOOK: type: DESCTABLE
POSTHOOK: Input: default@t4
x+1`                	string              	                    
y&y                 	string              	                    
PREHOOK: query: insert into table t4 select * from src
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@t4
POSTHOOK: query: insert into table t4 select * from src
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@t4
POSTHOOK: Lineage: t4.x+1` SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: t4.y&y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
PREHOOK: query: select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)  
from t4 where `x+1``` = '10' group by `x+1```, `y&y` having `x+1``` = '10'
PREHOOK: type: QUERY
PREHOOK: Input: default@t4
#### A masked pattern was here ####
POSTHOOK: query: select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)  
from t4 where `x+1``` = '10' group by `x+1```, `y&y` having `x+1``` = '10'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t4
#### A masked pattern was here ####
10	val_10	1
PREHOOK: query: create view v1 as 
select `x+1```, `y&y`
from t4 where `x+1``` < '200'
PREHOOK: type: CREATEVIEW
PREHOOK: Input: default@t4
PREHOOK: Output: database:default
PREHOOK: Output: default@v1
POSTHOOK: query: create view v1 as 
select `x+1```, `y&y`
from t4 where `x+1``` < '200'
POSTHOOK: type: CREATEVIEW
POSTHOOK: Input: default@t4
POSTHOOK: Output: database:default
POSTHOOK: Output: default@v1
POSTHOOK: Lineage: v1.x+1` SIMPLE [(t4)t4.FieldSchema(name:x+1`, type:string, comment:null), ]
POSTHOOK: Lineage: v1.y&y SIMPLE [(t4)t4.FieldSchema(name:y&y, type:string, comment:null), ]
PREHOOK: query: select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)
from v1
group by `x+1```, `y&y`
PREHOOK: type: QUERY
PREHOOK: Input: default@t4
PREHOOK: Input: default@v1
#### A masked pattern was here ####
POSTHOOK: query: select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)
from v1
group by `x+1```, `y&y`
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t4
POSTHOOK: Input: default@v1
#### A masked pattern was here ####
0	val_0	1
10	val_10	1
100	val_100	1
103	val_103	1
104	val_104	1
105	val_105	1
11	val_11	1
111	val_111	1
113	val_113	1
114	val_114	1
116	val_116	1
118	val_118	1
119	val_119	1
12	val_12	1
120	val_120	1
125	val_125	1
126	val_126	1
128	val_128	1
129	val_129	1
131	val_131	1
133	val_133	1
134	val_134	1
136	val_136	1
137	val_137	1
138	val_138	1
143	val_143	1
145	val_145	1
146	val_146	1
149	val_149	1
15	val_15	1
150	val_150	1
152	val_152	1
153	val_153	1
155	val_155	1
156	val_156	1
157	val_157	1
158	val_158	1
160	val_160	1
162	val_162	1
163	val_163	1
164	val_164	1
165	val_165	1
166	val_166	1
167	val_167	1
168	val_168	1
169	val_169	1
17	val_17	1
170	val_170	1
172	val_172	1
174	val_174	1
175	val_175	1
176	val_176	1
177	val_177	1
178	val_178	1
179	val_179	1
18	val_18	1
180	val_180	1
181	val_181	1
183	val_183	1
186	val_186	1
187	val_187	1
189	val_189	1
19	val_19	1
190	val_190	1
191	val_191	1
192	val_192	1
193	val_193	1
194	val_194	1
195	val_195	1
196	val_196	1
197	val_197	1
199	val_199	1
2	val_2	1
20	val_20	1
