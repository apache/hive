PREHOOK: query: DROP TABLE IF EXISTS UserVisits_web_text_none
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS UserVisits_web_text_none
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile
PREHOOK: type: CREATETABLE
POSTHOOK: query: CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@UserVisits_web_text_none
PREHOOK: query: LOAD DATA LOCAL INPATH "../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none
PREHOOK: type: LOAD
PREHOOK: Output: default@uservisits_web_text_none
POSTHOOK: query: LOAD DATA LOCAL INPATH "../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none
POSTHOOK: type: LOAD
POSTHOOK: Output: default@uservisits_web_text_none
PREHOOK: query: explain 
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
PREHOOK: type: QUERY
POSTHOOK: query: explain 
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME UserVisits_web_text_none)) (TOK_TABCOLNAME sourceIP avgTimeOnSite adRevenue))

STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-1 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Map Reduce
      Alias -> Map Operator Tree:
        uservisits_web_text_none 
          TableScan
            alias: uservisits_web_text_none
            Select Operator
              expressions:
                    expr: sourceip
                    type: string
                    expr: avgtimeonsite
                    type: int
                    expr: adrevenue
                    type: float
              outputColumnNames: sourceip, avgtimeonsite, adrevenue
              Group By Operator
                aggregations:
                      expr: compute_stats(sourceip, 16)
                      expr: compute_stats(avgtimeonsite, 16)
                      expr: compute_stats(adrevenue, 16)
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0, _col1, _col2
                Reduce Output Operator
                  sort order: 
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col1
                        type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col2
                        type: struct<columntype:string,min:double,max:double,countnulls:bigint,bitvector:string,numbitvectors:int>
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: compute_stats(VALUE._col0)
                expr: compute_stats(VALUE._col1)
                expr: compute_stats(VALUE._col2)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Select Operator
            expressions:
                  expr: _col0
                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col1
                  type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col2
                  type: struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
            outputColumnNames: _col0, _col1, _col2
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-1
    Column Stats Work
      Column Stats Desc:
          Columns: sourceIP, avgTimeOnSite, adRevenue
          Column Types: string, int, float
          Table: UserVisits_web_text_none


PREHOOK: query: explain extended
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
PREHOOK: type: QUERY
POSTHOOK: query: explain extended
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME UserVisits_web_text_none)) (TOK_TABCOLNAME sourceIP avgTimeOnSite adRevenue))

STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-1 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Map Reduce
      Alias -> Map Operator Tree:
        uservisits_web_text_none 
          TableScan
            alias: uservisits_web_text_none
            GatherStats: false
            Select Operator
              expressions:
                    expr: sourceip
                    type: string
                    expr: avgtimeonsite
                    type: int
                    expr: adrevenue
                    type: float
              outputColumnNames: sourceip, avgtimeonsite, adrevenue
              Group By Operator
                aggregations:
                      expr: compute_stats(sourceip, 16)
                      expr: compute_stats(avgtimeonsite, 16)
                      expr: compute_stats(adrevenue, 16)
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0, _col1, _col2
                Reduce Output Operator
                  sort order: 
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col1
                        type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col2
                        type: struct<columntype:string,min:double,max:double,countnulls:bigint,bitvector:string,numbitvectors:int>
      Needs Tagging: false
      Path -> Alias:
#### A masked pattern was here ####
      Path -> Partition:
#### A masked pattern was here ####
          Partition
            base file name: uservisits_web_text_none
            input format: org.apache.hadoop.mapred.TextInputFormat
            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
            properties:
              bucket_count -1
              columns sourceip,desturl,visitdate,adrevenue,useragent,ccode,lcode,skeyword,avgtimeonsite
              columns.types string:string:string:float:string:string:string:string:int
              field.delim |
#### A masked pattern was here ####
              name default.uservisits_web_text_none
              numFiles 1
              numPartitions 0
              numRows 0
              rawDataSize 0
              serialization.ddl struct uservisits_web_text_none { string sourceip, string desturl, string visitdate, float adrevenue, string useragent, string ccode, string lcode, string skeyword, i32 avgtimeonsite}
              serialization.format |
              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              totalSize 7060
#### A masked pattern was here ####
            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
          
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              properties:
                bucket_count -1
                columns sourceip,desturl,visitdate,adrevenue,useragent,ccode,lcode,skeyword,avgtimeonsite
                columns.types string:string:string:float:string:string:string:string:int
                field.delim |
#### A masked pattern was here ####
                name default.uservisits_web_text_none
                numFiles 1
                numPartitions 0
                numRows 0
                rawDataSize 0
                serialization.ddl struct uservisits_web_text_none { string sourceip, string desturl, string visitdate, float adrevenue, string useragent, string ccode, string lcode, string skeyword, i32 avgtimeonsite}
                serialization.format |
                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                totalSize 7060
#### A masked pattern was here ####
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.uservisits_web_text_none
            name: default.uservisits_web_text_none
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: compute_stats(VALUE._col0)
                expr: compute_stats(VALUE._col1)
                expr: compute_stats(VALUE._col2)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Select Operator
            expressions:
                  expr: _col0
                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col1
                  type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col2
                  type: struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
            outputColumnNames: _col0, _col1, _col2
            File Output Operator
              compressed: false
              GlobalTableId: 0
#### A masked pattern was here ####
              NumFilesPerFileSink: 1
#### A masked pattern was here ####
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                  properties:
                    columns _col0,_col1,_col2
                    columns.types struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>:struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>:struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
                    escape.delim \
                    serialization.format 1
              TotalFiles: 1
              GatherStats: false
              MultiFileSpray: false
      Truncated Path -> Alias:
        /uservisits_web_text_none [uservisits_web_text_none]

  Stage: Stage-1
    Column Stats Work
      Column Stats Desc:
          Columns: sourceIP, avgTimeOnSite, adRevenue
          Column Types: string, int, float
          Table: UserVisits_web_text_none
          Is Table Level Stats: true


PREHOOK: query: analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
PREHOOK: type: QUERY
PREHOOK: Input: default@uservisits_web_text_none
#### A masked pattern was here ####
POSTHOOK: query: analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue
POSTHOOK: type: QUERY
POSTHOOK: Input: default@uservisits_web_text_none
#### A masked pattern was here ####
PREHOOK: query: CREATE TABLE empty_tab(
   a int,
   b double,
   c string, 
   d boolean,
   e binary)
row format delimited fields terminated by '|'  stored as textfile
PREHOOK: type: CREATETABLE
POSTHOOK: query: CREATE TABLE empty_tab(
   a int,
   b double,
   c string, 
   d boolean,
   e binary)
row format delimited fields terminated by '|'  stored as textfile
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@empty_tab
PREHOOK: query: explain 
analyze table empty_tab compute statistics for columns a,b,c,d,e
PREHOOK: type: QUERY
POSTHOOK: query: explain 
analyze table empty_tab compute statistics for columns a,b,c,d,e
POSTHOOK: type: QUERY
ABSTRACT SYNTAX TREE:
  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME empty_tab)) (TOK_TABCOLNAME a b c d e))

STAGE DEPENDENCIES:
  Stage-0 is a root stage
  Stage-1 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Map Reduce
      Alias -> Map Operator Tree:
        empty_tab 
          TableScan
            alias: empty_tab
            Select Operator
              expressions:
                    expr: a
                    type: int
                    expr: b
                    type: double
                    expr: c
                    type: string
                    expr: d
                    type: boolean
                    expr: e
                    type: binary
              outputColumnNames: a, b, c, d, e
              Group By Operator
                aggregations:
                      expr: compute_stats(a, 16)
                      expr: compute_stats(b, 16)
                      expr: compute_stats(c, 16)
                      expr: compute_stats(d, 16)
                      expr: compute_stats(e, 16)
                bucketGroup: false
                mode: hash
                outputColumnNames: _col0, _col1, _col2, _col3, _col4
                Reduce Output Operator
                  sort order: 
                  tag: -1
                  value expressions:
                        expr: _col0
                        type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col1
                        type: struct<columntype:string,min:double,max:double,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col2
                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>
                        expr: _col3
                        type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>
                        expr: _col4
                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint>
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: compute_stats(VALUE._col0)
                expr: compute_stats(VALUE._col1)
                expr: compute_stats(VALUE._col2)
                expr: compute_stats(VALUE._col3)
                expr: compute_stats(VALUE._col4)
          bucketGroup: false
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Select Operator
            expressions:
                  expr: _col0
                  type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col1
                  type: struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col2
                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>
                  expr: _col3
                  type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>
                  expr: _col4
                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint>
            outputColumnNames: _col0, _col1, _col2, _col3, _col4
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-1
    Column Stats Work
      Column Stats Desc:
          Columns: a, b, c, d, e
          Column Types: int, double, string, boolean, binary
          Table: empty_tab


PREHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e
PREHOOK: type: QUERY
PREHOOK: Input: default@empty_tab
#### A masked pattern was here ####
POSTHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e
POSTHOOK: type: QUERY
POSTHOOK: Input: default@empty_tab
#### A masked pattern was here ####
