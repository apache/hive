Saving all output to "!!{outputDirectory}!!/alter_merge_stats.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/alter_merge_stats.q
>>>  create table src_rc_merge_test_stat(key int, value string) stored as rcfile;
No rows affected 
>>>  
>>>  load data local inpath '../data/files/smbbucket_1.rc' into table src_rc_merge_test_stat;
No rows affected 
>>>  load data local inpath '../data/files/smbbucket_2.rc' into table src_rc_merge_test_stat;
No rows affected 
>>>  load data local inpath '../data/files/smbbucket_3.rc' into table src_rc_merge_test_stat;
No rows affected 
>>>  
>>>  show table extended like `src_rc_merge_test_stat`;
'tab_name'
'tableName:src_rc_merge_test_stat'
'owner:!!{user.name}!!'
'location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_stat'
'inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'columns:struct columns { i32 key, string value}'
'partitioned:false'
'partitionColumns:'
'totalNumberFiles:3'
'totalFileSize:636'
'maxFileSize:222'
'minFileSize:206'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended src_rc_merge_test_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{numPartitions=0, numFiles=3, transient_lastDdlTime=!!UNIXTIME!!, totalSize=636, numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
4 rows selected 
>>>  
>>>  analyze table src_rc_merge_test_stat compute statistics;
'key','value'
No rows selected 
>>>  
>>>  desc extended src_rc_merge_test_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{numPartitions=0, numFiles=3, transient_lastDdlTime=!!UNIXTIME!!, numRows=15, totalSize=636, rawDataSize=110}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
4 rows selected 
>>>  
>>>  alter table src_rc_merge_test_stat concatenate;
No rows affected 
>>>  
>>>  show table extended like `src_rc_merge_test_stat`;
'tab_name'
'tableName:src_rc_merge_test_stat'
'owner:!!{user.name}!!'
'location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_stat'
'inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'columns:struct columns { i32 key, string value}'
'partitioned:false'
'partitionColumns:'
'totalNumberFiles:1'
'totalFileSize:239'
'maxFileSize:239'
'minFileSize:239'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended src_rc_merge_test_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{numPartitions=0, numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=15, totalSize=239, rawDataSize=110}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
4 rows selected 
>>>  
>>>  
>>>  create table src_rc_merge_test_part_stat(key int, value string) partitioned by (ds string) stored as rcfile;
No rows affected 
>>>  
>>>  alter table src_rc_merge_test_part_stat add partition (ds='2011');
No rows affected 
>>>  
>>>  load data local inpath '../data/files/smbbucket_1.rc' into table src_rc_merge_test_part_stat partition (ds='2011');
No rows affected 
>>>  load data local inpath '../data/files/smbbucket_2.rc' into table src_rc_merge_test_part_stat partition (ds='2011');
No rows affected 
>>>  load data local inpath '../data/files/smbbucket_3.rc' into table src_rc_merge_test_part_stat partition (ds='2011');
No rows affected 
>>>  
>>>  show table extended like `src_rc_merge_test_part_stat` partition (ds='2011');
'tab_name'
'tableName:src_rc_merge_test_part_stat'
'owner:!!{user.name}!!'
'location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_part_stat/ds=2011'
'inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'columns:struct columns { i32 key, string value}'
'partitioned:true'
'partitionColumns:struct partition_columns { string ds}'
'totalNumberFiles:3'
'totalFileSize:636'
'maxFileSize:222'
'minFileSize:206'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended src_rc_merge_test_part_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'ds','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_part_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_part_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{numPartitions=1, numFiles=3, transient_lastDdlTime=!!UNIXTIME!!, totalSize=636, numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
5 rows selected 
>>>  
>>>  analyze table src_rc_merge_test_part_stat partition(ds='2011') compute statistics;
'key','value','ds'
No rows selected 
>>>  
>>>  desc extended src_rc_merge_test_part_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'ds','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_part_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_part_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{numPartitions=1, numFiles=3, transient_lastDdlTime=!!UNIXTIME!!, numRows=15, totalSize=636, rawDataSize=110}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
5 rows selected 
>>>  
>>>  alter table src_rc_merge_test_part_stat partition (ds='2011') concatenate;
No rows affected 
>>>  
>>>  show table extended like `src_rc_merge_test_part_stat` partition (ds='2011');
'tab_name'
'tableName:src_rc_merge_test_part_stat'
'owner:!!{user.name}!!'
'location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_part_stat/ds=2011'
'inputformat:org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'columns:struct columns { i32 key, string value}'
'partitioned:true'
'partitionColumns:struct partition_columns { string ds}'
'totalNumberFiles:1'
'totalFileSize:239'
'maxFileSize:239'
'minFileSize:239'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended src_rc_merge_test_part_stat;
'col_name','data_type','comment'
'key','int',''
'value','string',''
'ds','string',''
'','',''
'Detailed Table Information','Table(tableName:src_rc_merge_test_part_stat, dbName:alter_merge_stats, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/alter_merge_stats.db/src_rc_merge_test_part_stat, inputFormat:org.apache.hadoop.hive.ql.io.RCFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.RCFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{numPartitions=1, numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=15, totalSize=239, rawDataSize=110}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
5 rows selected 
>>>  
>>>  drop table src_rc_merge_test_stat;
No rows affected 
>>>  drop table src_rc_merge_test_part_stat;
No rows affected 
>>>  !record
