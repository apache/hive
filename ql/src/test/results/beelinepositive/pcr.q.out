Saving all output to "!!{outputDirectory}!!/pcr.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/pcr.q
>>>  drop table pcr_t1;
No rows affected 
>>>  drop table pcr_t2;
No rows affected 
>>>  drop table pcr_t3;
No rows affected 
>>>  
>>>  create table pcr_t1 (key int, value string) partitioned by (ds string);
No rows affected 
>>>  
>>>  insert overwrite table pcr_t1 partition (ds='2000-04-08') select * from src where key < 20 order by key;
'_col0','_col1'
No rows selected 
>>>  insert overwrite table pcr_t1 partition (ds='2000-04-09') select * from src where key < 20 order by key;
'_col0','_col1'
No rows selected 
>>>  insert overwrite table pcr_t1 partition (ds='2000-04-10') select * from src where key < 20 order by key;
'_col0','_col1'
No rows selected 
>>>  
>>>  explain extended select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 order by key, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (and (<= (TOK_TABLE_OR_COL ds) '2000-04-09') (< (TOK_TABLE_OR_COL key) 5))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key < 5)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col2'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
174 rows selected 
>>>  select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 order by key, ds;
'key','value','ds'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'2','val_2','2000-04-08'
'2','val_2','2000-04-09'
'4','val_4','2000-04-08'
'4','val_4','2000-04-09'
10 rows selected 
>>>  
>>>  explain extended select key, value from pcr_t1 where ds<='2000-04-09' or key<5 order by key;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (or (<= (TOK_TABLE_OR_COL ds) '2000-04-09') (< (TOK_TABLE_OR_COL key) 5))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: ((ds <= '2000-04-09') or (key < 5))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                outputColumnNames: _col0, _col1'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                  sort order: +'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1'
'                  columns.types int:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
219 rows selected 
>>>  select key, value from pcr_t1 where ds<='2000-04-09' or key<5 order by key;
'key','value'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'2','val_2'
'2','val_2'
'2','val_2'
'4','val_4'
'4','val_4'
'4','val_4'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'8','val_8'
'8','val_8'
'9','val_9'
'9','val_9'
'10','val_10'
'10','val_10'
'11','val_11'
'11','val_11'
'12','val_12'
'12','val_12'
'12','val_12'
'12','val_12'
'15','val_15'
'15','val_15'
'15','val_15'
'15','val_15'
'17','val_17'
'17','val_17'
'18','val_18'
'18','val_18'
'18','val_18'
'18','val_18'
'19','val_19'
'19','val_19'
45 rows selected 
>>>  
>>>  explain extended select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 and value != 'val_2' order by key, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (and (and (<= (TOK_TABLE_OR_COL ds) '2000-04-09') (< (TOK_TABLE_OR_COL key) 5)) (!= (TOK_TABLE_OR_COL value) 'val_2'))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: ((key < 5) and (value <> 'val_2'))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col2'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
174 rows selected 
>>>  select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 and value != 'val_2' order by key, ds;
'key','value','ds'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'4','val_4','2000-04-08'
'4','val_4','2000-04-09'
8 rows selected 
>>>  
>>>  
>>>  explain extended 
select key, value, ds from pcr_t1 
where (ds < '2000-04-09' and key < 5) or (ds > '2000-04-09' and value == 'val_5') order by key, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (or (and (< (TOK_TABLE_OR_COL ds) '2000-04-09') (< (TOK_TABLE_OR_COL key) 5)) (and (> (TOK_TABLE_OR_COL ds) '2000-04-09') (== (TOK_TABLE_OR_COL value) 'val_5')))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (((ds < '2000-04-09') and (key < 5)) or ((ds > '2000-04-09') and (value = 'val_5')))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col2'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
174 rows selected 
>>>  
>>>  select key, value, ds from pcr_t1 
where (ds < '2000-04-09' and key < 5) or (ds > '2000-04-09' and value == 'val_5') order by key, ds;
'key','value','ds'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'2','val_2','2000-04-08'
'4','val_4','2000-04-08'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
8 rows selected 
>>>  
>>>  
>>>  explain extended 
select key, value, ds from pcr_t1 
where (ds < '2000-04-10' and key < 5) or (ds > '2000-04-08' and value == 'val_5') order by key, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (or (and (< (TOK_TABLE_OR_COL ds) '2000-04-10') (< (TOK_TABLE_OR_COL key) 5)) (and (> (TOK_TABLE_OR_COL ds) '2000-04-08') (== (TOK_TABLE_OR_COL value) 'val_5')))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (((ds < '2000-04-10') and (key < 5)) or ((ds > '2000-04-08') and (value = 'val_5')))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col2'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
225 rows selected 
>>>  
>>>  select key, value, ds from pcr_t1 
where (ds < '2000-04-10' and key < 5) or (ds > '2000-04-08' and value == 'val_5') order by key, ds;
'key','value','ds'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-08'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'2','val_2','2000-04-08'
'2','val_2','2000-04-09'
'4','val_4','2000-04-08'
'4','val_4','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
16 rows selected 
>>>  
>>>  
>>>  explain extended 
select key, value, ds from pcr_t1 
where (ds < '2000-04-10' or key < 5) and (ds > '2000-04-08' or value == 'val_5') order by key, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (and (or (< (TOK_TABLE_OR_COL ds) '2000-04-10') (< (TOK_TABLE_OR_COL key) 5)) (or (> (TOK_TABLE_OR_COL ds) '2000-04-08') (== (TOK_TABLE_OR_COL value) 'val_5')))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (((ds < '2000-04-10') or (key < 5)) and ((ds > '2000-04-08') or (value = 'val_5')))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col2'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
225 rows selected 
>>>  
>>>  select key, value, ds from pcr_t1 
where (ds < '2000-04-10' or key < 5) and (ds > '2000-04-08' or value == 'val_5') order by key, ds;
'key','value','ds'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'2','val_2','2000-04-09'
'2','val_2','2000-04-10'
'4','val_4','2000-04-09'
'4','val_4','2000-04-10'
'5','val_5','2000-04-08'
'5','val_5','2000-04-08'
'5','val_5','2000-04-08'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'8','val_8','2000-04-09'
'9','val_9','2000-04-09'
'10','val_10','2000-04-09'
'11','val_11','2000-04-09'
'12','val_12','2000-04-09'
'12','val_12','2000-04-09'
'15','val_15','2000-04-09'
'15','val_15','2000-04-09'
'17','val_17','2000-04-09'
'18','val_18','2000-04-09'
'18','val_18','2000-04-09'
'19','val_19','2000-04-09'
28 rows selected 
>>>  
>>>  
>>>  explain extended select key, value from pcr_t1 where (ds='2000-04-08' or ds='2000-04-09') and key=14 order by key, value;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (and (or (= (TOK_TABLE_OR_COL ds) '2000-04-08') (= (TOK_TABLE_OR_COL ds) '2000-04-09')) (= (TOK_TABLE_OR_COL key) 14))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key = 14)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                outputColumnNames: _col0, _col1'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                  sort order: ++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1'
'                  columns.types int:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
170 rows selected 
>>>  select key, value from pcr_t1 where (ds='2000-04-08' or ds='2000-04-09') and key=14 order by key, value;
'key','value'
No rows selected 
>>>  
>>>  explain extended select key, value from pcr_t1 where ds='2000-04-08' or ds='2000-04-09' order by key, value;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (or (= (TOK_TABLE_OR_COL ds) '2000-04-08') (= (TOK_TABLE_OR_COL ds) '2000-04-09'))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: int'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: int'
'                      expr: _col1'
'                      type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1'
'                  columns.types int:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
165 rows selected 
>>>  select key, value from pcr_t1 where ds='2000-04-08' or ds='2000-04-09' order by key, value;
'key','value'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'2','val_2'
'2','val_2'
'4','val_4'
'4','val_4'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'8','val_8'
'8','val_8'
'9','val_9'
'9','val_9'
'10','val_10'
'10','val_10'
'11','val_11'
'11','val_11'
'12','val_12'
'12','val_12'
'12','val_12'
'12','val_12'
'15','val_15'
'15','val_15'
'15','val_15'
'15','val_15'
'17','val_17'
'17','val_17'
'18','val_18'
'18','val_18'
'18','val_18'
'18','val_18'
'19','val_19'
'19','val_19'
40 rows selected 
>>>  
>>>  explain extended select key, value from pcr_t1 where ds>='2000-04-08' or ds<'2000-04-10' order by key, value;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (or (>= (TOK_TABLE_OR_COL ds) '2000-04-08') (< (TOK_TABLE_OR_COL ds) '2000-04-10'))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: int'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: int'
'                      expr: _col1'
'                      type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1'
'                  columns.types int:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
216 rows selected 
>>>  select key, value from pcr_t1 where ds>='2000-04-08' or ds<'2000-04-10' order by key, value;
'key','value'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'0','val_0'
'2','val_2'
'2','val_2'
'2','val_2'
'4','val_4'
'4','val_4'
'4','val_4'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'5','val_5'
'8','val_8'
'8','val_8'
'8','val_8'
'9','val_9'
'9','val_9'
'9','val_9'
'10','val_10'
'10','val_10'
'10','val_10'
'11','val_11'
'11','val_11'
'11','val_11'
'12','val_12'
'12','val_12'
'12','val_12'
'12','val_12'
'12','val_12'
'12','val_12'
'15','val_15'
'15','val_15'
'15','val_15'
'15','val_15'
'15','val_15'
'15','val_15'
'17','val_17'
'17','val_17'
'17','val_17'
'18','val_18'
'18','val_18'
'18','val_18'
'18','val_18'
'18','val_18'
'18','val_18'
'19','val_19'
'19','val_19'
'19','val_19'
60 rows selected 
>>>  
>>>  explain extended select key, value, ds from pcr_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (or (and (= (TOK_TABLE_OR_COL ds) '2000-04-08') (= (TOK_TABLE_OR_COL key) 1)) (and (= (TOK_TABLE_OR_COL ds) '2000-04-09') (= (TOK_TABLE_OR_COL key) 2)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (((ds = '2000-04-08') and (key = 1)) or ((ds = '2000-04-09') and (key = 2)))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                  sort order: +++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
176 rows selected 
>>>  select key, value, ds from pcr_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;
'key','value','ds'
'2','val_2','2000-04-09'
1 row selected 
>>>  
>>>  explain extended select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-08' order by t1.key;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME pcr_t1) t1) (TOK_TABREF (TOK_TABNAME pcr_t1) t2) (and (and (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL t2) key)) (= (. (TOK_TABLE_OR_COL t1) ds) '2000-04-08')) (= (. (TOK_TABLE_OR_COL t2) ds) '2000-04-08')))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (. (TOK_TABLE_OR_COL t1) key)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        t1 '
'          TableScan'
'            alias: t1'
'            GatherStats: false'
'            Reduce Output Operator'
'              key expressions:'
'                    expr: key'
'                    type: int'
'              sort order: +'
'              Map-reduce partition columns:'
'                    expr: key'
'                    type: int'
'              tag: 0'
'              value expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'                    expr: ds'
'                    type: string'
'        t2 '
'          TableScan'
'            alias: t2'
'            GatherStats: false'
'            Reduce Output Operator'
'              key expressions:'
'                    expr: key'
'                    type: int'
'              sort order: +'
'              Map-reduce partition columns:'
'                    expr: key'
'                    type: int'
'              tag: 1'
'              value expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'                    expr: ds'
'                    type: string'
'      Needs Tagging: true'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [t2, t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Join Operator'
'          condition map:'
'               Inner Join 0 to 1'
'          condition expressions:'
'            0 {VALUE._col0} {VALUE._col1} {VALUE._col2}'
'            1 {VALUE._col0} {VALUE._col1} {VALUE._col2}'
'          handleSkewJoin: false'
'          outputColumnNames: _col0, _col1, _col2, _col5, _col6, _col7'
'          Select Operator'
'            expressions:'
'                  expr: _col0'
'                  type: int'
'                  expr: _col1'
'                  type: string'
'                  expr: _col2'
'                  type: string'
'                  expr: _col5'
'                  type: int'
'                  expr: _col6'
'                  type: string'
'                  expr: _col7'
'                  type: string'
'            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: file:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'                  properties:'
'                    columns _col0,_col1,_col2,_col3,_col4,_col5'
'                    columns.types int,string,string,int,string,string'
'                    escape.delim \'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: int'
'              sort order: +'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: int'
'                    expr: _col1'
'                    type: string'
'                    expr: _col2'
'                    type: string'
'                    expr: _col3'
'                    type: int'
'                    expr: _col4'
'                    type: string'
'                    expr: _col5'
'                    type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        file:!!{hive.exec.scratchdir}!! [file:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        file:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -mr-10002'
'            input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'            properties:'
'              columns _col0,_col1,_col2,_col3,_col4,_col5'
'              columns.types int,string,string,int,string,string'
'              escape.delim \'
'          '
'              input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'              properties:'
'                columns _col0,_col1,_col2,_col3,_col4,_col5'
'                columns.types int,string,string,int,string,string'
'                escape.delim \'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2,_col3,_col4,_col5'
'                  columns.types int:string:string:int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
214 rows selected 
>>>  select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-08' order by t1.key;
'key','value','ds','key','value','ds'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'0','val_0','2000-04-08','0','val_0','2000-04-08'
'2','val_2','2000-04-08','2','val_2','2000-04-08'
'4','val_4','2000-04-08','4','val_4','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'5','val_5','2000-04-08','5','val_5','2000-04-08'
'8','val_8','2000-04-08','8','val_8','2000-04-08'
'9','val_9','2000-04-08','9','val_9','2000-04-08'
'10','val_10','2000-04-08','10','val_10','2000-04-08'
'11','val_11','2000-04-08','11','val_11','2000-04-08'
'12','val_12','2000-04-08','12','val_12','2000-04-08'
'12','val_12','2000-04-08','12','val_12','2000-04-08'
'12','val_12','2000-04-08','12','val_12','2000-04-08'
'12','val_12','2000-04-08','12','val_12','2000-04-08'
'15','val_15','2000-04-08','15','val_15','2000-04-08'
'15','val_15','2000-04-08','15','val_15','2000-04-08'
'15','val_15','2000-04-08','15','val_15','2000-04-08'
'15','val_15','2000-04-08','15','val_15','2000-04-08'
'17','val_17','2000-04-08','17','val_17','2000-04-08'
'18','val_18','2000-04-08','18','val_18','2000-04-08'
'18','val_18','2000-04-08','18','val_18','2000-04-08'
'18','val_18','2000-04-08','18','val_18','2000-04-08'
'18','val_18','2000-04-08','18','val_18','2000-04-08'
'19','val_19','2000-04-08','19','val_19','2000-04-08'
38 rows selected 
>>>  
>>>  explain extended select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-09' order by t1.key;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME pcr_t1) t1) (TOK_TABREF (TOK_TABNAME pcr_t1) t2) (and (and (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL t2) key)) (= (. (TOK_TABLE_OR_COL t1) ds) '2000-04-08')) (= (. (TOK_TABLE_OR_COL t2) ds) '2000-04-09')))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (. (TOK_TABLE_OR_COL t1) key)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        t1 '
'          TableScan'
'            alias: t1'
'            GatherStats: false'
'            Reduce Output Operator'
'              key expressions:'
'                    expr: key'
'                    type: int'
'              sort order: +'
'              Map-reduce partition columns:'
'                    expr: key'
'                    type: int'
'              tag: 0'
'              value expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'                    expr: ds'
'                    type: string'
'        t2 '
'          TableScan'
'            alias: t2'
'            GatherStats: false'
'            Reduce Output Operator'
'              key expressions:'
'                    expr: key'
'                    type: int'
'              sort order: +'
'              Map-reduce partition columns:'
'                    expr: key'
'                    type: int'
'              tag: 1'
'              value expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'                    expr: ds'
'                    type: string'
'      Needs Tagging: true'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [t2]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 3'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 3'
'                numPartitions 3'
'                numRows 60'
'                partition_columns ds'
'                rawDataSize 480'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 540'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Join Operator'
'          condition map:'
'               Inner Join 0 to 1'
'          condition expressions:'
'            0 {VALUE._col0} {VALUE._col1} {VALUE._col2}'
'            1 {VALUE._col0} {VALUE._col1} {VALUE._col2}'
'          handleSkewJoin: false'
'          outputColumnNames: _col0, _col1, _col2, _col5, _col6, _col7'
'          Select Operator'
'            expressions:'
'                  expr: _col0'
'                  type: int'
'                  expr: _col1'
'                  type: string'
'                  expr: _col2'
'                  type: string'
'                  expr: _col5'
'                  type: int'
'                  expr: _col6'
'                  type: string'
'                  expr: _col7'
'                  type: string'
'            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: file:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'                  properties:'
'                    columns _col0,_col1,_col2,_col3,_col4,_col5'
'                    columns.types int,string,string,int,string,string'
'                    escape.delim \'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: int'
'              sort order: +'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: int'
'                    expr: _col1'
'                    type: string'
'                    expr: _col2'
'                    type: string'
'                    expr: _col3'
'                    type: int'
'                    expr: _col4'
'                    type: string'
'                    expr: _col5'
'                    type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        file:!!{hive.exec.scratchdir}!! [file:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        file:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -mr-10002'
'            input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'            properties:'
'              columns _col0,_col1,_col2,_col3,_col4,_col5'
'              columns.types int,string,string,int,string,string'
'              escape.delim \'
'          '
'              input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'              properties:'
'                columns _col0,_col1,_col2,_col3,_col4,_col5'
'                columns.types int,string,string,int,string,string'
'                escape.delim \'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2,_col3,_col4,_col5'
'                  columns.types int:string:string:int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
265 rows selected 
>>>  select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-09' order by t1.key;
'key','value','ds','key','value','ds'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'0','val_0','2000-04-08','0','val_0','2000-04-09'
'2','val_2','2000-04-08','2','val_2','2000-04-09'
'4','val_4','2000-04-08','4','val_4','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'5','val_5','2000-04-08','5','val_5','2000-04-09'
'8','val_8','2000-04-08','8','val_8','2000-04-09'
'9','val_9','2000-04-08','9','val_9','2000-04-09'
'10','val_10','2000-04-08','10','val_10','2000-04-09'
'11','val_11','2000-04-08','11','val_11','2000-04-09'
'12','val_12','2000-04-08','12','val_12','2000-04-09'
'12','val_12','2000-04-08','12','val_12','2000-04-09'
'12','val_12','2000-04-08','12','val_12','2000-04-09'
'12','val_12','2000-04-08','12','val_12','2000-04-09'
'15','val_15','2000-04-08','15','val_15','2000-04-09'
'15','val_15','2000-04-08','15','val_15','2000-04-09'
'15','val_15','2000-04-08','15','val_15','2000-04-09'
'15','val_15','2000-04-08','15','val_15','2000-04-09'
'17','val_17','2000-04-08','17','val_17','2000-04-09'
'18','val_18','2000-04-08','18','val_18','2000-04-09'
'18','val_18','2000-04-08','18','val_18','2000-04-09'
'18','val_18','2000-04-08','18','val_18','2000-04-09'
'18','val_18','2000-04-08','18','val_18','2000-04-09'
'19','val_19','2000-04-08','19','val_19','2000-04-09'
38 rows selected 
>>>  
>>>  insert overwrite table pcr_t1 partition (ds='2000-04-11') select * from src where key < 20 order by key;
'_col0','_col1'
No rows selected 
>>>  
>>>  explain extended select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds>='2000-04-08' and ds<='2000-04-11' and key=2) order by key, value, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (or (and (> (TOK_TABLE_OR_COL ds) '2000-04-08') (< (TOK_TABLE_OR_COL ds) '2000-04-11')) (and (and (>= (TOK_TABLE_OR_COL ds) '2000-04-08') (<= (TOK_TABLE_OR_COL ds) '2000-04-11')) (= (TOK_TABLE_OR_COL key) 2)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (((ds > '2000-04-08') and (ds < '2000-04-11')) or (key = 2))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                  sort order: +++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-11 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-11 '
'          Partition'
'            base file name: ds=2000-04-11'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-11'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-11'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
278 rows selected 
>>>  select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds>='2000-04-08' and ds<='2000-04-11' and key=2) order by key, value, ds;
'key','value','ds'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'2','val_2','2000-04-08'
'2','val_2','2000-04-09'
'2','val_2','2000-04-10'
'2','val_2','2000-04-11'
'4','val_4','2000-04-09'
'4','val_4','2000-04-10'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'8','val_8','2000-04-09'
'8','val_8','2000-04-10'
'9','val_9','2000-04-09'
'9','val_9','2000-04-10'
'10','val_10','2000-04-09'
'10','val_10','2000-04-10'
'11','val_11','2000-04-09'
'11','val_11','2000-04-10'
'12','val_12','2000-04-09'
'12','val_12','2000-04-09'
'12','val_12','2000-04-10'
'12','val_12','2000-04-10'
'15','val_15','2000-04-09'
'15','val_15','2000-04-09'
'15','val_15','2000-04-10'
'15','val_15','2000-04-10'
'17','val_17','2000-04-09'
'17','val_17','2000-04-10'
'18','val_18','2000-04-09'
'18','val_18','2000-04-09'
'18','val_18','2000-04-10'
'18','val_18','2000-04-10'
'19','val_19','2000-04-09'
'19','val_19','2000-04-10'
42 rows selected 
>>>  
>>>  explain extended select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds<='2000-04-09' and key=2) order by key, value, ds;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds))) (TOK_WHERE (or (and (> (TOK_TABLE_OR_COL ds) '2000-04-08') (< (TOK_TABLE_OR_COL ds) '2000-04-11')) (and (<= (TOK_TABLE_OR_COL ds) '2000-04-09') (= (TOK_TABLE_OR_COL key) 2)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: ((ds > '2000-04-08') or ((ds <= '2000-04-09') and (key = 2)))'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                  sort order: +++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 [pcr_t1]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09 '
'          Partition'
'            base file name: ds=2000-04-09'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-09'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-09'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10 '
'          Partition'
'            base file name: ds=2000-04-10'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-10'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-10'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2'
'                  columns.types int:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
227 rows selected 
>>>  select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds<='2000-04-09' and key=2) order by key, value, ds;
'key','value','ds'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-09'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'0','val_0','2000-04-10'
'2','val_2','2000-04-08'
'2','val_2','2000-04-09'
'2','val_2','2000-04-10'
'4','val_4','2000-04-09'
'4','val_4','2000-04-10'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-09'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'5','val_5','2000-04-10'
'8','val_8','2000-04-09'
'8','val_8','2000-04-10'
'9','val_9','2000-04-09'
'9','val_9','2000-04-10'
'10','val_10','2000-04-09'
'10','val_10','2000-04-10'
'11','val_11','2000-04-09'
'11','val_11','2000-04-10'
'12','val_12','2000-04-09'
'12','val_12','2000-04-09'
'12','val_12','2000-04-10'
'12','val_12','2000-04-10'
'15','val_15','2000-04-09'
'15','val_15','2000-04-09'
'15','val_15','2000-04-10'
'15','val_15','2000-04-10'
'17','val_17','2000-04-09'
'17','val_17','2000-04-10'
'18','val_18','2000-04-09'
'18','val_18','2000-04-09'
'18','val_18','2000-04-10'
'18','val_18','2000-04-10'
'19','val_19','2000-04-09'
'19','val_19','2000-04-10'
41 rows selected 
>>>  
>>>  create table pcr_t2 (key int, value string);
No rows affected 
>>>  create table pcr_t3 (key int, value string);
No rows affected 
>>>  
>>>  explain extended 
from pcr_t1 
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' 
insert overwrite table pcr_t3 select key, value where ds='2000-04-08';
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME pcr_t2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '2000-04-08'))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME pcr_t3))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '2000-04-08'))))'
''
'STAGE DEPENDENCIES:'
'  Stage-2 is a root stage'
'  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6'
'  Stage-5'
'  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7'
'  Stage-3 depends on stages: Stage-0'
'  Stage-4'
'  Stage-6'
'  Stage-7 depends on stages: Stage-6'
'  Stage-14 depends on stages: Stage-2 , consists of Stage-11, Stage-10, Stage-12'
'  Stage-11'
'  Stage-1 depends on stages: Stage-11, Stage-10, Stage-13'
'  Stage-9 depends on stages: Stage-1'
'  Stage-10'
'  Stage-12'
'  Stage-13 depends on stages: Stage-12'
''
'STAGE PLANS:'
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              File Output Operator'
'                compressed: false'
'                GlobalTableId: 1'
'                directory: pfile:!!{hive.exec.scratchdir}!!'
'                NumFilesPerFileSink: 1'
'                Stats Publishing Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
'                table:'
'                    input format: org.apache.hadoop.mapred.TextInputFormat'
'                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    properties:'
'                      bucket_count -1'
'                      columns key,value'
'                      columns.types int:string'
'                      file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                      location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                      name pcr.pcr_t2'
'                      serialization.ddl struct pcr_t2 { i32 key, string value}'
'                      serialization.format 1'
'                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                      transient_lastDdlTime !!UNIXTIME!!'
'                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    name: pcr.pcr_t2'
'                TotalFiles: 1'
'                GatherStats: true'
'                MultiFileSpray: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: int'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              File Output Operator'
'                compressed: false'
'                GlobalTableId: 2'
'                directory: pfile:!!{hive.exec.scratchdir}!!'
'                NumFilesPerFileSink: 1'
'                Stats Publishing Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
'                table:'
'                    input format: org.apache.hadoop.mapred.TextInputFormat'
'                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    properties:'
'                      bucket_count -1'
'                      columns key,value'
'                      columns.types int:string'
'                      file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                      file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                      location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                      name pcr.pcr_t3'
'                      serialization.ddl struct pcr_t3 { i32 key, string value}'
'                      serialization.format 1'
'                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                      transient_lastDdlTime !!UNIXTIME!!'
'                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    name: pcr.pcr_t3'
'                TotalFiles: 1'
'                GatherStats: true'
'                MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
''
'  Stage: Stage-8'
'    Conditional Operator'
''
'  Stage: Stage-5'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-0'
'    Move Operator'
'      tables:'
'          replace: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          table:'
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'          tmp directory: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
'      Stats Aggregation Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-4'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                    name pcr.pcr_t2'
'                    serialization.ddl struct pcr_t2 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t2'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10004'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'              name pcr.pcr_t2'
'              serialization.ddl struct pcr_t2 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'            name: pcr.pcr_t2'
''
'  Stage: Stage-6'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                    name pcr.pcr_t2'
'                    serialization.ddl struct pcr_t2 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t2'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10004'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'              name pcr.pcr_t2'
'              serialization.ddl struct pcr_t2 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'            name: pcr.pcr_t2'
''
'  Stage: Stage-7'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-14'
'    Conditional Operator'
''
'  Stage: Stage-11'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-1'
'    Move Operator'
'      tables:'
'          replace: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          table:'
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'          tmp directory: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-9'
'    Stats-Aggr Operator'
'      Stats Aggregation Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-10'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                    name pcr.pcr_t3'
'                    serialization.ddl struct pcr_t3 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t3'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10005'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'              name pcr.pcr_t3'
'              serialization.ddl struct pcr_t3 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'            name: pcr.pcr_t3'
''
'  Stage: Stage-12'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                    name pcr.pcr_t3'
'                    serialization.ddl struct pcr_t3 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t3'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10005'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'              name pcr.pcr_t3'
'              serialization.ddl struct pcr_t3 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'            name: pcr.pcr_t3'
''
'  Stage: Stage-13'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
''
520 rows selected 
>>>  
>>>  from pcr_t1 
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' 
insert overwrite table pcr_t3 select key, value where ds='2000-04-08';
'key','value'
No rows selected 
>>>  
>>>  explain extended 
from pcr_t1 
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' and key=2 
insert overwrite table pcr_t3 select key, value where ds='2000-04-08' and key=3;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME pcr_t1))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME pcr_t2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '2000-04-08') (= (TOK_TABLE_OR_COL key) 2)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME pcr_t3))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '2000-04-08') (= (TOK_TABLE_OR_COL key) 3)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-2 is a root stage'
'  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6'
'  Stage-5'
'  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7'
'  Stage-3 depends on stages: Stage-0'
'  Stage-4'
'  Stage-6'
'  Stage-7 depends on stages: Stage-6'
'  Stage-14 depends on stages: Stage-2 , consists of Stage-11, Stage-10, Stage-12'
'  Stage-11'
'  Stage-1 depends on stages: Stage-11, Stage-10, Stage-13'
'  Stage-9 depends on stages: Stage-1'
'  Stage-10'
'  Stage-12'
'  Stage-13 depends on stages: Stage-12'
''
'STAGE PLANS:'
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pcr_t1 '
'          TableScan'
'            alias: pcr_t1'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key = 2)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                outputColumnNames: _col0, _col1'
'                File Output Operator'
'                  compressed: false'
'                  GlobalTableId: 1'
'                  directory: pfile:!!{hive.exec.scratchdir}!!'
'                  NumFilesPerFileSink: 1'
'                  Stats Publishing Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
'                  table:'
'                      input format: org.apache.hadoop.mapred.TextInputFormat'
'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                      properties:'
'                        bucket_count -1'
'                        columns key,value'
'                        columns.types int:string'
'                        file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                        file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                        location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                        name pcr.pcr_t2'
'                        numFiles 1'
'                        numPartitions 0'
'                        numRows 20'
'                        rawDataSize 160'
'                        serialization.ddl struct pcr_t2 { i32 key, string value}'
'                        serialization.format 1'
'                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                        totalSize 180'
'                        transient_lastDdlTime !!UNIXTIME!!'
'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                      name: pcr.pcr_t2'
'                  TotalFiles: 1'
'                  GatherStats: true'
'                  MultiFileSpray: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key = 3)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: int'
'                      expr: value'
'                      type: string'
'                outputColumnNames: _col0, _col1'
'                File Output Operator'
'                  compressed: false'
'                  GlobalTableId: 2'
'                  directory: pfile:!!{hive.exec.scratchdir}!!'
'                  NumFilesPerFileSink: 1'
'                  Stats Publishing Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
'                  table:'
'                      input format: org.apache.hadoop.mapred.TextInputFormat'
'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                      properties:'
'                        bucket_count -1'
'                        columns key,value'
'                        columns.types int:string'
'                        file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                        file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                        location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                        name pcr.pcr_t3'
'                        numFiles 1'
'                        numPartitions 0'
'                        numRows 20'
'                        rawDataSize 160'
'                        serialization.ddl struct pcr_t3 { i32 key, string value}'
'                        serialization.format 1'
'                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                        totalSize 180'
'                        transient_lastDdlTime !!UNIXTIME!!'
'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                      name: pcr.pcr_t3'
'                  TotalFiles: 1'
'                  GatherStats: true'
'                  MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 [pcr_t1]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08 '
'          Partition'
'            base file name: ds=2000-04-08'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2000-04-08'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1/ds=2000-04-08'
'              name pcr.pcr_t1'
'              numFiles 1'
'              numPartitions 4'
'              numRows 20'
'              partition_columns ds'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t1 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t1'
'                name pcr.pcr_t1'
'                numFiles 4'
'                numPartitions 4'
'                numRows 80'
'                partition_columns ds'
'                rawDataSize 640'
'                serialization.ddl struct pcr_t1 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 720'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t1'
'            name: pcr.pcr_t1'
''
'  Stage: Stage-8'
'    Conditional Operator'
''
'  Stage: Stage-5'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-0'
'    Move Operator'
'      tables:'
'          replace: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          table:'
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'          tmp directory: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
'      Stats Aggregation Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-4'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                    name pcr.pcr_t2'
'                    numFiles 1'
'                    numPartitions 0'
'                    numRows 20'
'                    rawDataSize 160'
'                    serialization.ddl struct pcr_t2 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    totalSize 180'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t2'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10004'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'              name pcr.pcr_t2'
'              numFiles 1'
'              numPartitions 0'
'              numRows 20'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t2 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'            name: pcr.pcr_t2'
''
'  Stage: Stage-6'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                    name pcr.pcr_t2'
'                    numFiles 1'
'                    numPartitions 0'
'                    numRows 20'
'                    rawDataSize 160'
'                    serialization.ddl struct pcr_t2 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    totalSize 180'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t2'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10004'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'              name pcr.pcr_t2'
'              numFiles 1'
'              numPartitions 0'
'              numRows 20'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t2 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t2'
'                name pcr.pcr_t2'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t2 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t2'
'            name: pcr.pcr_t2'
''
'  Stage: Stage-7'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-14'
'    Conditional Operator'
''
'  Stage: Stage-11'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-1'
'    Move Operator'
'      tables:'
'          replace: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          table:'
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'          tmp directory: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-9'
'    Stats-Aggr Operator'
'      Stats Aggregation Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-10'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                    name pcr.pcr_t3'
'                    numFiles 1'
'                    numPartitions 0'
'                    numRows 20'
'                    rawDataSize 160'
'                    serialization.ddl struct pcr_t3 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    totalSize 180'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t3'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10005'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'              name pcr.pcr_t3'
'              numFiles 1'
'              numPartitions 0'
'              numRows 20'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t3 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'            name: pcr.pcr_t3'
''
'  Stage: Stage-12'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    bucket_count -1'
'                    columns key,value'
'                    columns.types int:string'
'                    file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                    file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                    location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                    name pcr.pcr_t3'
'                    numFiles 1'
'                    numPartitions 0'
'                    numRows 20'
'                    rawDataSize 160'
'                    serialization.ddl struct pcr_t3 { i32 key, string value}'
'                    serialization.format 1'
'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                    totalSize 180'
'                    transient_lastDdlTime !!UNIXTIME!!'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: pcr.pcr_t3'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
'      Needs Tagging: false'
'      Path -> Alias:'
'        pfile:!!{hive.exec.scratchdir}!! [pfile:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        pfile:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -ext-10005'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types int:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'              name pcr.pcr_t3'
'              numFiles 1'
'              numPartitions 0'
'              numRows 20'
'              rawDataSize 160'
'              serialization.ddl struct pcr_t3 { i32 key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 180'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types int:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/pcr_t3'
'                name pcr.pcr_t3'
'                numFiles 1'
'                numPartitions 0'
'                numRows 20'
'                rawDataSize 160'
'                serialization.ddl struct pcr_t3 { i32 key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 180'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.pcr_t3'
'            name: pcr.pcr_t3'
''
'  Stage: Stage-13'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
''
610 rows selected 
>>>  
>>>  from pcr_t1 
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' and key=2 
insert overwrite table pcr_t3 select key, value where ds='2000-04-08' and key=3;
'key','value'
No rows selected 
>>>  
>>>  
>>>  explain extended select key, value from srcpart where ds='2008-04-08' and hr=11 order by key limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME srcpart))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '2008-04-08') (= (TOK_TABLE_OR_COL hr) 11))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key))) (TOK_LIMIT 10)))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        srcpart '
'          TableScan'
'            alias: srcpart'
'            GatherStats: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: string'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: string'
'                sort order: +'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 [srcpart]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 '
'          Partition'
'            base file name: hr=11'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2008-04-08'
'              hr 11'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11'
'              name pcr.srcpart'
'              numFiles 1'
'              numPartitions 4'
'              numRows 0'
'              partition_columns ds/hr'
'              rawDataSize 0'
'              serialization.ddl struct srcpart { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart'
'                name pcr.srcpart'
'                numFiles 4'
'                numPartitions 4'
'                numRows 0'
'                partition_columns ds/hr'
'                rawDataSize 0'
'                serialization.ddl struct srcpart { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 23248'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.srcpart'
'            name: pcr.srcpart'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: file:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    columns _col0,_col1'
'                    columns.types string:string'
'                    escape.delim \'
'                    serialization.format 1'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: 10'
''
''
114 rows selected 
>>>  select key, value from srcpart where ds='2008-04-04' and hr=11 order by key limit 10;
'key','value'
No rows selected 
>>>  
>>>  explain extended select key, value, ds, hr from srcpart where ds='2008-04-08' and (hr='11' or hr='12') and key=11 order by key, ds, hr;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME srcpart))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds)) (TOK_SELEXPR (TOK_TABLE_OR_COL hr))) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '2008-04-08') (or (= (TOK_TABLE_OR_COL hr) '11') (= (TOK_TABLE_OR_COL hr) '12'))) (= (TOK_TABLE_OR_COL key) 11))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL hr)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        srcpart '
'          TableScan'
'            alias: srcpart'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key = 11.0)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: string'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                      expr: hr'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2, _col3'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                        expr: _col3'
'                        type: string'
'                  sort order: +++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: string'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                        expr: _col3'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 [srcpart]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=12 [srcpart]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 '
'          Partition'
'            base file name: hr=11'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2008-04-08'
'              hr 11'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11'
'              name pcr.srcpart'
'              numFiles 1'
'              numPartitions 4'
'              numRows 0'
'              partition_columns ds/hr'
'              rawDataSize 0'
'              serialization.ddl struct srcpart { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart'
'                name pcr.srcpart'
'                numFiles 4'
'                numPartitions 4'
'                numRows 0'
'                partition_columns ds/hr'
'                rawDataSize 0'
'                serialization.ddl struct srcpart { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 23248'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.srcpart'
'            name: pcr.srcpart'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=12 '
'          Partition'
'            base file name: hr=12'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2008-04-08'
'              hr 12'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=12'
'              name pcr.srcpart'
'              numFiles 1'
'              numPartitions 4'
'              numRows 0'
'              partition_columns ds/hr'
'              rawDataSize 0'
'              serialization.ddl struct srcpart { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart'
'                name pcr.srcpart'
'                numFiles 4'
'                numPartitions 4'
'                numRows 0'
'                partition_columns ds/hr'
'                rawDataSize 0'
'                serialization.ddl struct srcpart { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 23248'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.srcpart'
'            name: pcr.srcpart'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2,_col3'
'                  columns.types string:string:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
182 rows selected 
>>>  select key, value, ds, hr from srcpart where ds='2008-04-08' and (hr='11' or hr='12') and key=11 order by key, ds, hr;
'key','value','ds','hr'
'11','val_11','2008-04-08','11'
'11','val_11','2008-04-08','12'
2 rows selected 
>>>  
>>>  explain extended select key, value, ds, hr from srcpart where hr='11' and key=11 order by key, ds, hr;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME srcpart))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_TABLE_OR_COL ds)) (TOK_SELEXPR (TOK_TABLE_OR_COL hr))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL hr) '11') (= (TOK_TABLE_OR_COL key) 11))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL ds)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL hr)))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-0 is a root stage'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        srcpart '
'          TableScan'
'            alias: srcpart'
'            GatherStats: false'
'            Filter Operator'
'              isSamplingPred: false'
'              predicate:'
'                  expr: (key = 11.0)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: string'
'                      expr: value'
'                      type: string'
'                      expr: ds'
'                      type: string'
'                      expr: hr'
'                      type: string'
'                outputColumnNames: _col0, _col1, _col2, _col3'
'                Reduce Output Operator'
'                  key expressions:'
'                        expr: _col0'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                        expr: _col3'
'                        type: string'
'                  sort order: +++'
'                  tag: -1'
'                  value expressions:'
'                        expr: _col0'
'                        type: string'
'                        expr: _col1'
'                        type: string'
'                        expr: _col2'
'                        type: string'
'                        expr: _col3'
'                        type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 [srcpart]'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-09/hr=11 [srcpart]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11 '
'          Partition'
'            base file name: hr=11'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2008-04-08'
'              hr 11'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-08/hr=11'
'              name pcr.srcpart'
'              numFiles 1'
'              numPartitions 4'
'              numRows 0'
'              partition_columns ds/hr'
'              rawDataSize 0'
'              serialization.ddl struct srcpart { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart'
'                name pcr.srcpart'
'                numFiles 4'
'                numPartitions 4'
'                numRows 0'
'                partition_columns ds/hr'
'                rawDataSize 0'
'                serialization.ddl struct srcpart { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 23248'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.srcpart'
'            name: pcr.srcpart'
'        !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-09/hr=11 '
'          Partition'
'            base file name: hr=11'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            partition values:'
'              ds 2008-04-09'
'              hr 11'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart/ds=2008-04-09/hr=11'
'              name pcr.srcpart'
'              numFiles 1'
'              numPartitions 4'
'              numRows 0'
'              partition_columns ds/hr'
'              rawDataSize 0'
'              serialization.ddl struct srcpart { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/pcr.db/srcpart'
'                name pcr.srcpart'
'                numFiles 4'
'                numPartitions 4'
'                numRows 0'
'                partition_columns ds/hr'
'                rawDataSize 0'
'                serialization.ddl struct srcpart { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 23248'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: pcr.srcpart'
'            name: pcr.srcpart'
'      Reduce Operator Tree:'
'        Extract'
'          File Output Operator'
'            compressed: false'
'            GlobalTableId: 0'
'            directory: file:!!{hive.exec.scratchdir}!!'
'            NumFilesPerFileSink: 1'
'            Stats Publishing Key Prefix: file:!!{hive.exec.scratchdir}!!'
'            table:'
'                input format: org.apache.hadoop.mapred.TextInputFormat'
'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                properties:'
'                  columns _col0,_col1,_col2,_col3'
'                  columns.types string:string:string:string'
'                  escape.delim \'
'                  serialization.format 1'
'            TotalFiles: 1'
'            GatherStats: false'
'            MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Fetch Operator'
'      limit: -1'
''
''
182 rows selected 
>>>  select key, value, ds, hr from srcpart where hr='11' and key=11 order by key, ds, hr;
'key','value','ds','hr'
'11','val_11','2008-04-08','11'
'11','val_11','2008-04-09','11'
2 rows selected 
>>>  
>>>  drop table pcr_t1;
No rows affected 
>>>  drop table pcr_t2;
No rows affected 
>>>  drop table pcr_t3;
No rows affected 
>>>  !record
