Saving all output to "!!{outputDirectory}!!/input11.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/input11.q
>>>  CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
No rows affected 
>>>  
>>>  EXPLAIN 
FROM src 
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) value))) (TOK_WHERE (< (. (TOK_TABLE_OR_COL src) key) 100))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5'
'  Stage-4'
'  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6'
'  Stage-2 depends on stages: Stage-0'
'  Stage-3'
'  Stage-5'
'  Stage-6 depends on stages: Stage-5'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            Filter Operator'
'              predicate:'
'                  expr: (key < 100.0)'
'                  type: boolean'
'              Select Operator'
'                expressions:'
'                      expr: key'
'                      type: string'
'                      expr: value'
'                      type: string'
'                outputColumnNames: _col0, _col1'
'                Select Operator'
'                  expressions:'
'                        expr: UDFToInteger(_col0)'
'                        type: int'
'                        expr: _col1'
'                        type: string'
'                  outputColumnNames: _col0, _col1'
'                  File Output Operator'
'                    compressed: false'
'                    GlobalTableId: 1'
'                    table:'
'                        input format: org.apache.hadoop.mapred.TextInputFormat'
'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                        name: input11.dest1'
''
'  Stage: Stage-7'
'    Conditional Operator'
''
'  Stage: Stage-4'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
'  Stage: Stage-0'
'    Move Operator'
'      tables:'
'          replace: true'
'          table:'
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: input11.dest1'
''
'  Stage: Stage-2'
'    Stats-Aggr Operator'
''
'  Stage: Stage-3'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: input11.dest1'
''
'  Stage: Stage-5'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        pfile:!!{hive.exec.scratchdir}!! '
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                  name: input11.dest1'
''
'  Stage: Stage-6'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: pfile:!!{hive.exec.scratchdir}!!'
''
''
102 rows selected 
>>>  
>>>  FROM src 
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100;
'_col0','_col1'
No rows selected 
>>>  
>>>  SELECT dest1.* FROM dest1;
'key','value'
'86','val_86'
'27','val_27'
'98','val_98'
'66','val_66'
'37','val_37'
'15','val_15'
'82','val_82'
'17','val_17'
'0','val_0'
'57','val_57'
'20','val_20'
'92','val_92'
'47','val_47'
'72','val_72'
'4','val_4'
'35','val_35'
'54','val_54'
'51','val_51'
'65','val_65'
'83','val_83'
'12','val_12'
'67','val_67'
'84','val_84'
'58','val_58'
'8','val_8'
'24','val_24'
'42','val_42'
'0','val_0'
'96','val_96'
'26','val_26'
'51','val_51'
'43','val_43'
'95','val_95'
'98','val_98'
'85','val_85'
'77','val_77'
'0','val_0'
'87','val_87'
'15','val_15'
'72','val_72'
'90','val_90'
'19','val_19'
'10','val_10'
'5','val_5'
'58','val_58'
'35','val_35'
'95','val_95'
'11','val_11'
'34','val_34'
'42','val_42'
'78','val_78'
'76','val_76'
'41','val_41'
'30','val_30'
'64','val_64'
'76','val_76'
'74','val_74'
'69','val_69'
'33','val_33'
'70','val_70'
'5','val_5'
'2','val_2'
'35','val_35'
'80','val_80'
'44','val_44'
'53','val_53'
'90','val_90'
'12','val_12'
'5','val_5'
'70','val_70'
'24','val_24'
'70','val_70'
'83','val_83'
'26','val_26'
'67','val_67'
'18','val_18'
'9','val_9'
'18','val_18'
'97','val_97'
'84','val_84'
'28','val_28'
'37','val_37'
'90','val_90'
'97','val_97'
84 rows selected 
>>>  !record
