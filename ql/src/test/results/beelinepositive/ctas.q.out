Saving all output to "!!{outputDirectory}!!/ctas.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/ctas.q
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  create table nzhang_Tmp(a int, b string);
No rows affected 
>>>  select * from nzhang_Tmp;
'a','b'
No rows selected 
>>>  
>>>  explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_CTAS1) TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key) k) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL k)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 depends on stages: Stage-2'
'  Stage-4 depends on stages: Stage-0'
'  Stage-3 depends on stages: Stage-4'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: string'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'              sort order: ++'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  name: ctas.nzhang_CTAS1'
''
'  Stage: Stage-0'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: !!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas1'
''
'  Stage: Stage-4'
'      Create Table Operator:'
'        Create Table'
'          columns: k string, value string'
'          if not exists: false'
'          input format: org.apache.hadoop.mapred.TextInputFormat'
'          # buckets: -1'
'          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
'          name: nzhang_CTAS1'
'          isExternal: false'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
''
''
96 rows selected 
>>>  
>>>  create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;
'k','value'
No rows selected 
>>>  
>>>  select * from nzhang_CTAS1;
'k','value'
'0','val_0'
'0','val_0'
'0','val_0'
'10','val_10'
'100','val_100'
'100','val_100'
'103','val_103'
'103','val_103'
'104','val_104'
'104','val_104'
10 rows selected 
>>>  
>>>  describe formatted nzhang_CTAS1;
'col_name','data_type','comment'
'# col_name            ','data_type           ','comment             '
'','',''
'k                   ','string              ','None                '
'value               ','string              ','None                '
'','',''
'# Detailed Table Information','',''
'Database:           ','ctas                ',''
'Owner:              ','!!{user.name}!!                ',''
'CreateTime:         ','!!TIMESTAMP!!',''
'LastAccessTime:     ','UNKNOWN             ',''
'Protect Mode:       ','None                ',''
'Retention:          ','0                   ',''
'Location:           ','!!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas1',''
'Table Type:         ','MANAGED_TABLE       ',''
'Table Parameters:','',''
'','numFiles            ','1                   '
'','numPartitions       ','0                   '
'','numRows             ','10                  '
'','rawDataSize         ','96                  '
'','totalSize           ','106                 '
'','transient_lastDdlTime','!!UNIXTIME!!          '
'','',''
'# Storage Information','',''
'SerDe Library:      ','org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',''
'InputFormat:        ','org.apache.hadoop.mapred.TextInputFormat',''
'OutputFormat:       ','org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',''
'Compressed:         ','No                  ',''
'Num Buckets:        ','-1                  ',''
'Bucket Columns:     ','[]                  ',''
'Sort Columns:       ','[]                  ',''
'Storage Desc Params:','',''
'','serialization.format','1                   '
32 rows selected 
>>>  
>>>  
>>>  explain create table nzhang_ctas2 as select * from src sort by key, value limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_ctas2) TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 depends on stages: Stage-2'
'  Stage-4 depends on stages: Stage-0'
'  Stage-3 depends on stages: Stage-4'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: string'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'              sort order: ++'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  name: ctas.nzhang_ctas2'
''
'  Stage: Stage-0'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: !!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas2'
''
'  Stage: Stage-4'
'      Create Table Operator:'
'        Create Table'
'          columns: key string, value string'
'          if not exists: false'
'          input format: org.apache.hadoop.mapred.TextInputFormat'
'          # buckets: -1'
'          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
'          name: nzhang_ctas2'
'          isExternal: false'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
''
''
96 rows selected 
>>>  
>>>  create table nzhang_ctas2 as select * from src sort by key, value limit 10;
'key','value'
No rows selected 
>>>  
>>>  select * from nzhang_ctas2;
'key','value'
'0','val_0'
'0','val_0'
'0','val_0'
'10','val_10'
'100','val_100'
'100','val_100'
'103','val_103'
'103','val_103'
'104','val_104'
'104','val_104'
10 rows selected 
>>>  
>>>  describe formatted nzhang_CTAS2;
'col_name','data_type','comment'
'# col_name            ','data_type           ','comment             '
'','',''
'key                 ','string              ','None                '
'value               ','string              ','None                '
'','',''
'# Detailed Table Information','',''
'Database:           ','ctas                ',''
'Owner:              ','!!{user.name}!!                ',''
'CreateTime:         ','!!TIMESTAMP!!',''
'LastAccessTime:     ','UNKNOWN             ',''
'Protect Mode:       ','None                ',''
'Retention:          ','0                   ',''
'Location:           ','!!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas2',''
'Table Type:         ','MANAGED_TABLE       ',''
'Table Parameters:','',''
'','numFiles            ','1                   '
'','numPartitions       ','0                   '
'','numRows             ','10                  '
'','rawDataSize         ','96                  '
'','totalSize           ','106                 '
'','transient_lastDdlTime','!!UNIXTIME!!          '
'','',''
'# Storage Information','',''
'SerDe Library:      ','org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',''
'InputFormat:        ','org.apache.hadoop.mapred.TextInputFormat',''
'OutputFormat:       ','org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',''
'Compressed:         ','No                  ',''
'Num Buckets:        ','-1                  ',''
'Bucket Columns:     ','[]                  ',''
'Sort Columns:       ','[]                  ',''
'Storage Desc Params:','',''
'','serialization.format','1                   '
32 rows selected 
>>>  
>>>  
>>>  explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_ctas3) TOK_LIKETABLE (TOK_TABLESERIALIZER (TOK_SERDENAME "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe")) TOK_TBLRCFILE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (/ (TOK_TABLE_OR_COL key) 2) half_key) (TOK_SELEXPR (TOK_FUNCTION concat (TOK_TABLE_OR_COL value) "_con") conb)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL half_key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL conb))) (TOK_LIMIT 10))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 depends on stages: Stage-2'
'  Stage-4 depends on stages: Stage-0'
'  Stage-3 depends on stages: Stage-4'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            Select Operator'
'              expressions:'
'                    expr: (key / 2)'
'                    type: double'
'                    expr: concat(value, '_con')'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: double'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: double'
'                      expr: _col1'
'                      type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: double'
'                    expr: _col1'
'                    type: string'
'              sort order: ++'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: double'
'                    expr: _col1'
'                    type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 1'
'              table:'
'                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'                  name: ctas.nzhang_ctas3'
''
'  Stage: Stage-0'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: !!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas3'
''
'  Stage: Stage-4'
'      Create Table Operator:'
'        Create Table'
'          columns: half_key double, conb string'
'          if not exists: false'
'          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat'
'          # buckets: -1'
'          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
'          serde name: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
'          name: nzhang_ctas3'
'          isExternal: false'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
''
''
97 rows selected 
>>>  
>>>  create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;
'half_key','conb'
No rows selected 
>>>  
>>>  select * from nzhang_ctas3;
'half_key','conb'
'0.0','val_0_con'
'0.0','val_0_con'
'0.0','val_0_con'
'1.0','val_2_con'
'2.0','val_4_con'
'2.5','val_5_con'
'2.5','val_5_con'
'2.5','val_5_con'
'4.0','val_8_con'
'4.5','val_9_con'
10 rows selected 
>>>  
>>>  describe formatted nzhang_CTAS3;
'col_name','data_type','comment'
'# col_name            ','data_type           ','comment             '
'','',''
'half_key            ','double              ','None                '
'conb                ','string              ','None                '
'','',''
'# Detailed Table Information','',''
'Database:           ','ctas                ',''
'Owner:              ','!!{user.name}!!                ',''
'CreateTime:         ','!!TIMESTAMP!!',''
'LastAccessTime:     ','UNKNOWN             ',''
'Protect Mode:       ','None                ',''
'Retention:          ','0                   ',''
'Location:           ','!!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas3',''
'Table Type:         ','MANAGED_TABLE       ',''
'Table Parameters:','',''
'','numFiles            ','1                   '
'','numPartitions       ','0                   '
'','numRows             ','10                  '
'','rawDataSize         ','120                 '
'','totalSize           ','199                 '
'','transient_lastDdlTime','!!UNIXTIME!!          '
'','',''
'# Storage Information','',''
'SerDe Library:      ','org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe',''
'InputFormat:        ','org.apache.hadoop.hive.ql.io.RCFileInputFormat',''
'OutputFormat:       ','org.apache.hadoop.hive.ql.io.RCFileOutputFormat',''
'Compressed:         ','No                  ',''
'Num Buckets:        ','-1                  ',''
'Bucket Columns:     ','[]                  ',''
'Sort Columns:       ','[]                  ',''
'Storage Desc Params:','',''
'','serialization.format','1                   '
32 rows selected 
>>>  
>>>  
>>>  explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_ctas3) TOK_IFNOTEXISTS TOK_LIKETABLE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 2))))'
''
'STAGE DEPENDENCIES:'
''
'STAGE PLANS:'
'STAGE PLANS:'
7 rows selected 
>>>  
>>>  create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;
No rows affected 
>>>  
>>>  select * from nzhang_ctas3;
'half_key','conb'
'0.0','val_0_con'
'0.0','val_0_con'
'0.0','val_0_con'
'1.0','val_2_con'
'2.0','val_4_con'
'2.5','val_5_con'
'2.5','val_5_con'
'2.5','val_5_con'
'4.0','val_8_con'
'4.5','val_9_con'
10 rows selected 
>>>  
>>>  describe formatted nzhang_CTAS3;
'col_name','data_type','comment'
'# col_name            ','data_type           ','comment             '
'','',''
'half_key            ','double              ','None                '
'conb                ','string              ','None                '
'','',''
'# Detailed Table Information','',''
'Database:           ','ctas                ',''
'Owner:              ','!!{user.name}!!                ',''
'CreateTime:         ','!!TIMESTAMP!!',''
'LastAccessTime:     ','UNKNOWN             ',''
'Protect Mode:       ','None                ',''
'Retention:          ','0                   ',''
'Location:           ','!!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas3',''
'Table Type:         ','MANAGED_TABLE       ',''
'Table Parameters:','',''
'','numFiles            ','1                   '
'','numPartitions       ','0                   '
'','numRows             ','10                  '
'','rawDataSize         ','120                 '
'','totalSize           ','199                 '
'','transient_lastDdlTime','!!UNIXTIME!!          '
'','',''
'# Storage Information','',''
'SerDe Library:      ','org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe',''
'InputFormat:        ','org.apache.hadoop.hive.ql.io.RCFileInputFormat',''
'OutputFormat:       ','org.apache.hadoop.hive.ql.io.RCFileOutputFormat',''
'Compressed:         ','No                  ',''
'Num Buckets:        ','-1                  ',''
'Bucket Columns:     ','[]                  ',''
'Sort Columns:       ','[]                  ',''
'Storage Desc Params:','',''
'','serialization.format','1                   '
32 rows selected 
>>>  
>>>  
>>>  explain create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_ctas4) TOK_LIKETABLE (TOK_TABLEROWFORMAT (TOK_SERDEPROPS (TOK_TABLEROWFORMATFIELD ','))) TOK_TBLTEXTFILE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 depends on stages: Stage-2'
'  Stage-4 depends on stages: Stage-0'
'  Stage-3 depends on stages: Stage-4'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: string'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'              sort order: ++'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  name: ctas.nzhang_ctas4'
''
'  Stage: Stage-0'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          destination: !!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas4'
''
'  Stage: Stage-4'
'      Create Table Operator:'
'        Create Table'
'          columns: key string, value string'
'          field delimiter: ,'
'          if not exists: false'
'          input format: org.apache.hadoop.mapred.TextInputFormat'
'          # buckets: -1'
'          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
'          name: nzhang_ctas4'
'          isExternal: false'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
''
''
97 rows selected 
>>>  
>>>  create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;
'key','value'
No rows selected 
>>>  
>>>  select * from nzhang_ctas4;
'key','value'
'0','val_0'
'0','val_0'
'0','val_0'
'10','val_10'
'100','val_100'
'100','val_100'
'103','val_103'
'103','val_103'
'104','val_104'
'104','val_104'
10 rows selected 
>>>  
>>>  describe formatted nzhang_CTAS4;
'col_name','data_type','comment'
'# col_name            ','data_type           ','comment             '
'','',''
'key                 ','string              ','None                '
'value               ','string              ','None                '
'','',''
'# Detailed Table Information','',''
'Database:           ','ctas                ',''
'Owner:              ','!!{user.name}!!                ',''
'CreateTime:         ','!!TIMESTAMP!!',''
'LastAccessTime:     ','UNKNOWN             ',''
'Protect Mode:       ','None                ',''
'Retention:          ','0                   ',''
'Location:           ','!!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas4',''
'Table Type:         ','MANAGED_TABLE       ',''
'Table Parameters:','',''
'','numFiles            ','1                   '
'','numPartitions       ','0                   '
'','numRows             ','10                  '
'','rawDataSize         ','96                  '
'','totalSize           ','106                 '
'','transient_lastDdlTime','!!UNIXTIME!!          '
'','',''
'# Storage Information','',''
'SerDe Library:      ','org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',''
'InputFormat:        ','org.apache.hadoop.mapred.TextInputFormat',''
'OutputFormat:       ','org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',''
'Compressed:         ','No                  ',''
'Num Buckets:        ','-1                  ',''
'Bucket Columns:     ','[]                  ',''
'Sort Columns:       ','[]                  ',''
'Storage Desc Params:','',''
'','field.delim         ',',                   '
'','serialization.format',',                   '
33 rows selected 
>>>  
>>>  explain extended create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_CREATETABLE (TOK_TABNAME nzhang_ctas5) TOK_LIKETABLE (TOK_TABLEROWFORMAT (TOK_SERDEPROPS (TOK_TABLEROWFORMATFIELD ',') (TOK_TABLEROWFORMATLINES '\012'))) TOK_TBLTEXTFILE (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 10))))'
''
'STAGE DEPENDENCIES:'
'  Stage-1 is a root stage'
'  Stage-2 depends on stages: Stage-1'
'  Stage-0 depends on stages: Stage-2'
'  Stage-4 depends on stages: Stage-0'
'  Stage-3 depends on stages: Stage-4'
''
'STAGE PLANS:'
'  Stage: Stage-1'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        src '
'          TableScan'
'            alias: src'
'            GatherStats: false'
'            Select Operator'
'              expressions:'
'                    expr: key'
'                    type: string'
'                    expr: value'
'                    type: string'
'              outputColumnNames: _col0, _col1'
'              Reduce Output Operator'
'                key expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'                sort order: ++'
'                tag: -1'
'                value expressions:'
'                      expr: _col0'
'                      type: string'
'                      expr: _col1'
'                      type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        !!{hive.metastore.warehouse.dir}!!/ctas.db/src [src]'
'      Path -> Partition:'
'        !!{hive.metastore.warehouse.dir}!!/ctas.db/src '
'          Partition'
'            base file name: src'
'            input format: org.apache.hadoop.mapred.TextInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'            properties:'
'              bucket_count -1'
'              columns key,value'
'              columns.types string:string'
'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              location !!{hive.metastore.warehouse.dir}!!/ctas.db/src'
'              name ctas.src'
'              numFiles 1'
'              numPartitions 0'
'              numRows 0'
'              rawDataSize 0'
'              serialization.ddl struct src { string key, string value}'
'              serialization.format 1'
'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              totalSize 5812'
'              transient_lastDdlTime !!UNIXTIME!!'
'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'          '
'              input format: org.apache.hadoop.mapred.TextInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'              properties:'
'                bucket_count -1'
'                columns key,value'
'                columns.types string:string'
'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                location !!{hive.metastore.warehouse.dir}!!/ctas.db/src'
'                name ctas.src'
'                numFiles 1'
'                numPartitions 0'
'                numRows 0'
'                rawDataSize 0'
'                serialization.ddl struct src { string key, string value}'
'                serialization.format 1'
'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'                totalSize 5812'
'                transient_lastDdlTime !!UNIXTIME!!'
'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
'              name: ctas.src'
'            name: ctas.src'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 0'
'              directory: file:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              table:'
'                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'                  properties:'
'                    columns _col0,_col1'
'                    columns.types string,string'
'                    escape.delim \'
'              TotalFiles: 1'
'              GatherStats: false'
'              MultiFileSpray: false'
''
'  Stage: Stage-2'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        file:!!{hive.exec.scratchdir}!! '
'            Reduce Output Operator'
'              key expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'              sort order: ++'
'              tag: -1'
'              value expressions:'
'                    expr: _col0'
'                    type: string'
'                    expr: _col1'
'                    type: string'
'      Needs Tagging: false'
'      Path -> Alias:'
'        file:!!{hive.exec.scratchdir}!! [file:!!{hive.exec.scratchdir}!!]'
'      Path -> Partition:'
'        file:!!{hive.exec.scratchdir}!! '
'          Partition'
'            base file name: -mr-10002'
'            input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'            properties:'
'              columns _col0,_col1'
'              columns.types string,string'
'              escape.delim \'
'          '
'              input format: org.apache.hadoop.mapred.SequenceFileInputFormat'
'              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
'              properties:'
'                columns _col0,_col1'
'                columns.types string,string'
'                escape.delim \'
'      Reduce Operator Tree:'
'        Extract'
'          Limit'
'            File Output Operator'
'              compressed: false'
'              GlobalTableId: 1'
'              directory: pfile:!!{hive.exec.scratchdir}!!'
'              NumFilesPerFileSink: 1'
'              Stats Publishing Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
'              table:'
'                  input format: org.apache.hadoop.mapred.TextInputFormat'
'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'                  properties:'
'                    columns _col0,_col1'
'                    columns.types string:string'
'                    field.delim ,'
'                    line.delim '
''
'                    name ctas.nzhang_ctas5'
'                    serialization.format ,'
'                  name: ctas.nzhang_ctas5'
'              TotalFiles: 1'
'              GatherStats: true'
'              MultiFileSpray: false'
''
'  Stage: Stage-0'
'    Move Operator'
'      files:'
'          hdfs directory: true'
'          source: pfile:!!{hive.exec.scratchdir}!!'
'          destination: !!{hive.metastore.warehouse.dir}!!/ctas.db/nzhang_ctas5'
''
'  Stage: Stage-4'
'      Create Table Operator:'
'        Create Table'
'          columns: key string, value string'
'          field delimiter: ,'
'          if not exists: false'
'          input format: org.apache.hadoop.mapred.TextInputFormat'
'          line delimiter: '
''
'          # buckets: -1'
'          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
'          name: nzhang_ctas5'
'          isExternal: false'
''
'  Stage: Stage-3'
'    Stats-Aggr Operator'
'      Stats Aggregation Key Prefix: pfile:!!{hive.exec.scratchdir}!!'
''
''
195 rows selected 
>>>  
>>>  set mapred.job.tracker=does.notexist.com:666;
No rows affected 
>>>  set hive.exec.mode.local.auto=true;
No rows affected 
>>>  
>>>  create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;
'key','value'
No rows selected 
>>>  
>>>  create table nzhang_ctas6 (key string, `to` string);
No rows affected 
>>>  insert overwrite table nzhang_ctas6 select key, value from src limit 10;
'key','value'
No rows selected 
>>>  create table nzhang_ctas7 as select key, `to` from nzhang_ctas6;
'key','to'
No rows selected 
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  
>>>  !record
