Saving all output to "!!{outputDirectory}!!/load_fs.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/load_fs.q
>>>  
>>>  create table load_overwrite (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/load_overwrite';
No rows affected 
>>>  create table load_overwrite2 (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/load2_overwrite2';
No rows affected 
>>>  
>>>  load data local inpath '../data/files/kv1.txt' into table load_overwrite;
No rows affected 
>>>  load data local inpath '../data/files/kv2.txt' into table load_overwrite;
No rows affected 
>>>  load data local inpath '../data/files/kv3.txt' into table load_overwrite;
No rows affected 
>>>  
>>>  show table extended like load_overwrite;
'tab_name'
'tableName:load_overwrite'
'owner:!!{user.name}!!'
'location:file:!!{hive.root}!!/build/ql/tmp/load_overwrite'
'inputformat:org.apache.hadoop.mapred.TextInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'columns:struct columns { string key, string value}'
'partitioned:false'
'partitionColumns:'
'totalNumberFiles:3'
'totalFileSize:11819'
'maxFileSize:5812'
'minFileSize:216'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended load_overwrite;
'col_name','data_type','comment'
'key','string',''
'value','string',''
'','',''
'Detailed Table Information','Table(tableName:load_overwrite, dbName:load_fs, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:!!{hive.root}!!/build/ql/tmp/load_overwrite, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{numPartitions=0, numFiles=0, transient_lastDdlTime=!!UNIXTIME!!, numRows=0, totalSize=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
4 rows selected 
>>>  select count(*) from load_overwrite;
'_c0'
'1025'
1 row selected 
>>>  
>>>  load data inpath '${system:test.tmp.dir}/load_overwrite/kv*.txt' overwrite into table load_overwrite2;
No rows affected 
>>>  
>>>  show table extended like load_overwrite2;
'tab_name'
'tableName:load_overwrite2'
'owner:!!{user.name}!!'
'location:file:!!{hive.root}!!/build/ql/tmp/load2_overwrite2'
'inputformat:org.apache.hadoop.mapred.TextInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'columns:struct columns { string key, string value}'
'partitioned:false'
'partitionColumns:'
'totalNumberFiles:3'
'totalFileSize:11819'
'maxFileSize:5812'
'minFileSize:216'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  desc extended load_overwrite2;
'col_name','data_type','comment'
'key','string',''
'value','string',''
'','',''
'Detailed Table Information','Table(tableName:load_overwrite2, dbName:load_fs, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:!!{hive.root}!!/build/ql/tmp/load2_overwrite2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{numPartitions=0, numFiles=0, transient_lastDdlTime=!!UNIXTIME!!, numRows=0, totalSize=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
4 rows selected 
>>>  select count(*) from load_overwrite2;
'_c0'
'1025'
1 row selected 
>>>  
>>>  load data inpath '${system:test.tmp.dir}/load2_*' overwrite into table load_overwrite;
No rows affected 
>>>  show table extended like load_overwrite;
'tab_name'
'tableName:load_overwrite'
'owner:!!{user.name}!!'
'location:file:!!{hive.root}!!/build/ql/tmp/load_overwrite'
'inputformat:org.apache.hadoop.mapred.TextInputFormat'
'outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
'columns:struct columns { string key, string value}'
'partitioned:false'
'partitionColumns:'
'totalNumberFiles:3'
'totalFileSize:11819'
'maxFileSize:5812'
'minFileSize:216'
'lastAccessTime:0'
'lastUpdateTime:!!UNIXTIMEMILLIS!!'
''
15 rows selected 
>>>  select count(*) from load_overwrite;
'_c0'
'1025'
1 row selected 
>>>  !record
