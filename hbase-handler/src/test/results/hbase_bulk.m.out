PREHOOK: query: drop table hbsort
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table hbsort
POSTHOOK: type: DROPTABLE
PREHOOK: query: drop table hbpartition
PREHOOK: type: DROPTABLE
POSTHOOK: query: drop table hbpartition
POSTHOOK: type: DROPTABLE
PREHOOK: query: -- this is a dummy table used for controlling how the HFiles are
-- created
create table hbsort(key string, val string, val2 string)
stored as
INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf')
PREHOOK: type: CREATETABLE
POSTHOOK: query: -- this is a dummy table used for controlling how the HFiles are
-- created
create table hbsort(key string, val string, val2 string)
stored as
INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@hbsort
PREHOOK: query: -- this is a dummy table used for controlling how the input file
-- for TotalOrderPartitioner is created
create external table hbpartition(part_break string)
row format serde 
'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
stored as 
inputformat 
'org.apache.hadoop.mapred.TextInputFormat'
outputformat 
'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
location '/tmp/hbpartitions'
PREHOOK: type: CREATETABLE
POSTHOOK: query: -- this is a dummy table used for controlling how the input file
-- for TotalOrderPartitioner is created
create external table hbpartition(part_break string)
row format serde 
'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
stored as 
inputformat 
'org.apache.hadoop.mapred.TextInputFormat'
outputformat 
'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
location '/tmp/hbpartitions'
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: default@hbpartition
PREHOOK: query: -- this should produce one file in /tmp/hbpartitions, but we do not
-- know what it will be called, so we will copy it to a well known
-- filename /tmp/hbpartition.lst
insert overwrite table hbpartition
select distinct value
from src
where value='val_100' or value='val_200'
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@hbpartition
POSTHOOK: query: -- this should produce one file in /tmp/hbpartitions, but we do not
-- know what it will be called, so we will copy it to a well known
-- filename /tmp/hbpartition.lst
insert overwrite table hbpartition
select distinct value
from src
where value='val_100' or value='val_200'
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@hbpartition
POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
           1            1                139 hdfs://localhost.localdomain:33778/tmp/hbpartitions
PREHOOK: query: -- this should produce three files in /tmp/hbsort/cf
-- include some trailing blanks and nulls to make sure we handle them correctly
insert overwrite table hbsort
select distinct value,
  case when key=103 then cast(null as string) else key end,
  case when key=103 then ''
       else cast(key+1 as string) end
from src
cluster by value
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: default@hbsort
POSTHOOK: query: -- this should produce three files in /tmp/hbsort/cf
-- include some trailing blanks and nulls to make sure we handle them correctly
insert overwrite table hbsort
select distinct value,
  case when key=103 then cast(null as string) else key end,
  case when key=103 then ''
       else cast(key+1 as string) end
from src
cluster by value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@src
POSTHOOK: Output: default@hbsort
POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
           1            3              23227 hdfs://localhost.localdomain:33778/tmp/hbsort/cf
PREHOOK: query: -- To get the files out to your local filesystem for loading into
-- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
-- semicolon-terminate the line below before running this test:
-- dfs -copyToLocal /tmp/hbsort/cf/* /tmp/blah/cf

drop table hbsort
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@hbsort
PREHOOK: Output: default@hbsort
POSTHOOK: query: -- To get the files out to your local filesystem for loading into
-- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
-- semicolon-terminate the line below before running this test:
-- dfs -copyToLocal /tmp/hbsort/cf/* /tmp/blah/cf

drop table hbsort
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@hbsort
POSTHOOK: Output: default@hbsort
POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
PREHOOK: query: drop table hbpartition
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@hbpartition
PREHOOK: Output: default@hbpartition
POSTHOOK: query: drop table hbpartition
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@hbpartition
POSTHOOK: Output: default@hbpartition
POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
