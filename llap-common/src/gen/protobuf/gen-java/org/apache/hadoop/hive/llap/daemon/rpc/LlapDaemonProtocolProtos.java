// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: LlapDaemonProtocol.proto

package org.apache.hadoop.hive.llap.daemon.rpc;

public final class LlapDaemonProtocolProtos {
  private LlapDaemonProtocolProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * Protobuf enum {@code SourceStateProto}
   */
  public enum SourceStateProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>S_SUCCEEDED = 1;</code>
     */
    S_SUCCEEDED(1),
    /**
     * <code>S_RUNNING = 2;</code>
     */
    S_RUNNING(2),
    ;

    /**
     * <code>S_SUCCEEDED = 1;</code>
     */
    public static final int S_SUCCEEDED_VALUE = 1;
    /**
     * <code>S_RUNNING = 2;</code>
     */
    public static final int S_RUNNING_VALUE = 2;


    public final int getNumber() {
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static SourceStateProto valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static SourceStateProto forNumber(int value) {
      switch (value) {
        case 1: return S_SUCCEEDED;
        case 2: return S_RUNNING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<SourceStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        SourceStateProto> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<SourceStateProto>() {
            public SourceStateProto findValueByNumber(int number) {
              return SourceStateProto.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final SourceStateProto[] VALUES = values();

    public static SourceStateProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private SourceStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:SourceStateProto)
  }

  /**
   * Protobuf enum {@code SubmissionStateProto}
   */
  public enum SubmissionStateProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>ACCEPTED = 1;</code>
     */
    ACCEPTED(1),
    /**
     * <code>REJECTED = 2;</code>
     */
    REJECTED(2),
    /**
     * <code>EVICTED_OTHER = 3;</code>
     */
    EVICTED_OTHER(3),
    ;

    /**
     * <code>ACCEPTED = 1;</code>
     */
    public static final int ACCEPTED_VALUE = 1;
    /**
     * <code>REJECTED = 2;</code>
     */
    public static final int REJECTED_VALUE = 2;
    /**
     * <code>EVICTED_OTHER = 3;</code>
     */
    public static final int EVICTED_OTHER_VALUE = 3;


    public final int getNumber() {
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static SubmissionStateProto valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static SubmissionStateProto forNumber(int value) {
      switch (value) {
        case 1: return ACCEPTED;
        case 2: return REJECTED;
        case 3: return EVICTED_OTHER;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<SubmissionStateProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        SubmissionStateProto> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<SubmissionStateProto>() {
            public SubmissionStateProto findValueByNumber(int number) {
              return SubmissionStateProto.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.getDescriptor().getEnumTypes().get(1);
    }

    private static final SubmissionStateProto[] VALUES = values();

    public static SubmissionStateProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private SubmissionStateProto(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:SubmissionStateProto)
  }

  public interface UserPayloadProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:UserPayloadProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional bytes user_payload = 1;</code>
     * @return Whether the userPayload field is set.
     */
    boolean hasUserPayload();
    /**
     * <code>optional bytes user_payload = 1;</code>
     * @return The userPayload.
     */
    com.google.protobuf.ByteString getUserPayload();

    /**
     * <code>optional int32 version = 2;</code>
     * @return Whether the version field is set.
     */
    boolean hasVersion();
    /**
     * <code>optional int32 version = 2;</code>
     * @return The version.
     */
    int getVersion();
  }
  /**
   * Protobuf type {@code UserPayloadProto}
   */
  public static final class UserPayloadProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:UserPayloadProto)
      UserPayloadProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use UserPayloadProto.newBuilder() to construct.
    private UserPayloadProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private UserPayloadProto() {
      userPayload_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new UserPayloadProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private UserPayloadProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              bitField0_ |= 0x00000001;
              userPayload_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              version_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder.class);
    }

    private int bitField0_;
    public static final int USER_PAYLOAD_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString userPayload_;
    /**
     * <code>optional bytes user_payload = 1;</code>
     * @return Whether the userPayload field is set.
     */
    @java.lang.Override
    public boolean hasUserPayload() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bytes user_payload = 1;</code>
     * @return The userPayload.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getUserPayload() {
      return userPayload_;
    }

    public static final int VERSION_FIELD_NUMBER = 2;
    private int version_;
    /**
     * <code>optional int32 version = 2;</code>
     * @return Whether the version field is set.
     */
    @java.lang.Override
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 version = 2;</code>
     * @return The version.
     */
    @java.lang.Override
    public int getVersion() {
      return version_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(1, userPayload_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, version_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, userPayload_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, version_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) obj;

      if (hasUserPayload() != other.hasUserPayload()) return false;
      if (hasUserPayload()) {
        if (!getUserPayload()
            .equals(other.getUserPayload())) return false;
      }
      if (hasVersion() != other.hasVersion()) return false;
      if (hasVersion()) {
        if (getVersion()
            != other.getVersion()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasUserPayload()) {
        hash = (37 * hash) + USER_PAYLOAD_FIELD_NUMBER;
        hash = (53 * hash) + getUserPayload().hashCode();
      }
      if (hasVersion()) {
        hash = (37 * hash) + VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getVersion();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code UserPayloadProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:UserPayloadProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        userPayload_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        version_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UserPayloadProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.userPayload_ = userPayload_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.version_ = version_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance()) return this;
        if (other.hasUserPayload()) {
          setUserPayload(other.getUserPayload());
        }
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString userPayload_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes user_payload = 1;</code>
       * @return Whether the userPayload field is set.
       */
      @java.lang.Override
      public boolean hasUserPayload() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bytes user_payload = 1;</code>
       * @return The userPayload.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getUserPayload() {
        return userPayload_;
      }
      /**
       * <code>optional bytes user_payload = 1;</code>
       * @param value The userPayload to set.
       * @return This builder for chaining.
       */
      public Builder setUserPayload(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        userPayload_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes user_payload = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearUserPayload() {
        bitField0_ = (bitField0_ & ~0x00000001);
        userPayload_ = getDefaultInstance().getUserPayload();
        onChanged();
        return this;
      }

      private int version_ ;
      /**
       * <code>optional int32 version = 2;</code>
       * @return Whether the version field is set.
       */
      @java.lang.Override
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 version = 2;</code>
       * @return The version.
       */
      @java.lang.Override
      public int getVersion() {
        return version_;
      }
      /**
       * <code>optional int32 version = 2;</code>
       * @param value The version to set.
       * @return This builder for chaining.
       */
      public Builder setVersion(int value) {
        bitField0_ |= 0x00000002;
        version_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 version = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        version_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:UserPayloadProto)
    }

    // @@protoc_insertion_point(class_scope:UserPayloadProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<UserPayloadProto>
        PARSER = new com.google.protobuf.AbstractParser<UserPayloadProto>() {
      @java.lang.Override
      public UserPayloadProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UserPayloadProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<UserPayloadProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<UserPayloadProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface EntityDescriptorProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:EntityDescriptorProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string class_name = 1;</code>
     * @return Whether the className field is set.
     */
    boolean hasClassName();
    /**
     * <code>optional string class_name = 1;</code>
     * @return The className.
     */
    java.lang.String getClassName();
    /**
     * <code>optional string class_name = 1;</code>
     * @return The bytes for className.
     */
    com.google.protobuf.ByteString
        getClassNameBytes();

    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     * @return Whether the userPayload field is set.
     */
    boolean hasUserPayload();
    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     * @return The userPayload.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload();
    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder getUserPayloadOrBuilder();

    /**
     * <code>optional bytes history_text = 3;</code>
     * @return Whether the historyText field is set.
     */
    boolean hasHistoryText();
    /**
     * <code>optional bytes history_text = 3;</code>
     * @return The historyText.
     */
    com.google.protobuf.ByteString getHistoryText();
  }
  /**
   * Protobuf type {@code EntityDescriptorProto}
   */
  public static final class EntityDescriptorProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:EntityDescriptorProto)
      EntityDescriptorProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use EntityDescriptorProto.newBuilder() to construct.
    private EntityDescriptorProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private EntityDescriptorProto() {
      className_ = "";
      historyText_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new EntityDescriptorProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private EntityDescriptorProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              className_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = userPayload_.toBuilder();
              }
              userPayload_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(userPayload_);
                userPayload_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              historyText_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder.class);
    }

    private int bitField0_;
    public static final int CLASS_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object className_;
    /**
     * <code>optional string class_name = 1;</code>
     * @return Whether the className field is set.
     */
    @java.lang.Override
    public boolean hasClassName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string class_name = 1;</code>
     * @return The className.
     */
    @java.lang.Override
    public java.lang.String getClassName() {
      java.lang.Object ref = className_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          className_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string class_name = 1;</code>
     * @return The bytes for className.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getClassNameBytes() {
      java.lang.Object ref = className_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        className_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int USER_PAYLOAD_FIELD_NUMBER = 2;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto userPayload_;
    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     * @return Whether the userPayload field is set.
     */
    @java.lang.Override
    public boolean hasUserPayload() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     * @return The userPayload.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload() {
      return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
    }
    /**
     * <code>optional .UserPayloadProto user_payload = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder getUserPayloadOrBuilder() {
      return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
    }

    public static final int HISTORY_TEXT_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString historyText_;
    /**
     * <code>optional bytes history_text = 3;</code>
     * @return Whether the historyText field is set.
     */
    @java.lang.Override
    public boolean hasHistoryText() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional bytes history_text = 3;</code>
     * @return The historyText.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getHistoryText() {
      return historyText_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, className_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getUserPayload());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeBytes(3, historyText_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, className_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getUserPayload());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, historyText_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) obj;

      if (hasClassName() != other.hasClassName()) return false;
      if (hasClassName()) {
        if (!getClassName()
            .equals(other.getClassName())) return false;
      }
      if (hasUserPayload() != other.hasUserPayload()) return false;
      if (hasUserPayload()) {
        if (!getUserPayload()
            .equals(other.getUserPayload())) return false;
      }
      if (hasHistoryText() != other.hasHistoryText()) return false;
      if (hasHistoryText()) {
        if (!getHistoryText()
            .equals(other.getHistoryText())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasClassName()) {
        hash = (37 * hash) + CLASS_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getClassName().hashCode();
      }
      if (hasUserPayload()) {
        hash = (37 * hash) + USER_PAYLOAD_FIELD_NUMBER;
        hash = (53 * hash) + getUserPayload().hashCode();
      }
      if (hasHistoryText()) {
        hash = (37 * hash) + HISTORY_TEXT_FIELD_NUMBER;
        hash = (53 * hash) + getHistoryText().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code EntityDescriptorProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:EntityDescriptorProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getUserPayloadFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        className_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (userPayloadBuilder_ == null) {
          userPayload_ = null;
        } else {
          userPayloadBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        historyText_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EntityDescriptorProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.className_ = className_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (userPayloadBuilder_ == null) {
            result.userPayload_ = userPayload_;
          } else {
            result.userPayload_ = userPayloadBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.historyText_ = historyText_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) return this;
        if (other.hasClassName()) {
          bitField0_ |= 0x00000001;
          className_ = other.className_;
          onChanged();
        }
        if (other.hasUserPayload()) {
          mergeUserPayload(other.getUserPayload());
        }
        if (other.hasHistoryText()) {
          setHistoryText(other.getHistoryText());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object className_ = "";
      /**
       * <code>optional string class_name = 1;</code>
       * @return Whether the className field is set.
       */
      public boolean hasClassName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string class_name = 1;</code>
       * @return The className.
       */
      public java.lang.String getClassName() {
        java.lang.Object ref = className_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            className_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string class_name = 1;</code>
       * @return The bytes for className.
       */
      public com.google.protobuf.ByteString
          getClassNameBytes() {
        java.lang.Object ref = className_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          className_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string class_name = 1;</code>
       * @param value The className to set.
       * @return This builder for chaining.
       */
      public Builder setClassName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        className_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string class_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearClassName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        className_ = getDefaultInstance().getClassName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string class_name = 1;</code>
       * @param value The bytes for className to set.
       * @return This builder for chaining.
       */
      public Builder setClassNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        className_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto userPayload_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder> userPayloadBuilder_;
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       * @return Whether the userPayload field is set.
       */
      public boolean hasUserPayload() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       * @return The userPayload.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto getUserPayload() {
        if (userPayloadBuilder_ == null) {
          return userPayload_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
        } else {
          return userPayloadBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public Builder setUserPayload(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto value) {
        if (userPayloadBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          userPayload_ = value;
          onChanged();
        } else {
          userPayloadBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public Builder setUserPayload(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder builderForValue) {
        if (userPayloadBuilder_ == null) {
          userPayload_ = builderForValue.build();
          onChanged();
        } else {
          userPayloadBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public Builder mergeUserPayload(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto value) {
        if (userPayloadBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              userPayload_ != null &&
              userPayload_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance()) {
            userPayload_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.newBuilder(userPayload_).mergeFrom(value).buildPartial();
          } else {
            userPayload_ = value;
          }
          onChanged();
        } else {
          userPayloadBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public Builder clearUserPayload() {
        if (userPayloadBuilder_ == null) {
          userPayload_ = null;
          onChanged();
        } else {
          userPayloadBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder getUserPayloadBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getUserPayloadFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder getUserPayloadOrBuilder() {
        if (userPayloadBuilder_ != null) {
          return userPayloadBuilder_.getMessageOrBuilder();
        } else {
          return userPayload_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.getDefaultInstance() : userPayload_;
        }
      }
      /**
       * <code>optional .UserPayloadProto user_payload = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder> 
          getUserPayloadFieldBuilder() {
        if (userPayloadBuilder_ == null) {
          userPayloadBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UserPayloadProtoOrBuilder>(
                  getUserPayload(),
                  getParentForChildren(),
                  isClean());
          userPayload_ = null;
        }
        return userPayloadBuilder_;
      }

      private com.google.protobuf.ByteString historyText_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes history_text = 3;</code>
       * @return Whether the historyText field is set.
       */
      @java.lang.Override
      public boolean hasHistoryText() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional bytes history_text = 3;</code>
       * @return The historyText.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getHistoryText() {
        return historyText_;
      }
      /**
       * <code>optional bytes history_text = 3;</code>
       * @param value The historyText to set.
       * @return This builder for chaining.
       */
      public Builder setHistoryText(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        historyText_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes history_text = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearHistoryText() {
        bitField0_ = (bitField0_ & ~0x00000004);
        historyText_ = getDefaultInstance().getHistoryText();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:EntityDescriptorProto)
    }

    // @@protoc_insertion_point(class_scope:EntityDescriptorProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<EntityDescriptorProto>
        PARSER = new com.google.protobuf.AbstractParser<EntityDescriptorProto>() {
      @java.lang.Override
      public EntityDescriptorProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EntityDescriptorProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<EntityDescriptorProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<EntityDescriptorProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface IOSpecProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:IOSpecProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return Whether the connectedVertexName field is set.
     */
    boolean hasConnectedVertexName();
    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return The connectedVertexName.
     */
    java.lang.String getConnectedVertexName();
    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return The bytes for connectedVertexName.
     */
    com.google.protobuf.ByteString
        getConnectedVertexNameBytes();

    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     * @return Whether the ioDescriptor field is set.
     */
    boolean hasIoDescriptor();
    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     * @return The ioDescriptor.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor();
    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getIoDescriptorOrBuilder();

    /**
     * <code>optional int32 physical_edge_count = 3;</code>
     * @return Whether the physicalEdgeCount field is set.
     */
    boolean hasPhysicalEdgeCount();
    /**
     * <code>optional int32 physical_edge_count = 3;</code>
     * @return The physicalEdgeCount.
     */
    int getPhysicalEdgeCount();
  }
  /**
   * Protobuf type {@code IOSpecProto}
   */
  public static final class IOSpecProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:IOSpecProto)
      IOSpecProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use IOSpecProto.newBuilder() to construct.
    private IOSpecProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private IOSpecProto() {
      connectedVertexName_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new IOSpecProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private IOSpecProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              connectedVertexName_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = ioDescriptor_.toBuilder();
              }
              ioDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(ioDescriptor_);
                ioDescriptor_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              physicalEdgeCount_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder.class);
    }

    private int bitField0_;
    public static final int CONNECTED_VERTEX_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object connectedVertexName_;
    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return Whether the connectedVertexName field is set.
     */
    @java.lang.Override
    public boolean hasConnectedVertexName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return The connectedVertexName.
     */
    @java.lang.Override
    public java.lang.String getConnectedVertexName() {
      java.lang.Object ref = connectedVertexName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          connectedVertexName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string connected_vertex_name = 1;</code>
     * @return The bytes for connectedVertexName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getConnectedVertexNameBytes() {
      java.lang.Object ref = connectedVertexName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        connectedVertexName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IO_DESCRIPTOR_FIELD_NUMBER = 2;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto ioDescriptor_;
    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     * @return Whether the ioDescriptor field is set.
     */
    @java.lang.Override
    public boolean hasIoDescriptor() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     * @return The ioDescriptor.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor() {
      return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
    }
    /**
     * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getIoDescriptorOrBuilder() {
      return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
    }

    public static final int PHYSICAL_EDGE_COUNT_FIELD_NUMBER = 3;
    private int physicalEdgeCount_;
    /**
     * <code>optional int32 physical_edge_count = 3;</code>
     * @return Whether the physicalEdgeCount field is set.
     */
    @java.lang.Override
    public boolean hasPhysicalEdgeCount() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int32 physical_edge_count = 3;</code>
     * @return The physicalEdgeCount.
     */
    @java.lang.Override
    public int getPhysicalEdgeCount() {
      return physicalEdgeCount_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, connectedVertexName_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getIoDescriptor());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, physicalEdgeCount_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, connectedVertexName_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getIoDescriptor());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, physicalEdgeCount_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) obj;

      if (hasConnectedVertexName() != other.hasConnectedVertexName()) return false;
      if (hasConnectedVertexName()) {
        if (!getConnectedVertexName()
            .equals(other.getConnectedVertexName())) return false;
      }
      if (hasIoDescriptor() != other.hasIoDescriptor()) return false;
      if (hasIoDescriptor()) {
        if (!getIoDescriptor()
            .equals(other.getIoDescriptor())) return false;
      }
      if (hasPhysicalEdgeCount() != other.hasPhysicalEdgeCount()) return false;
      if (hasPhysicalEdgeCount()) {
        if (getPhysicalEdgeCount()
            != other.getPhysicalEdgeCount()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasConnectedVertexName()) {
        hash = (37 * hash) + CONNECTED_VERTEX_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getConnectedVertexName().hashCode();
      }
      if (hasIoDescriptor()) {
        hash = (37 * hash) + IO_DESCRIPTOR_FIELD_NUMBER;
        hash = (53 * hash) + getIoDescriptor().hashCode();
      }
      if (hasPhysicalEdgeCount()) {
        hash = (37 * hash) + PHYSICAL_EDGE_COUNT_FIELD_NUMBER;
        hash = (53 * hash) + getPhysicalEdgeCount();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code IOSpecProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:IOSpecProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getIoDescriptorFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        connectedVertexName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (ioDescriptorBuilder_ == null) {
          ioDescriptor_ = null;
        } else {
          ioDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        physicalEdgeCount_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_IOSpecProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.connectedVertexName_ = connectedVertexName_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (ioDescriptorBuilder_ == null) {
            result.ioDescriptor_ = ioDescriptor_;
          } else {
            result.ioDescriptor_ = ioDescriptorBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.physicalEdgeCount_ = physicalEdgeCount_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance()) return this;
        if (other.hasConnectedVertexName()) {
          bitField0_ |= 0x00000001;
          connectedVertexName_ = other.connectedVertexName_;
          onChanged();
        }
        if (other.hasIoDescriptor()) {
          mergeIoDescriptor(other.getIoDescriptor());
        }
        if (other.hasPhysicalEdgeCount()) {
          setPhysicalEdgeCount(other.getPhysicalEdgeCount());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object connectedVertexName_ = "";
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @return Whether the connectedVertexName field is set.
       */
      public boolean hasConnectedVertexName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @return The connectedVertexName.
       */
      public java.lang.String getConnectedVertexName() {
        java.lang.Object ref = connectedVertexName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            connectedVertexName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @return The bytes for connectedVertexName.
       */
      public com.google.protobuf.ByteString
          getConnectedVertexNameBytes() {
        java.lang.Object ref = connectedVertexName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          connectedVertexName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @param value The connectedVertexName to set.
       * @return This builder for chaining.
       */
      public Builder setConnectedVertexName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        connectedVertexName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearConnectedVertexName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        connectedVertexName_ = getDefaultInstance().getConnectedVertexName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string connected_vertex_name = 1;</code>
       * @param value The bytes for connectedVertexName to set.
       * @return This builder for chaining.
       */
      public Builder setConnectedVertexNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        connectedVertexName_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto ioDescriptor_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> ioDescriptorBuilder_;
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       * @return Whether the ioDescriptor field is set.
       */
      public boolean hasIoDescriptor() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       * @return The ioDescriptor.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getIoDescriptor() {
        if (ioDescriptorBuilder_ == null) {
          return ioDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
        } else {
          return ioDescriptorBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public Builder setIoDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (ioDescriptorBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ioDescriptor_ = value;
          onChanged();
        } else {
          ioDescriptorBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public Builder setIoDescriptor(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
        if (ioDescriptorBuilder_ == null) {
          ioDescriptor_ = builderForValue.build();
          onChanged();
        } else {
          ioDescriptorBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public Builder mergeIoDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (ioDescriptorBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              ioDescriptor_ != null &&
              ioDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
            ioDescriptor_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(ioDescriptor_).mergeFrom(value).buildPartial();
          } else {
            ioDescriptor_ = value;
          }
          onChanged();
        } else {
          ioDescriptorBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public Builder clearIoDescriptor() {
        if (ioDescriptorBuilder_ == null) {
          ioDescriptor_ = null;
          onChanged();
        } else {
          ioDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder getIoDescriptorBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getIoDescriptorFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getIoDescriptorOrBuilder() {
        if (ioDescriptorBuilder_ != null) {
          return ioDescriptorBuilder_.getMessageOrBuilder();
        } else {
          return ioDescriptor_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : ioDescriptor_;
        }
      }
      /**
       * <code>optional .EntityDescriptorProto io_descriptor = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
          getIoDescriptorFieldBuilder() {
        if (ioDescriptorBuilder_ == null) {
          ioDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
                  getIoDescriptor(),
                  getParentForChildren(),
                  isClean());
          ioDescriptor_ = null;
        }
        return ioDescriptorBuilder_;
      }

      private int physicalEdgeCount_ ;
      /**
       * <code>optional int32 physical_edge_count = 3;</code>
       * @return Whether the physicalEdgeCount field is set.
       */
      @java.lang.Override
      public boolean hasPhysicalEdgeCount() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int32 physical_edge_count = 3;</code>
       * @return The physicalEdgeCount.
       */
      @java.lang.Override
      public int getPhysicalEdgeCount() {
        return physicalEdgeCount_;
      }
      /**
       * <code>optional int32 physical_edge_count = 3;</code>
       * @param value The physicalEdgeCount to set.
       * @return This builder for chaining.
       */
      public Builder setPhysicalEdgeCount(int value) {
        bitField0_ |= 0x00000004;
        physicalEdgeCount_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 physical_edge_count = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearPhysicalEdgeCount() {
        bitField0_ = (bitField0_ & ~0x00000004);
        physicalEdgeCount_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:IOSpecProto)
    }

    // @@protoc_insertion_point(class_scope:IOSpecProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<IOSpecProto>
        PARSER = new com.google.protobuf.AbstractParser<IOSpecProto>() {
      @java.lang.Override
      public IOSpecProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new IOSpecProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<IOSpecProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<IOSpecProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GroupInputSpecProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GroupInputSpecProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string group_name = 1;</code>
     * @return Whether the groupName field is set.
     */
    boolean hasGroupName();
    /**
     * <code>optional string group_name = 1;</code>
     * @return The groupName.
     */
    java.lang.String getGroupName();
    /**
     * <code>optional string group_name = 1;</code>
     * @return The bytes for groupName.
     */
    com.google.protobuf.ByteString
        getGroupNameBytes();

    /**
     * <code>repeated string group_vertices = 2;</code>
     * @return A list containing the groupVertices.
     */
    java.util.List<java.lang.String>
        getGroupVerticesList();
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @return The count of groupVertices.
     */
    int getGroupVerticesCount();
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @param index The index of the element to return.
     * @return The groupVertices at the given index.
     */
    java.lang.String getGroupVertices(int index);
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the groupVertices at the given index.
     */
    com.google.protobuf.ByteString
        getGroupVerticesBytes(int index);

    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     * @return Whether the mergedInputDescriptor field is set.
     */
    boolean hasMergedInputDescriptor();
    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     * @return The mergedInputDescriptor.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor();
    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getMergedInputDescriptorOrBuilder();
  }
  /**
   * Protobuf type {@code GroupInputSpecProto}
   */
  public static final class GroupInputSpecProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GroupInputSpecProto)
      GroupInputSpecProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GroupInputSpecProto.newBuilder() to construct.
    private GroupInputSpecProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GroupInputSpecProto() {
      groupName_ = "";
      groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GroupInputSpecProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GroupInputSpecProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              groupName_ = bs;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                groupVertices_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              groupVertices_.add(bs);
              break;
            }
            case 26: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = mergedInputDescriptor_.toBuilder();
              }
              mergedInputDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(mergedInputDescriptor_);
                mergedInputDescriptor_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          groupVertices_ = groupVertices_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder.class);
    }

    private int bitField0_;
    public static final int GROUP_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object groupName_;
    /**
     * <code>optional string group_name = 1;</code>
     * @return Whether the groupName field is set.
     */
    @java.lang.Override
    public boolean hasGroupName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string group_name = 1;</code>
     * @return The groupName.
     */
    @java.lang.Override
    public java.lang.String getGroupName() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          groupName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string group_name = 1;</code>
     * @return The bytes for groupName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getGroupNameBytes() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        groupName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int GROUP_VERTICES_FIELD_NUMBER = 2;
    private com.google.protobuf.LazyStringList groupVertices_;
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @return A list containing the groupVertices.
     */
    public com.google.protobuf.ProtocolStringList
        getGroupVerticesList() {
      return groupVertices_;
    }
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @return The count of groupVertices.
     */
    public int getGroupVerticesCount() {
      return groupVertices_.size();
    }
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @param index The index of the element to return.
     * @return The groupVertices at the given index.
     */
    public java.lang.String getGroupVertices(int index) {
      return groupVertices_.get(index);
    }
    /**
     * <code>repeated string group_vertices = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the groupVertices at the given index.
     */
    public com.google.protobuf.ByteString
        getGroupVerticesBytes(int index) {
      return groupVertices_.getByteString(index);
    }

    public static final int MERGED_INPUT_DESCRIPTOR_FIELD_NUMBER = 3;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto mergedInputDescriptor_;
    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     * @return Whether the mergedInputDescriptor field is set.
     */
    @java.lang.Override
    public boolean hasMergedInputDescriptor() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     * @return The mergedInputDescriptor.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor() {
      return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
    }
    /**
     * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getMergedInputDescriptorOrBuilder() {
      return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, groupName_);
      }
      for (int i = 0; i < groupVertices_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, groupVertices_.getRaw(i));
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(3, getMergedInputDescriptor());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, groupName_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < groupVertices_.size(); i++) {
          dataSize += computeStringSizeNoTag(groupVertices_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getGroupVerticesList().size();
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getMergedInputDescriptor());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) obj;

      if (hasGroupName() != other.hasGroupName()) return false;
      if (hasGroupName()) {
        if (!getGroupName()
            .equals(other.getGroupName())) return false;
      }
      if (!getGroupVerticesList()
          .equals(other.getGroupVerticesList())) return false;
      if (hasMergedInputDescriptor() != other.hasMergedInputDescriptor()) return false;
      if (hasMergedInputDescriptor()) {
        if (!getMergedInputDescriptor()
            .equals(other.getMergedInputDescriptor())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasGroupName()) {
        hash = (37 * hash) + GROUP_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getGroupName().hashCode();
      }
      if (getGroupVerticesCount() > 0) {
        hash = (37 * hash) + GROUP_VERTICES_FIELD_NUMBER;
        hash = (53 * hash) + getGroupVerticesList().hashCode();
      }
      if (hasMergedInputDescriptor()) {
        hash = (37 * hash) + MERGED_INPUT_DESCRIPTOR_FIELD_NUMBER;
        hash = (53 * hash) + getMergedInputDescriptor().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GroupInputSpecProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GroupInputSpecProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getMergedInputDescriptorFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        groupName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (mergedInputDescriptorBuilder_ == null) {
          mergedInputDescriptor_ = null;
        } else {
          mergedInputDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GroupInputSpecProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.groupName_ = groupName_;
        if (((bitField0_ & 0x00000002) != 0)) {
          groupVertices_ = groupVertices_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.groupVertices_ = groupVertices_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (mergedInputDescriptorBuilder_ == null) {
            result.mergedInputDescriptor_ = mergedInputDescriptor_;
          } else {
            result.mergedInputDescriptor_ = mergedInputDescriptorBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance()) return this;
        if (other.hasGroupName()) {
          bitField0_ |= 0x00000001;
          groupName_ = other.groupName_;
          onChanged();
        }
        if (!other.groupVertices_.isEmpty()) {
          if (groupVertices_.isEmpty()) {
            groupVertices_ = other.groupVertices_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureGroupVerticesIsMutable();
            groupVertices_.addAll(other.groupVertices_);
          }
          onChanged();
        }
        if (other.hasMergedInputDescriptor()) {
          mergeMergedInputDescriptor(other.getMergedInputDescriptor());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object groupName_ = "";
      /**
       * <code>optional string group_name = 1;</code>
       * @return Whether the groupName field is set.
       */
      public boolean hasGroupName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string group_name = 1;</code>
       * @return The groupName.
       */
      public java.lang.String getGroupName() {
        java.lang.Object ref = groupName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            groupName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string group_name = 1;</code>
       * @return The bytes for groupName.
       */
      public com.google.protobuf.ByteString
          getGroupNameBytes() {
        java.lang.Object ref = groupName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          groupName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string group_name = 1;</code>
       * @param value The groupName to set.
       * @return This builder for chaining.
       */
      public Builder setGroupName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        groupName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string group_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearGroupName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        groupName_ = getDefaultInstance().getGroupName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string group_name = 1;</code>
       * @param value The bytes for groupName to set.
       * @return This builder for chaining.
       */
      public Builder setGroupNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        groupName_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureGroupVerticesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          groupVertices_ = new com.google.protobuf.LazyStringArrayList(groupVertices_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @return A list containing the groupVertices.
       */
      public com.google.protobuf.ProtocolStringList
          getGroupVerticesList() {
        return groupVertices_.getUnmodifiableView();
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @return The count of groupVertices.
       */
      public int getGroupVerticesCount() {
        return groupVertices_.size();
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param index The index of the element to return.
       * @return The groupVertices at the given index.
       */
      public java.lang.String getGroupVertices(int index) {
        return groupVertices_.get(index);
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the groupVertices at the given index.
       */
      public com.google.protobuf.ByteString
          getGroupVerticesBytes(int index) {
        return groupVertices_.getByteString(index);
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param index The index to set the value at.
       * @param value The groupVertices to set.
       * @return This builder for chaining.
       */
      public Builder setGroupVertices(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureGroupVerticesIsMutable();
        groupVertices_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param value The groupVertices to add.
       * @return This builder for chaining.
       */
      public Builder addGroupVertices(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureGroupVerticesIsMutable();
        groupVertices_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param values The groupVertices to add.
       * @return This builder for chaining.
       */
      public Builder addAllGroupVertices(
          java.lang.Iterable<java.lang.String> values) {
        ensureGroupVerticesIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, groupVertices_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearGroupVertices() {
        groupVertices_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string group_vertices = 2;</code>
       * @param value The bytes of the groupVertices to add.
       * @return This builder for chaining.
       */
      public Builder addGroupVerticesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureGroupVerticesIsMutable();
        groupVertices_.add(value);
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto mergedInputDescriptor_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> mergedInputDescriptorBuilder_;
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       * @return Whether the mergedInputDescriptor field is set.
       */
      public boolean hasMergedInputDescriptor() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       * @return The mergedInputDescriptor.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getMergedInputDescriptor() {
        if (mergedInputDescriptorBuilder_ == null) {
          return mergedInputDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
        } else {
          return mergedInputDescriptorBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public Builder setMergedInputDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (mergedInputDescriptorBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          mergedInputDescriptor_ = value;
          onChanged();
        } else {
          mergedInputDescriptorBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public Builder setMergedInputDescriptor(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
        if (mergedInputDescriptorBuilder_ == null) {
          mergedInputDescriptor_ = builderForValue.build();
          onChanged();
        } else {
          mergedInputDescriptorBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public Builder mergeMergedInputDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (mergedInputDescriptorBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              mergedInputDescriptor_ != null &&
              mergedInputDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
            mergedInputDescriptor_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(mergedInputDescriptor_).mergeFrom(value).buildPartial();
          } else {
            mergedInputDescriptor_ = value;
          }
          onChanged();
        } else {
          mergedInputDescriptorBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public Builder clearMergedInputDescriptor() {
        if (mergedInputDescriptorBuilder_ == null) {
          mergedInputDescriptor_ = null;
          onChanged();
        } else {
          mergedInputDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder getMergedInputDescriptorBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getMergedInputDescriptorFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getMergedInputDescriptorOrBuilder() {
        if (mergedInputDescriptorBuilder_ != null) {
          return mergedInputDescriptorBuilder_.getMessageOrBuilder();
        } else {
          return mergedInputDescriptor_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : mergedInputDescriptor_;
        }
      }
      /**
       * <code>optional .EntityDescriptorProto merged_input_descriptor = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
          getMergedInputDescriptorFieldBuilder() {
        if (mergedInputDescriptorBuilder_ == null) {
          mergedInputDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
                  getMergedInputDescriptor(),
                  getParentForChildren(),
                  isClean());
          mergedInputDescriptor_ = null;
        }
        return mergedInputDescriptorBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GroupInputSpecProto)
    }

    // @@protoc_insertion_point(class_scope:GroupInputSpecProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GroupInputSpecProto>
        PARSER = new com.google.protobuf.AbstractParser<GroupInputSpecProto>() {
      @java.lang.Override
      public GroupInputSpecProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GroupInputSpecProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GroupInputSpecProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GroupInputSpecProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SignableVertexSpecOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SignableVertexSpec)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string user = 1;</code>
     * @return Whether the user field is set.
     */
    boolean hasUser();
    /**
     * <code>optional string user = 1;</code>
     * @return The user.
     */
    java.lang.String getUser();
    /**
     * <code>optional string user = 1;</code>
     * @return The bytes for user.
     */
    com.google.protobuf.ByteString
        getUserBytes();

    /**
     * <code>optional int64 signatureKeyId = 2;</code>
     * @return Whether the signatureKeyId field is set.
     */
    boolean hasSignatureKeyId();
    /**
     * <code>optional int64 signatureKeyId = 2;</code>
     * @return The signatureKeyId.
     */
    long getSignatureKeyId();

    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return Whether the hiveQueryId field is set.
     */
    boolean hasHiveQueryId();
    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return The hiveQueryId.
     */
    java.lang.String getHiveQueryId();
    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return The bytes for hiveQueryId.
     */
    com.google.protobuf.ByteString
        getHiveQueryIdBytes();

    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return Whether the dagName field is set.
     */
    boolean hasDagName();
    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return The dagName.
     */
    java.lang.String getDagName();
    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return The bytes for dagName.
     */
    com.google.protobuf.ByteString
        getDagNameBytes();

    /**
     * <code>optional string vertex_name = 6;</code>
     * @return Whether the vertexName field is set.
     */
    boolean hasVertexName();
    /**
     * <code>optional string vertex_name = 6;</code>
     * @return The vertexName.
     */
    java.lang.String getVertexName();
    /**
     * <code>optional string vertex_name = 6;</code>
     * @return The bytes for vertexName.
     */
    com.google.protobuf.ByteString
        getVertexNameBytes();

    /**
     * <code>optional int32 vertex_index = 7;</code>
     * @return Whether the vertexIndex field is set.
     */
    boolean hasVertexIndex();
    /**
     * <code>optional int32 vertex_index = 7;</code>
     * @return The vertexIndex.
     */
    int getVertexIndex();

    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return Whether the tokenIdentifier field is set.
     */
    boolean hasTokenIdentifier();
    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return The tokenIdentifier.
     */
    java.lang.String getTokenIdentifier();
    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return The bytes for tokenIdentifier.
     */
    com.google.protobuf.ByteString
        getTokenIdentifierBytes();

    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     * @return Whether the processorDescriptor field is set.
     */
    boolean hasProcessorDescriptor();
    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     * @return The processorDescriptor.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor();
    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getProcessorDescriptorOrBuilder();

    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> 
        getInputSpecsList();
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getInputSpecs(int index);
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    int getInputSpecsCount();
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
        getInputSpecsOrBuilderList();
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getInputSpecsOrBuilder(
        int index);

    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> 
        getOutputSpecsList();
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getOutputSpecs(int index);
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    int getOutputSpecsCount();
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
        getOutputSpecsOrBuilderList();
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getOutputSpecsOrBuilder(
        int index);

    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> 
        getGroupedInputSpecsList();
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getGroupedInputSpecs(int index);
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    int getGroupedInputSpecsCount();
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
        getGroupedInputSpecsOrBuilderList();
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder getGroupedInputSpecsOrBuilder(
        int index);

    /**
     * <pre>
     * An internal field required for Tez.
     * </pre>
     *
     * <code>optional int32 vertex_parallelism = 13;</code>
     * @return Whether the vertexParallelism field is set.
     */
    boolean hasVertexParallelism();
    /**
     * <pre>
     * An internal field required for Tez.
     * </pre>
     *
     * <code>optional int32 vertex_parallelism = 13;</code>
     * @return The vertexParallelism.
     */
    int getVertexParallelism();

    /**
     * <code>optional bool is_external_submission = 14 [default = false];</code>
     * @return Whether the isExternalSubmission field is set.
     */
    boolean hasIsExternalSubmission();
    /**
     * <code>optional bool is_external_submission = 14 [default = false];</code>
     * @return The isExternalSubmission.
     */
    boolean getIsExternalSubmission();
  }
  /**
   * <pre>
   * The part of SubmitWork that can be signed 
   * </pre>
   *
   * Protobuf type {@code SignableVertexSpec}
   */
  public static final class SignableVertexSpec extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SignableVertexSpec)
      SignableVertexSpecOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SignableVertexSpec.newBuilder() to construct.
    private SignableVertexSpec(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SignableVertexSpec() {
      user_ = "";
      hiveQueryId_ = "";
      dagName_ = "";
      vertexName_ = "";
      tokenIdentifier_ = "";
      inputSpecs_ = java.util.Collections.emptyList();
      outputSpecs_ = java.util.Collections.emptyList();
      groupedInputSpecs_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SignableVertexSpec();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SignableVertexSpec(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              user_ = bs;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              signatureKeyId_ = input.readInt64();
              break;
            }
            case 26: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000008;
              hiveQueryId_ = bs;
              break;
            }
            case 42: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              dagName_ = bs;
              break;
            }
            case 50: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              vertexName_ = bs;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              vertexIndex_ = input.readInt32();
              break;
            }
            case 66: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000080;
              tokenIdentifier_ = bs;
              break;
            }
            case 74: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) != 0)) {
                subBuilder = processorDescriptor_.toBuilder();
              }
              processorDescriptor_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(processorDescriptor_);
                processorDescriptor_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) != 0)) {
                inputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>();
                mutable_bitField0_ |= 0x00000200;
              }
              inputSpecs_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER, extensionRegistry));
              break;
            }
            case 90: {
              if (!((mutable_bitField0_ & 0x00000400) != 0)) {
                outputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>();
                mutable_bitField0_ |= 0x00000400;
              }
              outputSpecs_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.PARSER, extensionRegistry));
              break;
            }
            case 98: {
              if (!((mutable_bitField0_ & 0x00000800) != 0)) {
                groupedInputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto>();
                mutable_bitField0_ |= 0x00000800;
              }
              groupedInputSpecs_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.PARSER, extensionRegistry));
              break;
            }
            case 104: {
              bitField0_ |= 0x00000200;
              vertexParallelism_ = input.readInt32();
              break;
            }
            case 112: {
              bitField0_ |= 0x00000400;
              isExternalSubmission_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000200) != 0)) {
          inputSpecs_ = java.util.Collections.unmodifiableList(inputSpecs_);
        }
        if (((mutable_bitField0_ & 0x00000400) != 0)) {
          outputSpecs_ = java.util.Collections.unmodifiableList(outputSpecs_);
        }
        if (((mutable_bitField0_ & 0x00000800) != 0)) {
          groupedInputSpecs_ = java.util.Collections.unmodifiableList(groupedInputSpecs_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder.class);
    }

    private int bitField0_;
    public static final int USER_FIELD_NUMBER = 1;
    private volatile java.lang.Object user_;
    /**
     * <code>optional string user = 1;</code>
     * @return Whether the user field is set.
     */
    @java.lang.Override
    public boolean hasUser() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string user = 1;</code>
     * @return The user.
     */
    @java.lang.Override
    public java.lang.String getUser() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          user_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string user = 1;</code>
     * @return The bytes for user.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getUserBytes() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        user_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SIGNATUREKEYID_FIELD_NUMBER = 2;
    private long signatureKeyId_;
    /**
     * <code>optional int64 signatureKeyId = 2;</code>
     * @return Whether the signatureKeyId field is set.
     */
    @java.lang.Override
    public boolean hasSignatureKeyId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 signatureKeyId = 2;</code>
     * @return The signatureKeyId.
     */
    @java.lang.Override
    public long getSignatureKeyId() {
      return signatureKeyId_;
    }

    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 3;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int HIVE_QUERY_ID_FIELD_NUMBER = 4;
    private volatile java.lang.Object hiveQueryId_;
    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return Whether the hiveQueryId field is set.
     */
    @java.lang.Override
    public boolean hasHiveQueryId() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return The hiveQueryId.
     */
    @java.lang.Override
    public java.lang.String getHiveQueryId() {
      java.lang.Object ref = hiveQueryId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          hiveQueryId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string hive_query_id = 4;</code>
     * @return The bytes for hiveQueryId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getHiveQueryIdBytes() {
      java.lang.Object ref = hiveQueryId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        hiveQueryId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DAG_NAME_FIELD_NUMBER = 5;
    private volatile java.lang.Object dagName_;
    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return Whether the dagName field is set.
     */
    @java.lang.Override
    public boolean hasDagName() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return The dagName.
     */
    @java.lang.Override
    public java.lang.String getDagName() {
      java.lang.Object ref = dagName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          dagName_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
     * </pre>
     *
     * <code>optional string dag_name = 5;</code>
     * @return The bytes for dagName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDagNameBytes() {
      java.lang.Object ref = dagName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        dagName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERTEX_NAME_FIELD_NUMBER = 6;
    private volatile java.lang.Object vertexName_;
    /**
     * <code>optional string vertex_name = 6;</code>
     * @return Whether the vertexName field is set.
     */
    @java.lang.Override
    public boolean hasVertexName() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string vertex_name = 6;</code>
     * @return The vertexName.
     */
    @java.lang.Override
    public java.lang.String getVertexName() {
      java.lang.Object ref = vertexName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          vertexName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string vertex_name = 6;</code>
     * @return The bytes for vertexName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getVertexNameBytes() {
      java.lang.Object ref = vertexName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        vertexName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERTEX_INDEX_FIELD_NUMBER = 7;
    private int vertexIndex_;
    /**
     * <code>optional int32 vertex_index = 7;</code>
     * @return Whether the vertexIndex field is set.
     */
    @java.lang.Override
    public boolean hasVertexIndex() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int32 vertex_index = 7;</code>
     * @return The vertexIndex.
     */
    @java.lang.Override
    public int getVertexIndex() {
      return vertexIndex_;
    }

    public static final int TOKEN_IDENTIFIER_FIELD_NUMBER = 8;
    private volatile java.lang.Object tokenIdentifier_;
    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return Whether the tokenIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasTokenIdentifier() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return The tokenIdentifier.
     */
    @java.lang.Override
    public java.lang.String getTokenIdentifier() {
      java.lang.Object ref = tokenIdentifier_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          tokenIdentifier_ = s;
        }
        return s;
      }
    }
    /**
     * <pre>
     * The core vertex stuff 
     * </pre>
     *
     * <code>optional string token_identifier = 8;</code>
     * @return The bytes for tokenIdentifier.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTokenIdentifierBytes() {
      java.lang.Object ref = tokenIdentifier_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        tokenIdentifier_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PROCESSOR_DESCRIPTOR_FIELD_NUMBER = 9;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto processorDescriptor_;
    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     * @return Whether the processorDescriptor field is set.
     */
    @java.lang.Override
    public boolean hasProcessorDescriptor() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     * @return The processorDescriptor.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor() {
      return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
    }
    /**
     * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getProcessorDescriptorOrBuilder() {
      return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
    }

    public static final int INPUT_SPECS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> inputSpecs_;
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getInputSpecsList() {
      return inputSpecs_;
    }
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
        getInputSpecsOrBuilderList() {
      return inputSpecs_;
    }
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    @java.lang.Override
    public int getInputSpecsCount() {
      return inputSpecs_.size();
    }
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getInputSpecs(int index) {
      return inputSpecs_.get(index);
    }
    /**
     * <code>repeated .IOSpecProto input_specs = 10;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getInputSpecsOrBuilder(
        int index) {
      return inputSpecs_.get(index);
    }

    public static final int OUTPUT_SPECS_FIELD_NUMBER = 11;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> outputSpecs_;
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getOutputSpecsList() {
      return outputSpecs_;
    }
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
        getOutputSpecsOrBuilderList() {
      return outputSpecs_;
    }
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    @java.lang.Override
    public int getOutputSpecsCount() {
      return outputSpecs_.size();
    }
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getOutputSpecs(int index) {
      return outputSpecs_.get(index);
    }
    /**
     * <code>repeated .IOSpecProto output_specs = 11;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getOutputSpecsOrBuilder(
        int index) {
      return outputSpecs_.get(index);
    }

    public static final int GROUPED_INPUT_SPECS_FIELD_NUMBER = 12;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> groupedInputSpecs_;
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> getGroupedInputSpecsList() {
      return groupedInputSpecs_;
    }
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
        getGroupedInputSpecsOrBuilderList() {
      return groupedInputSpecs_;
    }
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    @java.lang.Override
    public int getGroupedInputSpecsCount() {
      return groupedInputSpecs_.size();
    }
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getGroupedInputSpecs(int index) {
      return groupedInputSpecs_.get(index);
    }
    /**
     * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder getGroupedInputSpecsOrBuilder(
        int index) {
      return groupedInputSpecs_.get(index);
    }

    public static final int VERTEX_PARALLELISM_FIELD_NUMBER = 13;
    private int vertexParallelism_;
    /**
     * <pre>
     * An internal field required for Tez.
     * </pre>
     *
     * <code>optional int32 vertex_parallelism = 13;</code>
     * @return Whether the vertexParallelism field is set.
     */
    @java.lang.Override
    public boolean hasVertexParallelism() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <pre>
     * An internal field required for Tez.
     * </pre>
     *
     * <code>optional int32 vertex_parallelism = 13;</code>
     * @return The vertexParallelism.
     */
    @java.lang.Override
    public int getVertexParallelism() {
      return vertexParallelism_;
    }

    public static final int IS_EXTERNAL_SUBMISSION_FIELD_NUMBER = 14;
    private boolean isExternalSubmission_;
    /**
     * <code>optional bool is_external_submission = 14 [default = false];</code>
     * @return Whether the isExternalSubmission field is set.
     */
    @java.lang.Override
    public boolean hasIsExternalSubmission() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional bool is_external_submission = 14 [default = false];</code>
     * @return The isExternalSubmission.
     */
    @java.lang.Override
    public boolean getIsExternalSubmission() {
      return isExternalSubmission_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, user_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, signatureKeyId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, hiveQueryId_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, dagName_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, vertexName_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt32(7, vertexIndex_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, tokenIdentifier_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeMessage(9, getProcessorDescriptor());
      }
      for (int i = 0; i < inputSpecs_.size(); i++) {
        output.writeMessage(10, inputSpecs_.get(i));
      }
      for (int i = 0; i < outputSpecs_.size(); i++) {
        output.writeMessage(11, outputSpecs_.get(i));
      }
      for (int i = 0; i < groupedInputSpecs_.size(); i++) {
        output.writeMessage(12, groupedInputSpecs_.get(i));
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeInt32(13, vertexParallelism_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeBool(14, isExternalSubmission_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, user_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, signatureKeyId_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, hiveQueryId_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, dagName_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, vertexName_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, vertexIndex_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, tokenIdentifier_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, getProcessorDescriptor());
      }
      for (int i = 0; i < inputSpecs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, inputSpecs_.get(i));
      }
      for (int i = 0; i < outputSpecs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, outputSpecs_.get(i));
      }
      for (int i = 0; i < groupedInputSpecs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, groupedInputSpecs_.get(i));
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(13, vertexParallelism_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(14, isExternalSubmission_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) obj;

      if (hasUser() != other.hasUser()) return false;
      if (hasUser()) {
        if (!getUser()
            .equals(other.getUser())) return false;
      }
      if (hasSignatureKeyId() != other.hasSignatureKeyId()) return false;
      if (hasSignatureKeyId()) {
        if (getSignatureKeyId()
            != other.getSignatureKeyId()) return false;
      }
      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasHiveQueryId() != other.hasHiveQueryId()) return false;
      if (hasHiveQueryId()) {
        if (!getHiveQueryId()
            .equals(other.getHiveQueryId())) return false;
      }
      if (hasDagName() != other.hasDagName()) return false;
      if (hasDagName()) {
        if (!getDagName()
            .equals(other.getDagName())) return false;
      }
      if (hasVertexName() != other.hasVertexName()) return false;
      if (hasVertexName()) {
        if (!getVertexName()
            .equals(other.getVertexName())) return false;
      }
      if (hasVertexIndex() != other.hasVertexIndex()) return false;
      if (hasVertexIndex()) {
        if (getVertexIndex()
            != other.getVertexIndex()) return false;
      }
      if (hasTokenIdentifier() != other.hasTokenIdentifier()) return false;
      if (hasTokenIdentifier()) {
        if (!getTokenIdentifier()
            .equals(other.getTokenIdentifier())) return false;
      }
      if (hasProcessorDescriptor() != other.hasProcessorDescriptor()) return false;
      if (hasProcessorDescriptor()) {
        if (!getProcessorDescriptor()
            .equals(other.getProcessorDescriptor())) return false;
      }
      if (!getInputSpecsList()
          .equals(other.getInputSpecsList())) return false;
      if (!getOutputSpecsList()
          .equals(other.getOutputSpecsList())) return false;
      if (!getGroupedInputSpecsList()
          .equals(other.getGroupedInputSpecsList())) return false;
      if (hasVertexParallelism() != other.hasVertexParallelism()) return false;
      if (hasVertexParallelism()) {
        if (getVertexParallelism()
            != other.getVertexParallelism()) return false;
      }
      if (hasIsExternalSubmission() != other.hasIsExternalSubmission()) return false;
      if (hasIsExternalSubmission()) {
        if (getIsExternalSubmission()
            != other.getIsExternalSubmission()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasUser()) {
        hash = (37 * hash) + USER_FIELD_NUMBER;
        hash = (53 * hash) + getUser().hashCode();
      }
      if (hasSignatureKeyId()) {
        hash = (37 * hash) + SIGNATUREKEYID_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getSignatureKeyId());
      }
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasHiveQueryId()) {
        hash = (37 * hash) + HIVE_QUERY_ID_FIELD_NUMBER;
        hash = (53 * hash) + getHiveQueryId().hashCode();
      }
      if (hasDagName()) {
        hash = (37 * hash) + DAG_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getDagName().hashCode();
      }
      if (hasVertexName()) {
        hash = (37 * hash) + VERTEX_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getVertexName().hashCode();
      }
      if (hasVertexIndex()) {
        hash = (37 * hash) + VERTEX_INDEX_FIELD_NUMBER;
        hash = (53 * hash) + getVertexIndex();
      }
      if (hasTokenIdentifier()) {
        hash = (37 * hash) + TOKEN_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getTokenIdentifier().hashCode();
      }
      if (hasProcessorDescriptor()) {
        hash = (37 * hash) + PROCESSOR_DESCRIPTOR_FIELD_NUMBER;
        hash = (53 * hash) + getProcessorDescriptor().hashCode();
      }
      if (getInputSpecsCount() > 0) {
        hash = (37 * hash) + INPUT_SPECS_FIELD_NUMBER;
        hash = (53 * hash) + getInputSpecsList().hashCode();
      }
      if (getOutputSpecsCount() > 0) {
        hash = (37 * hash) + OUTPUT_SPECS_FIELD_NUMBER;
        hash = (53 * hash) + getOutputSpecsList().hashCode();
      }
      if (getGroupedInputSpecsCount() > 0) {
        hash = (37 * hash) + GROUPED_INPUT_SPECS_FIELD_NUMBER;
        hash = (53 * hash) + getGroupedInputSpecsList().hashCode();
      }
      if (hasVertexParallelism()) {
        hash = (37 * hash) + VERTEX_PARALLELISM_FIELD_NUMBER;
        hash = (53 * hash) + getVertexParallelism();
      }
      if (hasIsExternalSubmission()) {
        hash = (37 * hash) + IS_EXTERNAL_SUBMISSION_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getIsExternalSubmission());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * The part of SubmitWork that can be signed 
     * </pre>
     *
     * Protobuf type {@code SignableVertexSpec}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SignableVertexSpec)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
          getProcessorDescriptorFieldBuilder();
          getInputSpecsFieldBuilder();
          getOutputSpecsFieldBuilder();
          getGroupedInputSpecsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        user_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        signatureKeyId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        hiveQueryId_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        dagName_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        vertexName_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        vertexIndex_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        tokenIdentifier_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        if (processorDescriptorBuilder_ == null) {
          processorDescriptor_ = null;
        } else {
          processorDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        if (inputSpecsBuilder_ == null) {
          inputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          inputSpecsBuilder_.clear();
        }
        if (outputSpecsBuilder_ == null) {
          outputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
        } else {
          outputSpecsBuilder_.clear();
        }
        if (groupedInputSpecsBuilder_ == null) {
          groupedInputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
        } else {
          groupedInputSpecsBuilder_.clear();
        }
        vertexParallelism_ = 0;
        bitField0_ = (bitField0_ & ~0x00001000);
        isExternalSubmission_ = false;
        bitField0_ = (bitField0_ & ~0x00002000);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SignableVertexSpec_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.user_ = user_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.signatureKeyId_ = signatureKeyId_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          to_bitField0_ |= 0x00000008;
        }
        result.hiveQueryId_ = hiveQueryId_;
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.dagName_ = dagName_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.vertexName_ = vertexName_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.vertexIndex_ = vertexIndex_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.tokenIdentifier_ = tokenIdentifier_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          if (processorDescriptorBuilder_ == null) {
            result.processorDescriptor_ = processorDescriptor_;
          } else {
            result.processorDescriptor_ = processorDescriptorBuilder_.build();
          }
          to_bitField0_ |= 0x00000100;
        }
        if (inputSpecsBuilder_ == null) {
          if (((bitField0_ & 0x00000200) != 0)) {
            inputSpecs_ = java.util.Collections.unmodifiableList(inputSpecs_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.inputSpecs_ = inputSpecs_;
        } else {
          result.inputSpecs_ = inputSpecsBuilder_.build();
        }
        if (outputSpecsBuilder_ == null) {
          if (((bitField0_ & 0x00000400) != 0)) {
            outputSpecs_ = java.util.Collections.unmodifiableList(outputSpecs_);
            bitField0_ = (bitField0_ & ~0x00000400);
          }
          result.outputSpecs_ = outputSpecs_;
        } else {
          result.outputSpecs_ = outputSpecsBuilder_.build();
        }
        if (groupedInputSpecsBuilder_ == null) {
          if (((bitField0_ & 0x00000800) != 0)) {
            groupedInputSpecs_ = java.util.Collections.unmodifiableList(groupedInputSpecs_);
            bitField0_ = (bitField0_ & ~0x00000800);
          }
          result.groupedInputSpecs_ = groupedInputSpecs_;
        } else {
          result.groupedInputSpecs_ = groupedInputSpecsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00001000) != 0)) {
          result.vertexParallelism_ = vertexParallelism_;
          to_bitField0_ |= 0x00000200;
        }
        if (((from_bitField0_ & 0x00002000) != 0)) {
          result.isExternalSubmission_ = isExternalSubmission_;
          to_bitField0_ |= 0x00000400;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance()) return this;
        if (other.hasUser()) {
          bitField0_ |= 0x00000001;
          user_ = other.user_;
          onChanged();
        }
        if (other.hasSignatureKeyId()) {
          setSignatureKeyId(other.getSignatureKeyId());
        }
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasHiveQueryId()) {
          bitField0_ |= 0x00000008;
          hiveQueryId_ = other.hiveQueryId_;
          onChanged();
        }
        if (other.hasDagName()) {
          bitField0_ |= 0x00000010;
          dagName_ = other.dagName_;
          onChanged();
        }
        if (other.hasVertexName()) {
          bitField0_ |= 0x00000020;
          vertexName_ = other.vertexName_;
          onChanged();
        }
        if (other.hasVertexIndex()) {
          setVertexIndex(other.getVertexIndex());
        }
        if (other.hasTokenIdentifier()) {
          bitField0_ |= 0x00000080;
          tokenIdentifier_ = other.tokenIdentifier_;
          onChanged();
        }
        if (other.hasProcessorDescriptor()) {
          mergeProcessorDescriptor(other.getProcessorDescriptor());
        }
        if (inputSpecsBuilder_ == null) {
          if (!other.inputSpecs_.isEmpty()) {
            if (inputSpecs_.isEmpty()) {
              inputSpecs_ = other.inputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureInputSpecsIsMutable();
              inputSpecs_.addAll(other.inputSpecs_);
            }
            onChanged();
          }
        } else {
          if (!other.inputSpecs_.isEmpty()) {
            if (inputSpecsBuilder_.isEmpty()) {
              inputSpecsBuilder_.dispose();
              inputSpecsBuilder_ = null;
              inputSpecs_ = other.inputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000200);
              inputSpecsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInputSpecsFieldBuilder() : null;
            } else {
              inputSpecsBuilder_.addAllMessages(other.inputSpecs_);
            }
          }
        }
        if (outputSpecsBuilder_ == null) {
          if (!other.outputSpecs_.isEmpty()) {
            if (outputSpecs_.isEmpty()) {
              outputSpecs_ = other.outputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000400);
            } else {
              ensureOutputSpecsIsMutable();
              outputSpecs_.addAll(other.outputSpecs_);
            }
            onChanged();
          }
        } else {
          if (!other.outputSpecs_.isEmpty()) {
            if (outputSpecsBuilder_.isEmpty()) {
              outputSpecsBuilder_.dispose();
              outputSpecsBuilder_ = null;
              outputSpecs_ = other.outputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000400);
              outputSpecsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputSpecsFieldBuilder() : null;
            } else {
              outputSpecsBuilder_.addAllMessages(other.outputSpecs_);
            }
          }
        }
        if (groupedInputSpecsBuilder_ == null) {
          if (!other.groupedInputSpecs_.isEmpty()) {
            if (groupedInputSpecs_.isEmpty()) {
              groupedInputSpecs_ = other.groupedInputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000800);
            } else {
              ensureGroupedInputSpecsIsMutable();
              groupedInputSpecs_.addAll(other.groupedInputSpecs_);
            }
            onChanged();
          }
        } else {
          if (!other.groupedInputSpecs_.isEmpty()) {
            if (groupedInputSpecsBuilder_.isEmpty()) {
              groupedInputSpecsBuilder_.dispose();
              groupedInputSpecsBuilder_ = null;
              groupedInputSpecs_ = other.groupedInputSpecs_;
              bitField0_ = (bitField0_ & ~0x00000800);
              groupedInputSpecsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getGroupedInputSpecsFieldBuilder() : null;
            } else {
              groupedInputSpecsBuilder_.addAllMessages(other.groupedInputSpecs_);
            }
          }
        }
        if (other.hasVertexParallelism()) {
          setVertexParallelism(other.getVertexParallelism());
        }
        if (other.hasIsExternalSubmission()) {
          setIsExternalSubmission(other.getIsExternalSubmission());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object user_ = "";
      /**
       * <code>optional string user = 1;</code>
       * @return Whether the user field is set.
       */
      public boolean hasUser() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string user = 1;</code>
       * @return The user.
       */
      public java.lang.String getUser() {
        java.lang.Object ref = user_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            user_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string user = 1;</code>
       * @return The bytes for user.
       */
      public com.google.protobuf.ByteString
          getUserBytes() {
        java.lang.Object ref = user_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          user_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string user = 1;</code>
       * @param value The user to set.
       * @return This builder for chaining.
       */
      public Builder setUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearUser() {
        bitField0_ = (bitField0_ & ~0x00000001);
        user_ = getDefaultInstance().getUser();
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 1;</code>
       * @param value The bytes for user to set.
       * @return This builder for chaining.
       */
      public Builder setUserBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }

      private long signatureKeyId_ ;
      /**
       * <code>optional int64 signatureKeyId = 2;</code>
       * @return Whether the signatureKeyId field is set.
       */
      @java.lang.Override
      public boolean hasSignatureKeyId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 signatureKeyId = 2;</code>
       * @return The signatureKeyId.
       */
      @java.lang.Override
      public long getSignatureKeyId() {
        return signatureKeyId_;
      }
      /**
       * <code>optional int64 signatureKeyId = 2;</code>
       * @param value The signatureKeyId to set.
       * @return This builder for chaining.
       */
      public Builder setSignatureKeyId(long value) {
        bitField0_ |= 0x00000002;
        signatureKeyId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 signatureKeyId = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearSignatureKeyId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        signatureKeyId_ = 0L;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private java.lang.Object hiveQueryId_ = "";
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @return Whether the hiveQueryId field is set.
       */
      public boolean hasHiveQueryId() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @return The hiveQueryId.
       */
      public java.lang.String getHiveQueryId() {
        java.lang.Object ref = hiveQueryId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            hiveQueryId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @return The bytes for hiveQueryId.
       */
      public com.google.protobuf.ByteString
          getHiveQueryIdBytes() {
        java.lang.Object ref = hiveQueryId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          hiveQueryId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @param value The hiveQueryId to set.
       * @return This builder for chaining.
       */
      public Builder setHiveQueryId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        hiveQueryId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearHiveQueryId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        hiveQueryId_ = getDefaultInstance().getHiveQueryId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string hive_query_id = 4;</code>
       * @param value The bytes for hiveQueryId to set.
       * @return This builder for chaining.
       */
      public Builder setHiveQueryIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        hiveQueryId_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object dagName_ = "";
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @return Whether the dagName field is set.
       */
      public boolean hasDagName() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @return The dagName.
       */
      public java.lang.String getDagName() {
        java.lang.Object ref = dagName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            dagName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @return The bytes for dagName.
       */
      public com.google.protobuf.ByteString
          getDagNameBytes() {
        java.lang.Object ref = dagName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          dagName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @param value The dagName to set.
       * @return This builder for chaining.
       */
      public Builder setDagName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        dagName_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDagName() {
        bitField0_ = (bitField0_ & ~0x00000010);
        dagName_ = getDefaultInstance().getDagName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Display names cannot be modified by the client for now. If needed, they should be sent to HS2 who will put them here.
       * </pre>
       *
       * <code>optional string dag_name = 5;</code>
       * @param value The bytes for dagName to set.
       * @return This builder for chaining.
       */
      public Builder setDagNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        dagName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object vertexName_ = "";
      /**
       * <code>optional string vertex_name = 6;</code>
       * @return Whether the vertexName field is set.
       */
      public boolean hasVertexName() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional string vertex_name = 6;</code>
       * @return The vertexName.
       */
      public java.lang.String getVertexName() {
        java.lang.Object ref = vertexName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            vertexName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string vertex_name = 6;</code>
       * @return The bytes for vertexName.
       */
      public com.google.protobuf.ByteString
          getVertexNameBytes() {
        java.lang.Object ref = vertexName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          vertexName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string vertex_name = 6;</code>
       * @param value The vertexName to set.
       * @return This builder for chaining.
       */
      public Builder setVertexName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        vertexName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string vertex_name = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearVertexName() {
        bitField0_ = (bitField0_ & ~0x00000020);
        vertexName_ = getDefaultInstance().getVertexName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string vertex_name = 6;</code>
       * @param value The bytes for vertexName to set.
       * @return This builder for chaining.
       */
      public Builder setVertexNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        vertexName_ = value;
        onChanged();
        return this;
      }

      private int vertexIndex_ ;
      /**
       * <code>optional int32 vertex_index = 7;</code>
       * @return Whether the vertexIndex field is set.
       */
      @java.lang.Override
      public boolean hasVertexIndex() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int32 vertex_index = 7;</code>
       * @return The vertexIndex.
       */
      @java.lang.Override
      public int getVertexIndex() {
        return vertexIndex_;
      }
      /**
       * <code>optional int32 vertex_index = 7;</code>
       * @param value The vertexIndex to set.
       * @return This builder for chaining.
       */
      public Builder setVertexIndex(int value) {
        bitField0_ |= 0x00000040;
        vertexIndex_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 vertex_index = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearVertexIndex() {
        bitField0_ = (bitField0_ & ~0x00000040);
        vertexIndex_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object tokenIdentifier_ = "";
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @return Whether the tokenIdentifier field is set.
       */
      public boolean hasTokenIdentifier() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @return The tokenIdentifier.
       */
      public java.lang.String getTokenIdentifier() {
        java.lang.Object ref = tokenIdentifier_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            tokenIdentifier_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @return The bytes for tokenIdentifier.
       */
      public com.google.protobuf.ByteString
          getTokenIdentifierBytes() {
        java.lang.Object ref = tokenIdentifier_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          tokenIdentifier_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @param value The tokenIdentifier to set.
       * @return This builder for chaining.
       */
      public Builder setTokenIdentifier(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        tokenIdentifier_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearTokenIdentifier() {
        bitField0_ = (bitField0_ & ~0x00000080);
        tokenIdentifier_ = getDefaultInstance().getTokenIdentifier();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * The core vertex stuff 
       * </pre>
       *
       * <code>optional string token_identifier = 8;</code>
       * @param value The bytes for tokenIdentifier to set.
       * @return This builder for chaining.
       */
      public Builder setTokenIdentifierBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        tokenIdentifier_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto processorDescriptor_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> processorDescriptorBuilder_;
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       * @return Whether the processorDescriptor field is set.
       */
      public boolean hasProcessorDescriptor() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       * @return The processorDescriptor.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto getProcessorDescriptor() {
        if (processorDescriptorBuilder_ == null) {
          return processorDescriptor_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
        } else {
          return processorDescriptorBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public Builder setProcessorDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (processorDescriptorBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          processorDescriptor_ = value;
          onChanged();
        } else {
          processorDescriptorBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public Builder setProcessorDescriptor(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder builderForValue) {
        if (processorDescriptorBuilder_ == null) {
          processorDescriptor_ = builderForValue.build();
          onChanged();
        } else {
          processorDescriptorBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public Builder mergeProcessorDescriptor(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto value) {
        if (processorDescriptorBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0) &&
              processorDescriptor_ != null &&
              processorDescriptor_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance()) {
            processorDescriptor_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.newBuilder(processorDescriptor_).mergeFrom(value).buildPartial();
          } else {
            processorDescriptor_ = value;
          }
          onChanged();
        } else {
          processorDescriptorBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public Builder clearProcessorDescriptor() {
        if (processorDescriptorBuilder_ == null) {
          processorDescriptor_ = null;
          onChanged();
        } else {
          processorDescriptorBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder getProcessorDescriptorBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getProcessorDescriptorFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder getProcessorDescriptorOrBuilder() {
        if (processorDescriptorBuilder_ != null) {
          return processorDescriptorBuilder_.getMessageOrBuilder();
        } else {
          return processorDescriptor_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.getDefaultInstance() : processorDescriptor_;
        }
      }
      /**
       * <code>optional .EntityDescriptorProto processor_descriptor = 9;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder> 
          getProcessorDescriptorFieldBuilder() {
        if (processorDescriptorBuilder_ == null) {
          processorDescriptorBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EntityDescriptorProtoOrBuilder>(
                  getProcessorDescriptor(),
                  getParentForChildren(),
                  isClean());
          processorDescriptor_ = null;
        }
        return processorDescriptorBuilder_;
      }

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> inputSpecs_ =
        java.util.Collections.emptyList();
      private void ensureInputSpecsIsMutable() {
        if (!((bitField0_ & 0x00000200) != 0)) {
          inputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>(inputSpecs_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> inputSpecsBuilder_;

      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getInputSpecsList() {
        if (inputSpecsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(inputSpecs_);
        } else {
          return inputSpecsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public int getInputSpecsCount() {
        if (inputSpecsBuilder_ == null) {
          return inputSpecs_.size();
        } else {
          return inputSpecsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getInputSpecs(int index) {
        if (inputSpecsBuilder_ == null) {
          return inputSpecs_.get(index);
        } else {
          return inputSpecsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder setInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (inputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputSpecsIsMutable();
          inputSpecs_.set(index, value);
          onChanged();
        } else {
          inputSpecsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder setInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (inputSpecsBuilder_ == null) {
          ensureInputSpecsIsMutable();
          inputSpecs_.set(index, builderForValue.build());
          onChanged();
        } else {
          inputSpecsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder addInputSpecs(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (inputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputSpecsIsMutable();
          inputSpecs_.add(value);
          onChanged();
        } else {
          inputSpecsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder addInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (inputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputSpecsIsMutable();
          inputSpecs_.add(index, value);
          onChanged();
        } else {
          inputSpecsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder addInputSpecs(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (inputSpecsBuilder_ == null) {
          ensureInputSpecsIsMutable();
          inputSpecs_.add(builderForValue.build());
          onChanged();
        } else {
          inputSpecsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder addInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (inputSpecsBuilder_ == null) {
          ensureInputSpecsIsMutable();
          inputSpecs_.add(index, builderForValue.build());
          onChanged();
        } else {
          inputSpecsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder addAllInputSpecs(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> values) {
        if (inputSpecsBuilder_ == null) {
          ensureInputSpecsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, inputSpecs_);
          onChanged();
        } else {
          inputSpecsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder clearInputSpecs() {
        if (inputSpecsBuilder_ == null) {
          inputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          inputSpecsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public Builder removeInputSpecs(int index) {
        if (inputSpecsBuilder_ == null) {
          ensureInputSpecsIsMutable();
          inputSpecs_.remove(index);
          onChanged();
        } else {
          inputSpecsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder getInputSpecsBuilder(
          int index) {
        return getInputSpecsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getInputSpecsOrBuilder(
          int index) {
        if (inputSpecsBuilder_ == null) {
          return inputSpecs_.get(index);  } else {
          return inputSpecsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
           getInputSpecsOrBuilderList() {
        if (inputSpecsBuilder_ != null) {
          return inputSpecsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(inputSpecs_);
        }
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder addInputSpecsBuilder() {
        return getInputSpecsFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder addInputSpecsBuilder(
          int index) {
        return getInputSpecsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .IOSpecProto input_specs = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder> 
           getInputSpecsBuilderList() {
        return getInputSpecsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
          getInputSpecsFieldBuilder() {
        if (inputSpecsBuilder_ == null) {
          inputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder>(
                  inputSpecs_,
                  ((bitField0_ & 0x00000200) != 0),
                  getParentForChildren(),
                  isClean());
          inputSpecs_ = null;
        }
        return inputSpecsBuilder_;
      }

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> outputSpecs_ =
        java.util.Collections.emptyList();
      private void ensureOutputSpecsIsMutable() {
        if (!((bitField0_ & 0x00000400) != 0)) {
          outputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto>(outputSpecs_);
          bitField0_ |= 0x00000400;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> outputSpecsBuilder_;

      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> getOutputSpecsList() {
        if (outputSpecsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(outputSpecs_);
        } else {
          return outputSpecsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public int getOutputSpecsCount() {
        if (outputSpecsBuilder_ == null) {
          return outputSpecs_.size();
        } else {
          return outputSpecsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto getOutputSpecs(int index) {
        if (outputSpecsBuilder_ == null) {
          return outputSpecs_.get(index);
        } else {
          return outputSpecsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder setOutputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (outputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputSpecsIsMutable();
          outputSpecs_.set(index, value);
          onChanged();
        } else {
          outputSpecsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder setOutputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (outputSpecsBuilder_ == null) {
          ensureOutputSpecsIsMutable();
          outputSpecs_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputSpecsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder addOutputSpecs(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (outputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputSpecsIsMutable();
          outputSpecs_.add(value);
          onChanged();
        } else {
          outputSpecsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder addOutputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto value) {
        if (outputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputSpecsIsMutable();
          outputSpecs_.add(index, value);
          onChanged();
        } else {
          outputSpecsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder addOutputSpecs(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (outputSpecsBuilder_ == null) {
          ensureOutputSpecsIsMutable();
          outputSpecs_.add(builderForValue.build());
          onChanged();
        } else {
          outputSpecsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder addOutputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder builderForValue) {
        if (outputSpecsBuilder_ == null) {
          ensureOutputSpecsIsMutable();
          outputSpecs_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputSpecsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder addAllOutputSpecs(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto> values) {
        if (outputSpecsBuilder_ == null) {
          ensureOutputSpecsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, outputSpecs_);
          onChanged();
        } else {
          outputSpecsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder clearOutputSpecs() {
        if (outputSpecsBuilder_ == null) {
          outputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
          onChanged();
        } else {
          outputSpecsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public Builder removeOutputSpecs(int index) {
        if (outputSpecsBuilder_ == null) {
          ensureOutputSpecsIsMutable();
          outputSpecs_.remove(index);
          onChanged();
        } else {
          outputSpecsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder getOutputSpecsBuilder(
          int index) {
        return getOutputSpecsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder getOutputSpecsOrBuilder(
          int index) {
        if (outputSpecsBuilder_ == null) {
          return outputSpecs_.get(index);  } else {
          return outputSpecsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
           getOutputSpecsOrBuilderList() {
        if (outputSpecsBuilder_ != null) {
          return outputSpecsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(outputSpecs_);
        }
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder addOutputSpecsBuilder() {
        return getOutputSpecsFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder addOutputSpecsBuilder(
          int index) {
        return getOutputSpecsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .IOSpecProto output_specs = 11;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder> 
           getOutputSpecsBuilderList() {
        return getOutputSpecsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder> 
          getOutputSpecsFieldBuilder() {
        if (outputSpecsBuilder_ == null) {
          outputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.IOSpecProtoOrBuilder>(
                  outputSpecs_,
                  ((bitField0_ & 0x00000400) != 0),
                  getParentForChildren(),
                  isClean());
          outputSpecs_ = null;
        }
        return outputSpecsBuilder_;
      }

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> groupedInputSpecs_ =
        java.util.Collections.emptyList();
      private void ensureGroupedInputSpecsIsMutable() {
        if (!((bitField0_ & 0x00000800) != 0)) {
          groupedInputSpecs_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto>(groupedInputSpecs_);
          bitField0_ |= 0x00000800;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> groupedInputSpecsBuilder_;

      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> getGroupedInputSpecsList() {
        if (groupedInputSpecsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(groupedInputSpecs_);
        } else {
          return groupedInputSpecsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public int getGroupedInputSpecsCount() {
        if (groupedInputSpecsBuilder_ == null) {
          return groupedInputSpecs_.size();
        } else {
          return groupedInputSpecsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto getGroupedInputSpecs(int index) {
        if (groupedInputSpecsBuilder_ == null) {
          return groupedInputSpecs_.get(index);
        } else {
          return groupedInputSpecsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder setGroupedInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto value) {
        if (groupedInputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.set(index, value);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder setGroupedInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder builderForValue) {
        if (groupedInputSpecsBuilder_ == null) {
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.set(index, builderForValue.build());
          onChanged();
        } else {
          groupedInputSpecsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder addGroupedInputSpecs(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto value) {
        if (groupedInputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.add(value);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder addGroupedInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto value) {
        if (groupedInputSpecsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.add(index, value);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder addGroupedInputSpecs(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder builderForValue) {
        if (groupedInputSpecsBuilder_ == null) {
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.add(builderForValue.build());
          onChanged();
        } else {
          groupedInputSpecsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder addGroupedInputSpecs(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder builderForValue) {
        if (groupedInputSpecsBuilder_ == null) {
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.add(index, builderForValue.build());
          onChanged();
        } else {
          groupedInputSpecsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder addAllGroupedInputSpecs(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto> values) {
        if (groupedInputSpecsBuilder_ == null) {
          ensureGroupedInputSpecsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, groupedInputSpecs_);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder clearGroupedInputSpecs() {
        if (groupedInputSpecsBuilder_ == null) {
          groupedInputSpecs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000800);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public Builder removeGroupedInputSpecs(int index) {
        if (groupedInputSpecsBuilder_ == null) {
          ensureGroupedInputSpecsIsMutable();
          groupedInputSpecs_.remove(index);
          onChanged();
        } else {
          groupedInputSpecsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder getGroupedInputSpecsBuilder(
          int index) {
        return getGroupedInputSpecsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder getGroupedInputSpecsOrBuilder(
          int index) {
        if (groupedInputSpecsBuilder_ == null) {
          return groupedInputSpecs_.get(index);  } else {
          return groupedInputSpecsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
           getGroupedInputSpecsOrBuilderList() {
        if (groupedInputSpecsBuilder_ != null) {
          return groupedInputSpecsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(groupedInputSpecs_);
        }
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder addGroupedInputSpecsBuilder() {
        return getGroupedInputSpecsFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder addGroupedInputSpecsBuilder(
          int index) {
        return getGroupedInputSpecsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.getDefaultInstance());
      }
      /**
       * <code>repeated .GroupInputSpecProto grouped_input_specs = 12;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder> 
           getGroupedInputSpecsBuilderList() {
        return getGroupedInputSpecsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder> 
          getGroupedInputSpecsFieldBuilder() {
        if (groupedInputSpecsBuilder_ == null) {
          groupedInputSpecsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GroupInputSpecProtoOrBuilder>(
                  groupedInputSpecs_,
                  ((bitField0_ & 0x00000800) != 0),
                  getParentForChildren(),
                  isClean());
          groupedInputSpecs_ = null;
        }
        return groupedInputSpecsBuilder_;
      }

      private int vertexParallelism_ ;
      /**
       * <pre>
       * An internal field required for Tez.
       * </pre>
       *
       * <code>optional int32 vertex_parallelism = 13;</code>
       * @return Whether the vertexParallelism field is set.
       */
      @java.lang.Override
      public boolean hasVertexParallelism() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <pre>
       * An internal field required for Tez.
       * </pre>
       *
       * <code>optional int32 vertex_parallelism = 13;</code>
       * @return The vertexParallelism.
       */
      @java.lang.Override
      public int getVertexParallelism() {
        return vertexParallelism_;
      }
      /**
       * <pre>
       * An internal field required for Tez.
       * </pre>
       *
       * <code>optional int32 vertex_parallelism = 13;</code>
       * @param value The vertexParallelism to set.
       * @return This builder for chaining.
       */
      public Builder setVertexParallelism(int value) {
        bitField0_ |= 0x00001000;
        vertexParallelism_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * An internal field required for Tez.
       * </pre>
       *
       * <code>optional int32 vertex_parallelism = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearVertexParallelism() {
        bitField0_ = (bitField0_ & ~0x00001000);
        vertexParallelism_ = 0;
        onChanged();
        return this;
      }

      private boolean isExternalSubmission_ ;
      /**
       * <code>optional bool is_external_submission = 14 [default = false];</code>
       * @return Whether the isExternalSubmission field is set.
       */
      @java.lang.Override
      public boolean hasIsExternalSubmission() {
        return ((bitField0_ & 0x00002000) != 0);
      }
      /**
       * <code>optional bool is_external_submission = 14 [default = false];</code>
       * @return The isExternalSubmission.
       */
      @java.lang.Override
      public boolean getIsExternalSubmission() {
        return isExternalSubmission_;
      }
      /**
       * <code>optional bool is_external_submission = 14 [default = false];</code>
       * @param value The isExternalSubmission to set.
       * @return This builder for chaining.
       */
      public Builder setIsExternalSubmission(boolean value) {
        bitField0_ |= 0x00002000;
        isExternalSubmission_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_external_submission = 14 [default = false];</code>
       * @return This builder for chaining.
       */
      public Builder clearIsExternalSubmission() {
        bitField0_ = (bitField0_ & ~0x00002000);
        isExternalSubmission_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SignableVertexSpec)
    }

    // @@protoc_insertion_point(class_scope:SignableVertexSpec)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SignableVertexSpec>
        PARSER = new com.google.protobuf.AbstractParser<SignableVertexSpec>() {
      @java.lang.Override
      public SignableVertexSpec parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SignableVertexSpec(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SignableVertexSpec> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SignableVertexSpec> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface VertexOrBinaryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:VertexOrBinary)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     * @return Whether the vertex field is set.
     */
    boolean hasVertex();
    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     * @return The vertex.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex();
    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder getVertexOrBuilder();

    /**
     * <pre>
     * SignableVertexSpec
     * </pre>
     *
     * <code>optional bytes vertexBinary = 2;</code>
     * @return Whether the vertexBinary field is set.
     */
    boolean hasVertexBinary();
    /**
     * <pre>
     * SignableVertexSpec
     * </pre>
     *
     * <code>optional bytes vertexBinary = 2;</code>
     * @return The vertexBinary.
     */
    com.google.protobuf.ByteString getVertexBinary();
  }
  /**
   * <pre>
   * Union
   * </pre>
   *
   * Protobuf type {@code VertexOrBinary}
   */
  public static final class VertexOrBinary extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:VertexOrBinary)
      VertexOrBinaryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use VertexOrBinary.newBuilder() to construct.
    private VertexOrBinary(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private VertexOrBinary() {
      vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new VertexOrBinary();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private VertexOrBinary(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = vertex_.toBuilder();
              }
              vertex_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(vertex_);
                vertex_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              vertexBinary_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder.class);
    }

    private int bitField0_;
    public static final int VERTEX_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec vertex_;
    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     * @return Whether the vertex field is set.
     */
    @java.lang.Override
    public boolean hasVertex() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     * @return The vertex.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex() {
      return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
    }
    /**
     * <code>optional .SignableVertexSpec vertex = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder getVertexOrBuilder() {
      return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
    }

    public static final int VERTEXBINARY_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString vertexBinary_;
    /**
     * <pre>
     * SignableVertexSpec
     * </pre>
     *
     * <code>optional bytes vertexBinary = 2;</code>
     * @return Whether the vertexBinary field is set.
     */
    @java.lang.Override
    public boolean hasVertexBinary() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * SignableVertexSpec
     * </pre>
     *
     * <code>optional bytes vertexBinary = 2;</code>
     * @return The vertexBinary.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getVertexBinary() {
      return vertexBinary_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getVertex());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBytes(2, vertexBinary_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getVertex());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, vertexBinary_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) obj;

      if (hasVertex() != other.hasVertex()) return false;
      if (hasVertex()) {
        if (!getVertex()
            .equals(other.getVertex())) return false;
      }
      if (hasVertexBinary() != other.hasVertexBinary()) return false;
      if (hasVertexBinary()) {
        if (!getVertexBinary()
            .equals(other.getVertexBinary())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasVertex()) {
        hash = (37 * hash) + VERTEX_FIELD_NUMBER;
        hash = (53 * hash) + getVertex().hashCode();
      }
      if (hasVertexBinary()) {
        hash = (37 * hash) + VERTEXBINARY_FIELD_NUMBER;
        hash = (53 * hash) + getVertexBinary().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Union
     * </pre>
     *
     * Protobuf type {@code VertexOrBinary}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:VertexOrBinary)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getVertexFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (vertexBuilder_ == null) {
          vertex_ = null;
        } else {
          vertexBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_VertexOrBinary_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (vertexBuilder_ == null) {
            result.vertex_ = vertex_;
          } else {
            result.vertex_ = vertexBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.vertexBinary_ = vertexBinary_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance()) return this;
        if (other.hasVertex()) {
          mergeVertex(other.getVertex());
        }
        if (other.hasVertexBinary()) {
          setVertexBinary(other.getVertexBinary());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec vertex_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder> vertexBuilder_;
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       * @return Whether the vertex field is set.
       */
      public boolean hasVertex() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       * @return The vertex.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec getVertex() {
        if (vertexBuilder_ == null) {
          return vertex_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
        } else {
          return vertexBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public Builder setVertex(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec value) {
        if (vertexBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          vertex_ = value;
          onChanged();
        } else {
          vertexBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public Builder setVertex(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder builderForValue) {
        if (vertexBuilder_ == null) {
          vertex_ = builderForValue.build();
          onChanged();
        } else {
          vertexBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public Builder mergeVertex(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec value) {
        if (vertexBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              vertex_ != null &&
              vertex_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance()) {
            vertex_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.newBuilder(vertex_).mergeFrom(value).buildPartial();
          } else {
            vertex_ = value;
          }
          onChanged();
        } else {
          vertexBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public Builder clearVertex() {
        if (vertexBuilder_ == null) {
          vertex_ = null;
          onChanged();
        } else {
          vertexBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder getVertexBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getVertexFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder getVertexOrBuilder() {
        if (vertexBuilder_ != null) {
          return vertexBuilder_.getMessageOrBuilder();
        } else {
          return vertex_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.getDefaultInstance() : vertex_;
        }
      }
      /**
       * <code>optional .SignableVertexSpec vertex = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder> 
          getVertexFieldBuilder() {
        if (vertexBuilder_ == null) {
          vertexBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpec.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SignableVertexSpecOrBuilder>(
                  getVertex(),
                  getParentForChildren(),
                  isClean());
          vertex_ = null;
        }
        return vertexBuilder_;
      }

      private com.google.protobuf.ByteString vertexBinary_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * SignableVertexSpec
       * </pre>
       *
       * <code>optional bytes vertexBinary = 2;</code>
       * @return Whether the vertexBinary field is set.
       */
      @java.lang.Override
      public boolean hasVertexBinary() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * SignableVertexSpec
       * </pre>
       *
       * <code>optional bytes vertexBinary = 2;</code>
       * @return The vertexBinary.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getVertexBinary() {
        return vertexBinary_;
      }
      /**
       * <pre>
       * SignableVertexSpec
       * </pre>
       *
       * <code>optional bytes vertexBinary = 2;</code>
       * @param value The vertexBinary to set.
       * @return This builder for chaining.
       */
      public Builder setVertexBinary(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        vertexBinary_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * SignableVertexSpec
       * </pre>
       *
       * <code>optional bytes vertexBinary = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearVertexBinary() {
        bitField0_ = (bitField0_ & ~0x00000002);
        vertexBinary_ = getDefaultInstance().getVertexBinary();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:VertexOrBinary)
    }

    // @@protoc_insertion_point(class_scope:VertexOrBinary)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<VertexOrBinary>
        PARSER = new com.google.protobuf.AbstractParser<VertexOrBinary>() {
      @java.lang.Override
      public VertexOrBinary parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new VertexOrBinary(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<VertexOrBinary> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<VertexOrBinary> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface FragmentRuntimeInfoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:FragmentRuntimeInfo)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
     * @return Whether the numSelfAndUpstreamTasks field is set.
     */
    boolean hasNumSelfAndUpstreamTasks();
    /**
     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
     * @return The numSelfAndUpstreamTasks.
     */
    int getNumSelfAndUpstreamTasks();

    /**
     * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
     * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
     */
    boolean hasNumSelfAndUpstreamCompletedTasks();
    /**
     * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
     * @return The numSelfAndUpstreamCompletedTasks.
     */
    int getNumSelfAndUpstreamCompletedTasks();

    /**
     * <code>optional int32 within_dag_priority = 3;</code>
     * @return Whether the withinDagPriority field is set.
     */
    boolean hasWithinDagPriority();
    /**
     * <code>optional int32 within_dag_priority = 3;</code>
     * @return The withinDagPriority.
     */
    int getWithinDagPriority();

    /**
     * <code>optional int64 dag_start_time = 4;</code>
     * @return Whether the dagStartTime field is set.
     */
    boolean hasDagStartTime();
    /**
     * <code>optional int64 dag_start_time = 4;</code>
     * @return The dagStartTime.
     */
    long getDagStartTime();

    /**
     * <code>optional int64 first_attempt_start_time = 5;</code>
     * @return Whether the firstAttemptStartTime field is set.
     */
    boolean hasFirstAttemptStartTime();
    /**
     * <code>optional int64 first_attempt_start_time = 5;</code>
     * @return The firstAttemptStartTime.
     */
    long getFirstAttemptStartTime();

    /**
     * <code>optional int64 current_attempt_start_time = 6;</code>
     * @return Whether the currentAttemptStartTime field is set.
     */
    boolean hasCurrentAttemptStartTime();
    /**
     * <code>optional int64 current_attempt_start_time = 6;</code>
     * @return The currentAttemptStartTime.
     */
    long getCurrentAttemptStartTime();
  }
  /**
   * Protobuf type {@code FragmentRuntimeInfo}
   */
  public static final class FragmentRuntimeInfo extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:FragmentRuntimeInfo)
      FragmentRuntimeInfoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use FragmentRuntimeInfo.newBuilder() to construct.
    private FragmentRuntimeInfo(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private FragmentRuntimeInfo() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new FragmentRuntimeInfo();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private FragmentRuntimeInfo(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              numSelfAndUpstreamTasks_ = input.readInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              numSelfAndUpstreamCompletedTasks_ = input.readInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              withinDagPriority_ = input.readInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              dagStartTime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              firstAttemptStartTime_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              currentAttemptStartTime_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder.class);
    }

    private int bitField0_;
    public static final int NUM_SELF_AND_UPSTREAM_TASKS_FIELD_NUMBER = 1;
    private int numSelfAndUpstreamTasks_;
    /**
     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
     * @return Whether the numSelfAndUpstreamTasks field is set.
     */
    @java.lang.Override
    public boolean hasNumSelfAndUpstreamTasks() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
     * @return The numSelfAndUpstreamTasks.
     */
    @java.lang.Override
    public int getNumSelfAndUpstreamTasks() {
      return numSelfAndUpstreamTasks_;
    }

    public static final int NUM_SELF_AND_UPSTREAM_COMPLETED_TASKS_FIELD_NUMBER = 2;
    private int numSelfAndUpstreamCompletedTasks_;
    /**
     * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
     * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
     */
    @java.lang.Override
    public boolean hasNumSelfAndUpstreamCompletedTasks() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
     * @return The numSelfAndUpstreamCompletedTasks.
     */
    @java.lang.Override
    public int getNumSelfAndUpstreamCompletedTasks() {
      return numSelfAndUpstreamCompletedTasks_;
    }

    public static final int WITHIN_DAG_PRIORITY_FIELD_NUMBER = 3;
    private int withinDagPriority_;
    /**
     * <code>optional int32 within_dag_priority = 3;</code>
     * @return Whether the withinDagPriority field is set.
     */
    @java.lang.Override
    public boolean hasWithinDagPriority() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int32 within_dag_priority = 3;</code>
     * @return The withinDagPriority.
     */
    @java.lang.Override
    public int getWithinDagPriority() {
      return withinDagPriority_;
    }

    public static final int DAG_START_TIME_FIELD_NUMBER = 4;
    private long dagStartTime_;
    /**
     * <code>optional int64 dag_start_time = 4;</code>
     * @return Whether the dagStartTime field is set.
     */
    @java.lang.Override
    public boolean hasDagStartTime() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int64 dag_start_time = 4;</code>
     * @return The dagStartTime.
     */
    @java.lang.Override
    public long getDagStartTime() {
      return dagStartTime_;
    }

    public static final int FIRST_ATTEMPT_START_TIME_FIELD_NUMBER = 5;
    private long firstAttemptStartTime_;
    /**
     * <code>optional int64 first_attempt_start_time = 5;</code>
     * @return Whether the firstAttemptStartTime field is set.
     */
    @java.lang.Override
    public boolean hasFirstAttemptStartTime() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional int64 first_attempt_start_time = 5;</code>
     * @return The firstAttemptStartTime.
     */
    @java.lang.Override
    public long getFirstAttemptStartTime() {
      return firstAttemptStartTime_;
    }

    public static final int CURRENT_ATTEMPT_START_TIME_FIELD_NUMBER = 6;
    private long currentAttemptStartTime_;
    /**
     * <code>optional int64 current_attempt_start_time = 6;</code>
     * @return Whether the currentAttemptStartTime field is set.
     */
    @java.lang.Override
    public boolean hasCurrentAttemptStartTime() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional int64 current_attempt_start_time = 6;</code>
     * @return The currentAttemptStartTime.
     */
    @java.lang.Override
    public long getCurrentAttemptStartTime() {
      return currentAttemptStartTime_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(1, numSelfAndUpstreamTasks_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, numSelfAndUpstreamCompletedTasks_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, withinDagPriority_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt64(4, dagStartTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        output.writeInt64(5, firstAttemptStartTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        output.writeInt64(6, currentAttemptStartTime_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, numSelfAndUpstreamTasks_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, numSelfAndUpstreamCompletedTasks_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, withinDagPriority_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, dagStartTime_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, firstAttemptStartTime_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, currentAttemptStartTime_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) obj;

      if (hasNumSelfAndUpstreamTasks() != other.hasNumSelfAndUpstreamTasks()) return false;
      if (hasNumSelfAndUpstreamTasks()) {
        if (getNumSelfAndUpstreamTasks()
            != other.getNumSelfAndUpstreamTasks()) return false;
      }
      if (hasNumSelfAndUpstreamCompletedTasks() != other.hasNumSelfAndUpstreamCompletedTasks()) return false;
      if (hasNumSelfAndUpstreamCompletedTasks()) {
        if (getNumSelfAndUpstreamCompletedTasks()
            != other.getNumSelfAndUpstreamCompletedTasks()) return false;
      }
      if (hasWithinDagPriority() != other.hasWithinDagPriority()) return false;
      if (hasWithinDagPriority()) {
        if (getWithinDagPriority()
            != other.getWithinDagPriority()) return false;
      }
      if (hasDagStartTime() != other.hasDagStartTime()) return false;
      if (hasDagStartTime()) {
        if (getDagStartTime()
            != other.getDagStartTime()) return false;
      }
      if (hasFirstAttemptStartTime() != other.hasFirstAttemptStartTime()) return false;
      if (hasFirstAttemptStartTime()) {
        if (getFirstAttemptStartTime()
            != other.getFirstAttemptStartTime()) return false;
      }
      if (hasCurrentAttemptStartTime() != other.hasCurrentAttemptStartTime()) return false;
      if (hasCurrentAttemptStartTime()) {
        if (getCurrentAttemptStartTime()
            != other.getCurrentAttemptStartTime()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasNumSelfAndUpstreamTasks()) {
        hash = (37 * hash) + NUM_SELF_AND_UPSTREAM_TASKS_FIELD_NUMBER;
        hash = (53 * hash) + getNumSelfAndUpstreamTasks();
      }
      if (hasNumSelfAndUpstreamCompletedTasks()) {
        hash = (37 * hash) + NUM_SELF_AND_UPSTREAM_COMPLETED_TASKS_FIELD_NUMBER;
        hash = (53 * hash) + getNumSelfAndUpstreamCompletedTasks();
      }
      if (hasWithinDagPriority()) {
        hash = (37 * hash) + WITHIN_DAG_PRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getWithinDagPriority();
      }
      if (hasDagStartTime()) {
        hash = (37 * hash) + DAG_START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getDagStartTime());
      }
      if (hasFirstAttemptStartTime()) {
        hash = (37 * hash) + FIRST_ATTEMPT_START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getFirstAttemptStartTime());
      }
      if (hasCurrentAttemptStartTime()) {
        hash = (37 * hash) + CURRENT_ATTEMPT_START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getCurrentAttemptStartTime());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code FragmentRuntimeInfo}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:FragmentRuntimeInfo)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        numSelfAndUpstreamTasks_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        numSelfAndUpstreamCompletedTasks_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        withinDagPriority_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        dagStartTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        firstAttemptStartTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        currentAttemptStartTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_FragmentRuntimeInfo_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.numSelfAndUpstreamTasks_ = numSelfAndUpstreamTasks_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.numSelfAndUpstreamCompletedTasks_ = numSelfAndUpstreamCompletedTasks_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.withinDagPriority_ = withinDagPriority_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.dagStartTime_ = dagStartTime_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          result.firstAttemptStartTime_ = firstAttemptStartTime_;
          to_bitField0_ |= 0x00000010;
        }
        if (((from_bitField0_ & 0x00000020) != 0)) {
          result.currentAttemptStartTime_ = currentAttemptStartTime_;
          to_bitField0_ |= 0x00000020;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance()) return this;
        if (other.hasNumSelfAndUpstreamTasks()) {
          setNumSelfAndUpstreamTasks(other.getNumSelfAndUpstreamTasks());
        }
        if (other.hasNumSelfAndUpstreamCompletedTasks()) {
          setNumSelfAndUpstreamCompletedTasks(other.getNumSelfAndUpstreamCompletedTasks());
        }
        if (other.hasWithinDagPriority()) {
          setWithinDagPriority(other.getWithinDagPriority());
        }
        if (other.hasDagStartTime()) {
          setDagStartTime(other.getDagStartTime());
        }
        if (other.hasFirstAttemptStartTime()) {
          setFirstAttemptStartTime(other.getFirstAttemptStartTime());
        }
        if (other.hasCurrentAttemptStartTime()) {
          setCurrentAttemptStartTime(other.getCurrentAttemptStartTime());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int numSelfAndUpstreamTasks_ ;
      /**
       * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
       * @return Whether the numSelfAndUpstreamTasks field is set.
       */
      @java.lang.Override
      public boolean hasNumSelfAndUpstreamTasks() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
       * @return The numSelfAndUpstreamTasks.
       */
      @java.lang.Override
      public int getNumSelfAndUpstreamTasks() {
        return numSelfAndUpstreamTasks_;
      }
      /**
       * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
       * @param value The numSelfAndUpstreamTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumSelfAndUpstreamTasks(int value) {
        bitField0_ |= 0x00000001;
        numSelfAndUpstreamTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 num_self_and_upstream_tasks = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumSelfAndUpstreamTasks() {
        bitField0_ = (bitField0_ & ~0x00000001);
        numSelfAndUpstreamTasks_ = 0;
        onChanged();
        return this;
      }

      private int numSelfAndUpstreamCompletedTasks_ ;
      /**
       * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
       * @return Whether the numSelfAndUpstreamCompletedTasks field is set.
       */
      @java.lang.Override
      public boolean hasNumSelfAndUpstreamCompletedTasks() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
       * @return The numSelfAndUpstreamCompletedTasks.
       */
      @java.lang.Override
      public int getNumSelfAndUpstreamCompletedTasks() {
        return numSelfAndUpstreamCompletedTasks_;
      }
      /**
       * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
       * @param value The numSelfAndUpstreamCompletedTasks to set.
       * @return This builder for chaining.
       */
      public Builder setNumSelfAndUpstreamCompletedTasks(int value) {
        bitField0_ |= 0x00000002;
        numSelfAndUpstreamCompletedTasks_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 num_self_and_upstream_completed_tasks = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearNumSelfAndUpstreamCompletedTasks() {
        bitField0_ = (bitField0_ & ~0x00000002);
        numSelfAndUpstreamCompletedTasks_ = 0;
        onChanged();
        return this;
      }

      private int withinDagPriority_ ;
      /**
       * <code>optional int32 within_dag_priority = 3;</code>
       * @return Whether the withinDagPriority field is set.
       */
      @java.lang.Override
      public boolean hasWithinDagPriority() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int32 within_dag_priority = 3;</code>
       * @return The withinDagPriority.
       */
      @java.lang.Override
      public int getWithinDagPriority() {
        return withinDagPriority_;
      }
      /**
       * <code>optional int32 within_dag_priority = 3;</code>
       * @param value The withinDagPriority to set.
       * @return This builder for chaining.
       */
      public Builder setWithinDagPriority(int value) {
        bitField0_ |= 0x00000004;
        withinDagPriority_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 within_dag_priority = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearWithinDagPriority() {
        bitField0_ = (bitField0_ & ~0x00000004);
        withinDagPriority_ = 0;
        onChanged();
        return this;
      }

      private long dagStartTime_ ;
      /**
       * <code>optional int64 dag_start_time = 4;</code>
       * @return Whether the dagStartTime field is set.
       */
      @java.lang.Override
      public boolean hasDagStartTime() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int64 dag_start_time = 4;</code>
       * @return The dagStartTime.
       */
      @java.lang.Override
      public long getDagStartTime() {
        return dagStartTime_;
      }
      /**
       * <code>optional int64 dag_start_time = 4;</code>
       * @param value The dagStartTime to set.
       * @return This builder for chaining.
       */
      public Builder setDagStartTime(long value) {
        bitField0_ |= 0x00000008;
        dagStartTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 dag_start_time = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDagStartTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        dagStartTime_ = 0L;
        onChanged();
        return this;
      }

      private long firstAttemptStartTime_ ;
      /**
       * <code>optional int64 first_attempt_start_time = 5;</code>
       * @return Whether the firstAttemptStartTime field is set.
       */
      @java.lang.Override
      public boolean hasFirstAttemptStartTime() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional int64 first_attempt_start_time = 5;</code>
       * @return The firstAttemptStartTime.
       */
      @java.lang.Override
      public long getFirstAttemptStartTime() {
        return firstAttemptStartTime_;
      }
      /**
       * <code>optional int64 first_attempt_start_time = 5;</code>
       * @param value The firstAttemptStartTime to set.
       * @return This builder for chaining.
       */
      public Builder setFirstAttemptStartTime(long value) {
        bitField0_ |= 0x00000010;
        firstAttemptStartTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 first_attempt_start_time = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearFirstAttemptStartTime() {
        bitField0_ = (bitField0_ & ~0x00000010);
        firstAttemptStartTime_ = 0L;
        onChanged();
        return this;
      }

      private long currentAttemptStartTime_ ;
      /**
       * <code>optional int64 current_attempt_start_time = 6;</code>
       * @return Whether the currentAttemptStartTime field is set.
       */
      @java.lang.Override
      public boolean hasCurrentAttemptStartTime() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional int64 current_attempt_start_time = 6;</code>
       * @return The currentAttemptStartTime.
       */
      @java.lang.Override
      public long getCurrentAttemptStartTime() {
        return currentAttemptStartTime_;
      }
      /**
       * <code>optional int64 current_attempt_start_time = 6;</code>
       * @param value The currentAttemptStartTime to set.
       * @return This builder for chaining.
       */
      public Builder setCurrentAttemptStartTime(long value) {
        bitField0_ |= 0x00000020;
        currentAttemptStartTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 current_attempt_start_time = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearCurrentAttemptStartTime() {
        bitField0_ = (bitField0_ & ~0x00000020);
        currentAttemptStartTime_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:FragmentRuntimeInfo)
    }

    // @@protoc_insertion_point(class_scope:FragmentRuntimeInfo)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<FragmentRuntimeInfo>
        PARSER = new com.google.protobuf.AbstractParser<FragmentRuntimeInfo>() {
      @java.lang.Override
      public FragmentRuntimeInfo parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FragmentRuntimeInfo(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<FragmentRuntimeInfo> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<FragmentRuntimeInfo> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface QueryIdentifierProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:QueryIdentifierProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string application_id_string = 1;</code>
     * @return Whether the applicationIdString field is set.
     */
    boolean hasApplicationIdString();
    /**
     * <code>optional string application_id_string = 1;</code>
     * @return The applicationIdString.
     */
    java.lang.String getApplicationIdString();
    /**
     * <code>optional string application_id_string = 1;</code>
     * @return The bytes for applicationIdString.
     */
    com.google.protobuf.ByteString
        getApplicationIdStringBytes();

    /**
     * <code>optional int32 dag_index = 2;</code>
     * @return Whether the dagIndex field is set.
     */
    boolean hasDagIndex();
    /**
     * <code>optional int32 dag_index = 2;</code>
     * @return The dagIndex.
     */
    int getDagIndex();

    /**
     * <code>optional int32 app_attempt_number = 3;</code>
     * @return Whether the appAttemptNumber field is set.
     */
    boolean hasAppAttemptNumber();
    /**
     * <code>optional int32 app_attempt_number = 3;</code>
     * @return The appAttemptNumber.
     */
    int getAppAttemptNumber();
  }
  /**
   * Protobuf type {@code QueryIdentifierProto}
   */
  public static final class QueryIdentifierProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:QueryIdentifierProto)
      QueryIdentifierProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use QueryIdentifierProto.newBuilder() to construct.
    private QueryIdentifierProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private QueryIdentifierProto() {
      applicationIdString_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new QueryIdentifierProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private QueryIdentifierProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              applicationIdString_ = bs;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              dagIndex_ = input.readInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              appAttemptNumber_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder.class);
    }

    private int bitField0_;
    public static final int APPLICATION_ID_STRING_FIELD_NUMBER = 1;
    private volatile java.lang.Object applicationIdString_;
    /**
     * <code>optional string application_id_string = 1;</code>
     * @return Whether the applicationIdString field is set.
     */
    @java.lang.Override
    public boolean hasApplicationIdString() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string application_id_string = 1;</code>
     * @return The applicationIdString.
     */
    @java.lang.Override
    public java.lang.String getApplicationIdString() {
      java.lang.Object ref = applicationIdString_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          applicationIdString_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string application_id_string = 1;</code>
     * @return The bytes for applicationIdString.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getApplicationIdStringBytes() {
      java.lang.Object ref = applicationIdString_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        applicationIdString_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DAG_INDEX_FIELD_NUMBER = 2;
    private int dagIndex_;
    /**
     * <code>optional int32 dag_index = 2;</code>
     * @return Whether the dagIndex field is set.
     */
    @java.lang.Override
    public boolean hasDagIndex() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 dag_index = 2;</code>
     * @return The dagIndex.
     */
    @java.lang.Override
    public int getDagIndex() {
      return dagIndex_;
    }

    public static final int APP_ATTEMPT_NUMBER_FIELD_NUMBER = 3;
    private int appAttemptNumber_;
    /**
     * <code>optional int32 app_attempt_number = 3;</code>
     * @return Whether the appAttemptNumber field is set.
     */
    @java.lang.Override
    public boolean hasAppAttemptNumber() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int32 app_attempt_number = 3;</code>
     * @return The appAttemptNumber.
     */
    @java.lang.Override
    public int getAppAttemptNumber() {
      return appAttemptNumber_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, applicationIdString_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, dagIndex_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, appAttemptNumber_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, applicationIdString_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, dagIndex_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, appAttemptNumber_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) obj;

      if (hasApplicationIdString() != other.hasApplicationIdString()) return false;
      if (hasApplicationIdString()) {
        if (!getApplicationIdString()
            .equals(other.getApplicationIdString())) return false;
      }
      if (hasDagIndex() != other.hasDagIndex()) return false;
      if (hasDagIndex()) {
        if (getDagIndex()
            != other.getDagIndex()) return false;
      }
      if (hasAppAttemptNumber() != other.hasAppAttemptNumber()) return false;
      if (hasAppAttemptNumber()) {
        if (getAppAttemptNumber()
            != other.getAppAttemptNumber()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasApplicationIdString()) {
        hash = (37 * hash) + APPLICATION_ID_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationIdString().hashCode();
      }
      if (hasDagIndex()) {
        hash = (37 * hash) + DAG_INDEX_FIELD_NUMBER;
        hash = (53 * hash) + getDagIndex();
      }
      if (hasAppAttemptNumber()) {
        hash = (37 * hash) + APP_ATTEMPT_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + getAppAttemptNumber();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code QueryIdentifierProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:QueryIdentifierProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        applicationIdString_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        dagIndex_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        appAttemptNumber_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryIdentifierProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.applicationIdString_ = applicationIdString_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.dagIndex_ = dagIndex_;
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.appAttemptNumber_ = appAttemptNumber_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) return this;
        if (other.hasApplicationIdString()) {
          bitField0_ |= 0x00000001;
          applicationIdString_ = other.applicationIdString_;
          onChanged();
        }
        if (other.hasDagIndex()) {
          setDagIndex(other.getDagIndex());
        }
        if (other.hasAppAttemptNumber()) {
          setAppAttemptNumber(other.getAppAttemptNumber());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object applicationIdString_ = "";
      /**
       * <code>optional string application_id_string = 1;</code>
       * @return Whether the applicationIdString field is set.
       */
      public boolean hasApplicationIdString() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string application_id_string = 1;</code>
       * @return The applicationIdString.
       */
      public java.lang.String getApplicationIdString() {
        java.lang.Object ref = applicationIdString_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            applicationIdString_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string application_id_string = 1;</code>
       * @return The bytes for applicationIdString.
       */
      public com.google.protobuf.ByteString
          getApplicationIdStringBytes() {
        java.lang.Object ref = applicationIdString_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          applicationIdString_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string application_id_string = 1;</code>
       * @param value The applicationIdString to set.
       * @return This builder for chaining.
       */
      public Builder setApplicationIdString(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        applicationIdString_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string application_id_string = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearApplicationIdString() {
        bitField0_ = (bitField0_ & ~0x00000001);
        applicationIdString_ = getDefaultInstance().getApplicationIdString();
        onChanged();
        return this;
      }
      /**
       * <code>optional string application_id_string = 1;</code>
       * @param value The bytes for applicationIdString to set.
       * @return This builder for chaining.
       */
      public Builder setApplicationIdStringBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        applicationIdString_ = value;
        onChanged();
        return this;
      }

      private int dagIndex_ ;
      /**
       * <code>optional int32 dag_index = 2;</code>
       * @return Whether the dagIndex field is set.
       */
      @java.lang.Override
      public boolean hasDagIndex() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 dag_index = 2;</code>
       * @return The dagIndex.
       */
      @java.lang.Override
      public int getDagIndex() {
        return dagIndex_;
      }
      /**
       * <code>optional int32 dag_index = 2;</code>
       * @param value The dagIndex to set.
       * @return This builder for chaining.
       */
      public Builder setDagIndex(int value) {
        bitField0_ |= 0x00000002;
        dagIndex_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 dag_index = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDagIndex() {
        bitField0_ = (bitField0_ & ~0x00000002);
        dagIndex_ = 0;
        onChanged();
        return this;
      }

      private int appAttemptNumber_ ;
      /**
       * <code>optional int32 app_attempt_number = 3;</code>
       * @return Whether the appAttemptNumber field is set.
       */
      @java.lang.Override
      public boolean hasAppAttemptNumber() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int32 app_attempt_number = 3;</code>
       * @return The appAttemptNumber.
       */
      @java.lang.Override
      public int getAppAttemptNumber() {
        return appAttemptNumber_;
      }
      /**
       * <code>optional int32 app_attempt_number = 3;</code>
       * @param value The appAttemptNumber to set.
       * @return This builder for chaining.
       */
      public Builder setAppAttemptNumber(int value) {
        bitField0_ |= 0x00000004;
        appAttemptNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 app_attempt_number = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAppAttemptNumber() {
        bitField0_ = (bitField0_ & ~0x00000004);
        appAttemptNumber_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:QueryIdentifierProto)
    }

    // @@protoc_insertion_point(class_scope:QueryIdentifierProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryIdentifierProto>
        PARSER = new com.google.protobuf.AbstractParser<QueryIdentifierProto>() {
      @java.lang.Override
      public QueryIdentifierProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new QueryIdentifierProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<QueryIdentifierProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<QueryIdentifierProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface NotTezEventOrBuilder extends
      // @@protoc_insertion_point(interface_extends:NotTezEvent)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required bytes input_event_proto_bytes = 1;</code>
     * @return Whether the inputEventProtoBytes field is set.
     */
    boolean hasInputEventProtoBytes();
    /**
     * <code>required bytes input_event_proto_bytes = 1;</code>
     * @return The inputEventProtoBytes.
     */
    com.google.protobuf.ByteString getInputEventProtoBytes();

    /**
     * <code>required string vertex_name = 2;</code>
     * @return Whether the vertexName field is set.
     */
    boolean hasVertexName();
    /**
     * <code>required string vertex_name = 2;</code>
     * @return The vertexName.
     */
    java.lang.String getVertexName();
    /**
     * <code>required string vertex_name = 2;</code>
     * @return The bytes for vertexName.
     */
    com.google.protobuf.ByteString
        getVertexNameBytes();

    /**
     * <code>required string dest_input_name = 3;</code>
     * @return Whether the destInputName field is set.
     */
    boolean hasDestInputName();
    /**
     * <code>required string dest_input_name = 3;</code>
     * @return The destInputName.
     */
    java.lang.String getDestInputName();
    /**
     * <code>required string dest_input_name = 3;</code>
     * @return The bytes for destInputName.
     */
    com.google.protobuf.ByteString
        getDestInputNameBytes();

    /**
     * <code>optional int32 key_id = 4;</code>
     * @return Whether the keyId field is set.
     */
    boolean hasKeyId();
    /**
     * <code>optional int32 key_id = 4;</code>
     * @return The keyId.
     */
    int getKeyId();
  }
  /**
   * <pre>
   **
   * Tez API implementation derives an enum value from instanceof on the event, then uses that enum
   * in a giant switch statement to re-implement virtual functions. In addition the event classes
   * are final and serialization is a mix of writables and protobuf. As a result, it is impossible
   * three times over to add anything there. So, we'd do our own "inspired" serialization.
   * Eventually we'll move away from events for API.
   * </pre>
   *
   * Protobuf type {@code NotTezEvent}
   */
  public static final class NotTezEvent extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:NotTezEvent)
      NotTezEventOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use NotTezEvent.newBuilder() to construct.
    private NotTezEvent(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private NotTezEvent() {
      inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
      vertexName_ = "";
      destInputName_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new NotTezEvent();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private NotTezEvent(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              bitField0_ |= 0x00000001;
              inputEventProtoBytes_ = input.readBytes();
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              vertexName_ = bs;
              break;
            }
            case 26: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000004;
              destInputName_ = bs;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              keyId_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.Builder.class);
    }

    private int bitField0_;
    public static final int INPUT_EVENT_PROTO_BYTES_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString inputEventProtoBytes_;
    /**
     * <code>required bytes input_event_proto_bytes = 1;</code>
     * @return Whether the inputEventProtoBytes field is set.
     */
    @java.lang.Override
    public boolean hasInputEventProtoBytes() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required bytes input_event_proto_bytes = 1;</code>
     * @return The inputEventProtoBytes.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getInputEventProtoBytes() {
      return inputEventProtoBytes_;
    }

    public static final int VERTEX_NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object vertexName_;
    /**
     * <code>required string vertex_name = 2;</code>
     * @return Whether the vertexName field is set.
     */
    @java.lang.Override
    public boolean hasVertexName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required string vertex_name = 2;</code>
     * @return The vertexName.
     */
    @java.lang.Override
    public java.lang.String getVertexName() {
      java.lang.Object ref = vertexName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          vertexName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string vertex_name = 2;</code>
     * @return The bytes for vertexName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getVertexNameBytes() {
      java.lang.Object ref = vertexName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        vertexName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DEST_INPUT_NAME_FIELD_NUMBER = 3;
    private volatile java.lang.Object destInputName_;
    /**
     * <code>required string dest_input_name = 3;</code>
     * @return Whether the destInputName field is set.
     */
    @java.lang.Override
    public boolean hasDestInputName() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>required string dest_input_name = 3;</code>
     * @return The destInputName.
     */
    @java.lang.Override
    public java.lang.String getDestInputName() {
      java.lang.Object ref = destInputName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          destInputName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string dest_input_name = 3;</code>
     * @return The bytes for destInputName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDestInputNameBytes() {
      java.lang.Object ref = destInputName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        destInputName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int KEY_ID_FIELD_NUMBER = 4;
    private int keyId_;
    /**
     * <code>optional int32 key_id = 4;</code>
     * @return Whether the keyId field is set.
     */
    @java.lang.Override
    public boolean hasKeyId() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int32 key_id = 4;</code>
     * @return The keyId.
     */
    @java.lang.Override
    public int getKeyId() {
      return keyId_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasInputEventProtoBytes()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasVertexName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasDestInputName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(1, inputEventProtoBytes_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, vertexName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, destInputName_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt32(4, keyId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, inputEventProtoBytes_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, vertexName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, destInputName_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, keyId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) obj;

      if (hasInputEventProtoBytes() != other.hasInputEventProtoBytes()) return false;
      if (hasInputEventProtoBytes()) {
        if (!getInputEventProtoBytes()
            .equals(other.getInputEventProtoBytes())) return false;
      }
      if (hasVertexName() != other.hasVertexName()) return false;
      if (hasVertexName()) {
        if (!getVertexName()
            .equals(other.getVertexName())) return false;
      }
      if (hasDestInputName() != other.hasDestInputName()) return false;
      if (hasDestInputName()) {
        if (!getDestInputName()
            .equals(other.getDestInputName())) return false;
      }
      if (hasKeyId() != other.hasKeyId()) return false;
      if (hasKeyId()) {
        if (getKeyId()
            != other.getKeyId()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasInputEventProtoBytes()) {
        hash = (37 * hash) + INPUT_EVENT_PROTO_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getInputEventProtoBytes().hashCode();
      }
      if (hasVertexName()) {
        hash = (37 * hash) + VERTEX_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getVertexName().hashCode();
      }
      if (hasDestInputName()) {
        hash = (37 * hash) + DEST_INPUT_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getDestInputName().hashCode();
      }
      if (hasKeyId()) {
        hash = (37 * hash) + KEY_ID_FIELD_NUMBER;
        hash = (53 * hash) + getKeyId();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     **
     * Tez API implementation derives an enum value from instanceof on the event, then uses that enum
     * in a giant switch statement to re-implement virtual functions. In addition the event classes
     * are final and serialization is a mix of writables and protobuf. As a result, it is impossible
     * three times over to add anything there. So, we'd do our own "inspired" serialization.
     * Eventually we'll move away from events for API.
     * </pre>
     *
     * Protobuf type {@code NotTezEvent}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:NotTezEvent)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEventOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        vertexName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        destInputName_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        keyId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_NotTezEvent_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.inputEventProtoBytes_ = inputEventProtoBytes_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.vertexName_ = vertexName_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.destInputName_ = destInputName_;
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.keyId_ = keyId_;
          to_bitField0_ |= 0x00000008;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent.getDefaultInstance()) return this;
        if (other.hasInputEventProtoBytes()) {
          setInputEventProtoBytes(other.getInputEventProtoBytes());
        }
        if (other.hasVertexName()) {
          bitField0_ |= 0x00000002;
          vertexName_ = other.vertexName_;
          onChanged();
        }
        if (other.hasDestInputName()) {
          bitField0_ |= 0x00000004;
          destInputName_ = other.destInputName_;
          onChanged();
        }
        if (other.hasKeyId()) {
          setKeyId(other.getKeyId());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasInputEventProtoBytes()) {
          return false;
        }
        if (!hasVertexName()) {
          return false;
        }
        if (!hasDestInputName()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString inputEventProtoBytes_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes input_event_proto_bytes = 1;</code>
       * @return Whether the inputEventProtoBytes field is set.
       */
      @java.lang.Override
      public boolean hasInputEventProtoBytes() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required bytes input_event_proto_bytes = 1;</code>
       * @return The inputEventProtoBytes.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getInputEventProtoBytes() {
        return inputEventProtoBytes_;
      }
      /**
       * <code>required bytes input_event_proto_bytes = 1;</code>
       * @param value The inputEventProtoBytes to set.
       * @return This builder for chaining.
       */
      public Builder setInputEventProtoBytes(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        inputEventProtoBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes input_event_proto_bytes = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearInputEventProtoBytes() {
        bitField0_ = (bitField0_ & ~0x00000001);
        inputEventProtoBytes_ = getDefaultInstance().getInputEventProtoBytes();
        onChanged();
        return this;
      }

      private java.lang.Object vertexName_ = "";
      /**
       * <code>required string vertex_name = 2;</code>
       * @return Whether the vertexName field is set.
       */
      public boolean hasVertexName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required string vertex_name = 2;</code>
       * @return The vertexName.
       */
      public java.lang.String getVertexName() {
        java.lang.Object ref = vertexName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            vertexName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string vertex_name = 2;</code>
       * @return The bytes for vertexName.
       */
      public com.google.protobuf.ByteString
          getVertexNameBytes() {
        java.lang.Object ref = vertexName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          vertexName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string vertex_name = 2;</code>
       * @param value The vertexName to set.
       * @return This builder for chaining.
       */
      public Builder setVertexName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        vertexName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string vertex_name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearVertexName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        vertexName_ = getDefaultInstance().getVertexName();
        onChanged();
        return this;
      }
      /**
       * <code>required string vertex_name = 2;</code>
       * @param value The bytes for vertexName to set.
       * @return This builder for chaining.
       */
      public Builder setVertexNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        vertexName_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object destInputName_ = "";
      /**
       * <code>required string dest_input_name = 3;</code>
       * @return Whether the destInputName field is set.
       */
      public boolean hasDestInputName() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>required string dest_input_name = 3;</code>
       * @return The destInputName.
       */
      public java.lang.String getDestInputName() {
        java.lang.Object ref = destInputName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            destInputName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string dest_input_name = 3;</code>
       * @return The bytes for destInputName.
       */
      public com.google.protobuf.ByteString
          getDestInputNameBytes() {
        java.lang.Object ref = destInputName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          destInputName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string dest_input_name = 3;</code>
       * @param value The destInputName to set.
       * @return This builder for chaining.
       */
      public Builder setDestInputName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        destInputName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string dest_input_name = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDestInputName() {
        bitField0_ = (bitField0_ & ~0x00000004);
        destInputName_ = getDefaultInstance().getDestInputName();
        onChanged();
        return this;
      }
      /**
       * <code>required string dest_input_name = 3;</code>
       * @param value The bytes for destInputName to set.
       * @return This builder for chaining.
       */
      public Builder setDestInputNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        destInputName_ = value;
        onChanged();
        return this;
      }

      private int keyId_ ;
      /**
       * <code>optional int32 key_id = 4;</code>
       * @return Whether the keyId field is set.
       */
      @java.lang.Override
      public boolean hasKeyId() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int32 key_id = 4;</code>
       * @return The keyId.
       */
      @java.lang.Override
      public int getKeyId() {
        return keyId_;
      }
      /**
       * <code>optional int32 key_id = 4;</code>
       * @param value The keyId to set.
       * @return This builder for chaining.
       */
      public Builder setKeyId(int value) {
        bitField0_ |= 0x00000008;
        keyId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 key_id = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearKeyId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        keyId_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:NotTezEvent)
    }

    // @@protoc_insertion_point(class_scope:NotTezEvent)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<NotTezEvent>
        PARSER = new com.google.protobuf.AbstractParser<NotTezEvent>() {
      @java.lang.Override
      public NotTezEvent parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NotTezEvent(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<NotTezEvent> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<NotTezEvent> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.NotTezEvent getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SubmitWorkRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SubmitWorkRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     * @return Whether the workSpec field is set.
     */
    boolean hasWorkSpec();
    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     * @return The workSpec.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec();
    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder getWorkSpecOrBuilder();

    /**
     * <code>optional bytes work_spec_signature = 2;</code>
     * @return Whether the workSpecSignature field is set.
     */
    boolean hasWorkSpecSignature();
    /**
     * <code>optional bytes work_spec_signature = 2;</code>
     * @return The workSpecSignature.
     */
    com.google.protobuf.ByteString getWorkSpecSignature();

    /**
     * <code>optional int32 fragment_number = 3;</code>
     * @return Whether the fragmentNumber field is set.
     */
    boolean hasFragmentNumber();
    /**
     * <code>optional int32 fragment_number = 3;</code>
     * @return The fragmentNumber.
     */
    int getFragmentNumber();

    /**
     * <code>optional int32 attempt_number = 4;</code>
     * @return Whether the attemptNumber field is set.
     */
    boolean hasAttemptNumber();
    /**
     * <code>optional int32 attempt_number = 4;</code>
     * @return The attemptNumber.
     */
    int getAttemptNumber();

    /**
     * <code>optional string container_id_string = 5;</code>
     * @return Whether the containerIdString field is set.
     */
    boolean hasContainerIdString();
    /**
     * <code>optional string container_id_string = 5;</code>
     * @return The containerIdString.
     */
    java.lang.String getContainerIdString();
    /**
     * <code>optional string container_id_string = 5;</code>
     * @return The bytes for containerIdString.
     */
    com.google.protobuf.ByteString
        getContainerIdStringBytes();

    /**
     * <code>optional string am_host = 6;</code>
     * @return Whether the amHost field is set.
     */
    boolean hasAmHost();
    /**
     * <code>optional string am_host = 6;</code>
     * @return The amHost.
     */
    java.lang.String getAmHost();
    /**
     * <code>optional string am_host = 6;</code>
     * @return The bytes for amHost.
     */
    com.google.protobuf.ByteString
        getAmHostBytes();

    /**
     * <code>optional int32 am_port = 7;</code>
     * @return Whether the amPort field is set.
     */
    boolean hasAmPort();
    /**
     * <code>optional int32 am_port = 7;</code>
     * @return The amPort.
     */
    int getAmPort();

    /**
     * <pre>
     * Credentials are not signed - the client can add e.g. his own HDFS tokens.
     * </pre>
     *
     * <code>optional bytes credentials_binary = 8;</code>
     * @return Whether the credentialsBinary field is set.
     */
    boolean hasCredentialsBinary();
    /**
     * <pre>
     * Credentials are not signed - the client can add e.g. his own HDFS tokens.
     * </pre>
     *
     * <code>optional bytes credentials_binary = 8;</code>
     * @return The credentialsBinary.
     */
    com.google.protobuf.ByteString getCredentialsBinary();

    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     * @return Whether the fragmentRuntimeInfo field is set.
     */
    boolean hasFragmentRuntimeInfo();
    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     * @return The fragmentRuntimeInfo.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo();
    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder();

    /**
     * <pre>
     * Serialized (and signed) NotTezEvent; used only for external clients for now.
     * </pre>
     *
     * <code>optional bytes initial_event_bytes = 10;</code>
     * @return Whether the initialEventBytes field is set.
     */
    boolean hasInitialEventBytes();
    /**
     * <pre>
     * Serialized (and signed) NotTezEvent; used only for external clients for now.
     * </pre>
     *
     * <code>optional bytes initial_event_bytes = 10;</code>
     * @return The initialEventBytes.
     */
    com.google.protobuf.ByteString getInitialEventBytes();

    /**
     * <code>optional bytes initial_event_signature = 11;</code>
     * @return Whether the initialEventSignature field is set.
     */
    boolean hasInitialEventSignature();
    /**
     * <code>optional bytes initial_event_signature = 11;</code>
     * @return The initialEventSignature.
     */
    com.google.protobuf.ByteString getInitialEventSignature();

    /**
     * <code>optional bool is_guaranteed = 12 [default = false];</code>
     * @return Whether the isGuaranteed field is set.
     */
    boolean hasIsGuaranteed();
    /**
     * <code>optional bool is_guaranteed = 12 [default = false];</code>
     * @return The isGuaranteed.
     */
    boolean getIsGuaranteed();

    /**
     * <code>optional string jwt = 13;</code>
     * @return Whether the jwt field is set.
     */
    boolean hasJwt();
    /**
     * <code>optional string jwt = 13;</code>
     * @return The jwt.
     */
    java.lang.String getJwt();
    /**
     * <code>optional string jwt = 13;</code>
     * @return The bytes for jwt.
     */
    com.google.protobuf.ByteString
        getJwtBytes();

    /**
     * <code>optional bool is_external_client_request = 14 [default = false];</code>
     * @return Whether the isExternalClientRequest field is set.
     */
    boolean hasIsExternalClientRequest();
    /**
     * <code>optional bool is_external_client_request = 14 [default = false];</code>
     * @return The isExternalClientRequest.
     */
    boolean getIsExternalClientRequest();
  }
  /**
   * Protobuf type {@code SubmitWorkRequestProto}
   */
  public static final class SubmitWorkRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SubmitWorkRequestProto)
      SubmitWorkRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SubmitWorkRequestProto.newBuilder() to construct.
    private SubmitWorkRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SubmitWorkRequestProto() {
      workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
      containerIdString_ = "";
      amHost_ = "";
      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
      initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
      initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
      jwt_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SubmitWorkRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitWorkRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = workSpec_.toBuilder();
              }
              workSpec_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(workSpec_);
                workSpec_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              workSpecSignature_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              fragmentNumber_ = input.readInt32();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              attemptNumber_ = input.readInt32();
              break;
            }
            case 42: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000010;
              containerIdString_ = bs;
              break;
            }
            case 50: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000020;
              amHost_ = bs;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              amPort_ = input.readInt32();
              break;
            }
            case 66: {
              bitField0_ |= 0x00000080;
              credentialsBinary_ = input.readBytes();
              break;
            }
            case 74: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder subBuilder = null;
              if (((bitField0_ & 0x00000100) != 0)) {
                subBuilder = fragmentRuntimeInfo_.toBuilder();
              }
              fragmentRuntimeInfo_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(fragmentRuntimeInfo_);
                fragmentRuntimeInfo_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000100;
              break;
            }
            case 82: {
              bitField0_ |= 0x00000200;
              initialEventBytes_ = input.readBytes();
              break;
            }
            case 90: {
              bitField0_ |= 0x00000400;
              initialEventSignature_ = input.readBytes();
              break;
            }
            case 96: {
              bitField0_ |= 0x00000800;
              isGuaranteed_ = input.readBool();
              break;
            }
            case 106: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00001000;
              jwt_ = bs;
              break;
            }
            case 112: {
              bitField0_ |= 0x00002000;
              isExternalClientRequest_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int WORK_SPEC_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary workSpec_;
    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     * @return Whether the workSpec field is set.
     */
    @java.lang.Override
    public boolean hasWorkSpec() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     * @return The workSpec.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec() {
      return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
    }
    /**
     * <code>optional .VertexOrBinary work_spec = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder getWorkSpecOrBuilder() {
      return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
    }

    public static final int WORK_SPEC_SIGNATURE_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString workSpecSignature_;
    /**
     * <code>optional bytes work_spec_signature = 2;</code>
     * @return Whether the workSpecSignature field is set.
     */
    @java.lang.Override
    public boolean hasWorkSpecSignature() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional bytes work_spec_signature = 2;</code>
     * @return The workSpecSignature.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getWorkSpecSignature() {
      return workSpecSignature_;
    }

    public static final int FRAGMENT_NUMBER_FIELD_NUMBER = 3;
    private int fragmentNumber_;
    /**
     * <code>optional int32 fragment_number = 3;</code>
     * @return Whether the fragmentNumber field is set.
     */
    @java.lang.Override
    public boolean hasFragmentNumber() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional int32 fragment_number = 3;</code>
     * @return The fragmentNumber.
     */
    @java.lang.Override
    public int getFragmentNumber() {
      return fragmentNumber_;
    }

    public static final int ATTEMPT_NUMBER_FIELD_NUMBER = 4;
    private int attemptNumber_;
    /**
     * <code>optional int32 attempt_number = 4;</code>
     * @return Whether the attemptNumber field is set.
     */
    @java.lang.Override
    public boolean hasAttemptNumber() {
      return ((bitField0_ & 0x00000008) != 0);
    }
    /**
     * <code>optional int32 attempt_number = 4;</code>
     * @return The attemptNumber.
     */
    @java.lang.Override
    public int getAttemptNumber() {
      return attemptNumber_;
    }

    public static final int CONTAINER_ID_STRING_FIELD_NUMBER = 5;
    private volatile java.lang.Object containerIdString_;
    /**
     * <code>optional string container_id_string = 5;</code>
     * @return Whether the containerIdString field is set.
     */
    @java.lang.Override
    public boolean hasContainerIdString() {
      return ((bitField0_ & 0x00000010) != 0);
    }
    /**
     * <code>optional string container_id_string = 5;</code>
     * @return The containerIdString.
     */
    @java.lang.Override
    public java.lang.String getContainerIdString() {
      java.lang.Object ref = containerIdString_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          containerIdString_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string container_id_string = 5;</code>
     * @return The bytes for containerIdString.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getContainerIdStringBytes() {
      java.lang.Object ref = containerIdString_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        containerIdString_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int AM_HOST_FIELD_NUMBER = 6;
    private volatile java.lang.Object amHost_;
    /**
     * <code>optional string am_host = 6;</code>
     * @return Whether the amHost field is set.
     */
    @java.lang.Override
    public boolean hasAmHost() {
      return ((bitField0_ & 0x00000020) != 0);
    }
    /**
     * <code>optional string am_host = 6;</code>
     * @return The amHost.
     */
    @java.lang.Override
    public java.lang.String getAmHost() {
      java.lang.Object ref = amHost_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          amHost_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string am_host = 6;</code>
     * @return The bytes for amHost.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getAmHostBytes() {
      java.lang.Object ref = amHost_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        amHost_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int AM_PORT_FIELD_NUMBER = 7;
    private int amPort_;
    /**
     * <code>optional int32 am_port = 7;</code>
     * @return Whether the amPort field is set.
     */
    @java.lang.Override
    public boolean hasAmPort() {
      return ((bitField0_ & 0x00000040) != 0);
    }
    /**
     * <code>optional int32 am_port = 7;</code>
     * @return The amPort.
     */
    @java.lang.Override
    public int getAmPort() {
      return amPort_;
    }

    public static final int CREDENTIALS_BINARY_FIELD_NUMBER = 8;
    private com.google.protobuf.ByteString credentialsBinary_;
    /**
     * <pre>
     * Credentials are not signed - the client can add e.g. his own HDFS tokens.
     * </pre>
     *
     * <code>optional bytes credentials_binary = 8;</code>
     * @return Whether the credentialsBinary field is set.
     */
    @java.lang.Override
    public boolean hasCredentialsBinary() {
      return ((bitField0_ & 0x00000080) != 0);
    }
    /**
     * <pre>
     * Credentials are not signed - the client can add e.g. his own HDFS tokens.
     * </pre>
     *
     * <code>optional bytes credentials_binary = 8;</code>
     * @return The credentialsBinary.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getCredentialsBinary() {
      return credentialsBinary_;
    }

    public static final int FRAGMENT_RUNTIME_INFO_FIELD_NUMBER = 9;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo fragmentRuntimeInfo_;
    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     * @return Whether the fragmentRuntimeInfo field is set.
     */
    @java.lang.Override
    public boolean hasFragmentRuntimeInfo() {
      return ((bitField0_ & 0x00000100) != 0);
    }
    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     * @return The fragmentRuntimeInfo.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo() {
      return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
    }
    /**
     * <pre>
     * Not supported/honored for external clients right now.
     * </pre>
     *
     * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder() {
      return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
    }

    public static final int INITIAL_EVENT_BYTES_FIELD_NUMBER = 10;
    private com.google.protobuf.ByteString initialEventBytes_;
    /**
     * <pre>
     * Serialized (and signed) NotTezEvent; used only for external clients for now.
     * </pre>
     *
     * <code>optional bytes initial_event_bytes = 10;</code>
     * @return Whether the initialEventBytes field is set.
     */
    @java.lang.Override
    public boolean hasInitialEventBytes() {
      return ((bitField0_ & 0x00000200) != 0);
    }
    /**
     * <pre>
     * Serialized (and signed) NotTezEvent; used only for external clients for now.
     * </pre>
     *
     * <code>optional bytes initial_event_bytes = 10;</code>
     * @return The initialEventBytes.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getInitialEventBytes() {
      return initialEventBytes_;
    }

    public static final int INITIAL_EVENT_SIGNATURE_FIELD_NUMBER = 11;
    private com.google.protobuf.ByteString initialEventSignature_;
    /**
     * <code>optional bytes initial_event_signature = 11;</code>
     * @return Whether the initialEventSignature field is set.
     */
    @java.lang.Override
    public boolean hasInitialEventSignature() {
      return ((bitField0_ & 0x00000400) != 0);
    }
    /**
     * <code>optional bytes initial_event_signature = 11;</code>
     * @return The initialEventSignature.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getInitialEventSignature() {
      return initialEventSignature_;
    }

    public static final int IS_GUARANTEED_FIELD_NUMBER = 12;
    private boolean isGuaranteed_;
    /**
     * <code>optional bool is_guaranteed = 12 [default = false];</code>
     * @return Whether the isGuaranteed field is set.
     */
    @java.lang.Override
    public boolean hasIsGuaranteed() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <code>optional bool is_guaranteed = 12 [default = false];</code>
     * @return The isGuaranteed.
     */
    @java.lang.Override
    public boolean getIsGuaranteed() {
      return isGuaranteed_;
    }

    public static final int JWT_FIELD_NUMBER = 13;
    private volatile java.lang.Object jwt_;
    /**
     * <code>optional string jwt = 13;</code>
     * @return Whether the jwt field is set.
     */
    @java.lang.Override
    public boolean hasJwt() {
      return ((bitField0_ & 0x00001000) != 0);
    }
    /**
     * <code>optional string jwt = 13;</code>
     * @return The jwt.
     */
    @java.lang.Override
    public java.lang.String getJwt() {
      java.lang.Object ref = jwt_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          jwt_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string jwt = 13;</code>
     * @return The bytes for jwt.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getJwtBytes() {
      java.lang.Object ref = jwt_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        jwt_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_EXTERNAL_CLIENT_REQUEST_FIELD_NUMBER = 14;
    private boolean isExternalClientRequest_;
    /**
     * <code>optional bool is_external_client_request = 14 [default = false];</code>
     * @return Whether the isExternalClientRequest field is set.
     */
    @java.lang.Override
    public boolean hasIsExternalClientRequest() {
      return ((bitField0_ & 0x00002000) != 0);
    }
    /**
     * <code>optional bool is_external_client_request = 14 [default = false];</code>
     * @return The isExternalClientRequest.
     */
    @java.lang.Override
    public boolean getIsExternalClientRequest() {
      return isExternalClientRequest_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getWorkSpec());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBytes(2, workSpecSignature_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeInt32(3, fragmentNumber_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        output.writeInt32(4, attemptNumber_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, containerIdString_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, amHost_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        output.writeInt32(7, amPort_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        output.writeBytes(8, credentialsBinary_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        output.writeMessage(9, getFragmentRuntimeInfo());
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        output.writeBytes(10, initialEventBytes_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        output.writeBytes(11, initialEventSignature_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        output.writeBool(12, isGuaranteed_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 13, jwt_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        output.writeBool(14, isExternalClientRequest_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getWorkSpec());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, workSpecSignature_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, fragmentNumber_);
      }
      if (((bitField0_ & 0x00000008) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, attemptNumber_);
      }
      if (((bitField0_ & 0x00000010) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, containerIdString_);
      }
      if (((bitField0_ & 0x00000020) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, amHost_);
      }
      if (((bitField0_ & 0x00000040) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, amPort_);
      }
      if (((bitField0_ & 0x00000080) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(8, credentialsBinary_);
      }
      if (((bitField0_ & 0x00000100) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, getFragmentRuntimeInfo());
      }
      if (((bitField0_ & 0x00000200) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(10, initialEventBytes_);
      }
      if (((bitField0_ & 0x00000400) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(11, initialEventSignature_);
      }
      if (((bitField0_ & 0x00000800) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(12, isGuaranteed_);
      }
      if (((bitField0_ & 0x00001000) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(13, jwt_);
      }
      if (((bitField0_ & 0x00002000) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(14, isExternalClientRequest_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) obj;

      if (hasWorkSpec() != other.hasWorkSpec()) return false;
      if (hasWorkSpec()) {
        if (!getWorkSpec()
            .equals(other.getWorkSpec())) return false;
      }
      if (hasWorkSpecSignature() != other.hasWorkSpecSignature()) return false;
      if (hasWorkSpecSignature()) {
        if (!getWorkSpecSignature()
            .equals(other.getWorkSpecSignature())) return false;
      }
      if (hasFragmentNumber() != other.hasFragmentNumber()) return false;
      if (hasFragmentNumber()) {
        if (getFragmentNumber()
            != other.getFragmentNumber()) return false;
      }
      if (hasAttemptNumber() != other.hasAttemptNumber()) return false;
      if (hasAttemptNumber()) {
        if (getAttemptNumber()
            != other.getAttemptNumber()) return false;
      }
      if (hasContainerIdString() != other.hasContainerIdString()) return false;
      if (hasContainerIdString()) {
        if (!getContainerIdString()
            .equals(other.getContainerIdString())) return false;
      }
      if (hasAmHost() != other.hasAmHost()) return false;
      if (hasAmHost()) {
        if (!getAmHost()
            .equals(other.getAmHost())) return false;
      }
      if (hasAmPort() != other.hasAmPort()) return false;
      if (hasAmPort()) {
        if (getAmPort()
            != other.getAmPort()) return false;
      }
      if (hasCredentialsBinary() != other.hasCredentialsBinary()) return false;
      if (hasCredentialsBinary()) {
        if (!getCredentialsBinary()
            .equals(other.getCredentialsBinary())) return false;
      }
      if (hasFragmentRuntimeInfo() != other.hasFragmentRuntimeInfo()) return false;
      if (hasFragmentRuntimeInfo()) {
        if (!getFragmentRuntimeInfo()
            .equals(other.getFragmentRuntimeInfo())) return false;
      }
      if (hasInitialEventBytes() != other.hasInitialEventBytes()) return false;
      if (hasInitialEventBytes()) {
        if (!getInitialEventBytes()
            .equals(other.getInitialEventBytes())) return false;
      }
      if (hasInitialEventSignature() != other.hasInitialEventSignature()) return false;
      if (hasInitialEventSignature()) {
        if (!getInitialEventSignature()
            .equals(other.getInitialEventSignature())) return false;
      }
      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
      if (hasIsGuaranteed()) {
        if (getIsGuaranteed()
            != other.getIsGuaranteed()) return false;
      }
      if (hasJwt() != other.hasJwt()) return false;
      if (hasJwt()) {
        if (!getJwt()
            .equals(other.getJwt())) return false;
      }
      if (hasIsExternalClientRequest() != other.hasIsExternalClientRequest()) return false;
      if (hasIsExternalClientRequest()) {
        if (getIsExternalClientRequest()
            != other.getIsExternalClientRequest()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasWorkSpec()) {
        hash = (37 * hash) + WORK_SPEC_FIELD_NUMBER;
        hash = (53 * hash) + getWorkSpec().hashCode();
      }
      if (hasWorkSpecSignature()) {
        hash = (37 * hash) + WORK_SPEC_SIGNATURE_FIELD_NUMBER;
        hash = (53 * hash) + getWorkSpecSignature().hashCode();
      }
      if (hasFragmentNumber()) {
        hash = (37 * hash) + FRAGMENT_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + getFragmentNumber();
      }
      if (hasAttemptNumber()) {
        hash = (37 * hash) + ATTEMPT_NUMBER_FIELD_NUMBER;
        hash = (53 * hash) + getAttemptNumber();
      }
      if (hasContainerIdString()) {
        hash = (37 * hash) + CONTAINER_ID_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getContainerIdString().hashCode();
      }
      if (hasAmHost()) {
        hash = (37 * hash) + AM_HOST_FIELD_NUMBER;
        hash = (53 * hash) + getAmHost().hashCode();
      }
      if (hasAmPort()) {
        hash = (37 * hash) + AM_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getAmPort();
      }
      if (hasCredentialsBinary()) {
        hash = (37 * hash) + CREDENTIALS_BINARY_FIELD_NUMBER;
        hash = (53 * hash) + getCredentialsBinary().hashCode();
      }
      if (hasFragmentRuntimeInfo()) {
        hash = (37 * hash) + FRAGMENT_RUNTIME_INFO_FIELD_NUMBER;
        hash = (53 * hash) + getFragmentRuntimeInfo().hashCode();
      }
      if (hasInitialEventBytes()) {
        hash = (37 * hash) + INITIAL_EVENT_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + getInitialEventBytes().hashCode();
      }
      if (hasInitialEventSignature()) {
        hash = (37 * hash) + INITIAL_EVENT_SIGNATURE_FIELD_NUMBER;
        hash = (53 * hash) + getInitialEventSignature().hashCode();
      }
      if (hasIsGuaranteed()) {
        hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getIsGuaranteed());
      }
      if (hasJwt()) {
        hash = (37 * hash) + JWT_FIELD_NUMBER;
        hash = (53 * hash) + getJwt().hashCode();
      }
      if (hasIsExternalClientRequest()) {
        hash = (37 * hash) + IS_EXTERNAL_CLIENT_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getIsExternalClientRequest());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SubmitWorkRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SubmitWorkRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getWorkSpecFieldBuilder();
          getFragmentRuntimeInfoFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (workSpecBuilder_ == null) {
          workSpec_ = null;
        } else {
          workSpecBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        fragmentNumber_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        attemptNumber_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        containerIdString_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        amHost_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        amPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000080);
        if (fragmentRuntimeInfoBuilder_ == null) {
          fragmentRuntimeInfo_ = null;
        } else {
          fragmentRuntimeInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000200);
        initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000400);
        isGuaranteed_ = false;
        bitField0_ = (bitField0_ & ~0x00000800);
        jwt_ = "";
        bitField0_ = (bitField0_ & ~0x00001000);
        isExternalClientRequest_ = false;
        bitField0_ = (bitField0_ & ~0x00002000);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (workSpecBuilder_ == null) {
            result.workSpec_ = workSpec_;
          } else {
            result.workSpec_ = workSpecBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.workSpecSignature_ = workSpecSignature_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.fragmentNumber_ = fragmentNumber_;
          to_bitField0_ |= 0x00000004;
        }
        if (((from_bitField0_ & 0x00000008) != 0)) {
          result.attemptNumber_ = attemptNumber_;
          to_bitField0_ |= 0x00000008;
        }
        if (((from_bitField0_ & 0x00000010) != 0)) {
          to_bitField0_ |= 0x00000010;
        }
        result.containerIdString_ = containerIdString_;
        if (((from_bitField0_ & 0x00000020) != 0)) {
          to_bitField0_ |= 0x00000020;
        }
        result.amHost_ = amHost_;
        if (((from_bitField0_ & 0x00000040) != 0)) {
          result.amPort_ = amPort_;
          to_bitField0_ |= 0x00000040;
        }
        if (((from_bitField0_ & 0x00000080) != 0)) {
          to_bitField0_ |= 0x00000080;
        }
        result.credentialsBinary_ = credentialsBinary_;
        if (((from_bitField0_ & 0x00000100) != 0)) {
          if (fragmentRuntimeInfoBuilder_ == null) {
            result.fragmentRuntimeInfo_ = fragmentRuntimeInfo_;
          } else {
            result.fragmentRuntimeInfo_ = fragmentRuntimeInfoBuilder_.build();
          }
          to_bitField0_ |= 0x00000100;
        }
        if (((from_bitField0_ & 0x00000200) != 0)) {
          to_bitField0_ |= 0x00000200;
        }
        result.initialEventBytes_ = initialEventBytes_;
        if (((from_bitField0_ & 0x00000400) != 0)) {
          to_bitField0_ |= 0x00000400;
        }
        result.initialEventSignature_ = initialEventSignature_;
        if (((from_bitField0_ & 0x00000800) != 0)) {
          result.isGuaranteed_ = isGuaranteed_;
          to_bitField0_ |= 0x00000800;
        }
        if (((from_bitField0_ & 0x00001000) != 0)) {
          to_bitField0_ |= 0x00001000;
        }
        result.jwt_ = jwt_;
        if (((from_bitField0_ & 0x00002000) != 0)) {
          result.isExternalClientRequest_ = isExternalClientRequest_;
          to_bitField0_ |= 0x00002000;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.getDefaultInstance()) return this;
        if (other.hasWorkSpec()) {
          mergeWorkSpec(other.getWorkSpec());
        }
        if (other.hasWorkSpecSignature()) {
          setWorkSpecSignature(other.getWorkSpecSignature());
        }
        if (other.hasFragmentNumber()) {
          setFragmentNumber(other.getFragmentNumber());
        }
        if (other.hasAttemptNumber()) {
          setAttemptNumber(other.getAttemptNumber());
        }
        if (other.hasContainerIdString()) {
          bitField0_ |= 0x00000010;
          containerIdString_ = other.containerIdString_;
          onChanged();
        }
        if (other.hasAmHost()) {
          bitField0_ |= 0x00000020;
          amHost_ = other.amHost_;
          onChanged();
        }
        if (other.hasAmPort()) {
          setAmPort(other.getAmPort());
        }
        if (other.hasCredentialsBinary()) {
          setCredentialsBinary(other.getCredentialsBinary());
        }
        if (other.hasFragmentRuntimeInfo()) {
          mergeFragmentRuntimeInfo(other.getFragmentRuntimeInfo());
        }
        if (other.hasInitialEventBytes()) {
          setInitialEventBytes(other.getInitialEventBytes());
        }
        if (other.hasInitialEventSignature()) {
          setInitialEventSignature(other.getInitialEventSignature());
        }
        if (other.hasIsGuaranteed()) {
          setIsGuaranteed(other.getIsGuaranteed());
        }
        if (other.hasJwt()) {
          bitField0_ |= 0x00001000;
          jwt_ = other.jwt_;
          onChanged();
        }
        if (other.hasIsExternalClientRequest()) {
          setIsExternalClientRequest(other.getIsExternalClientRequest());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary workSpec_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder> workSpecBuilder_;
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       * @return Whether the workSpec field is set.
       */
      public boolean hasWorkSpec() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       * @return The workSpec.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary getWorkSpec() {
        if (workSpecBuilder_ == null) {
          return workSpec_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
        } else {
          return workSpecBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public Builder setWorkSpec(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary value) {
        if (workSpecBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          workSpec_ = value;
          onChanged();
        } else {
          workSpecBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public Builder setWorkSpec(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder builderForValue) {
        if (workSpecBuilder_ == null) {
          workSpec_ = builderForValue.build();
          onChanged();
        } else {
          workSpecBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public Builder mergeWorkSpec(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary value) {
        if (workSpecBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              workSpec_ != null &&
              workSpec_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance()) {
            workSpec_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.newBuilder(workSpec_).mergeFrom(value).buildPartial();
          } else {
            workSpec_ = value;
          }
          onChanged();
        } else {
          workSpecBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public Builder clearWorkSpec() {
        if (workSpecBuilder_ == null) {
          workSpec_ = null;
          onChanged();
        } else {
          workSpecBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder getWorkSpecBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getWorkSpecFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder getWorkSpecOrBuilder() {
        if (workSpecBuilder_ != null) {
          return workSpecBuilder_.getMessageOrBuilder();
        } else {
          return workSpec_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.getDefaultInstance() : workSpec_;
        }
      }
      /**
       * <code>optional .VertexOrBinary work_spec = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder> 
          getWorkSpecFieldBuilder() {
        if (workSpecBuilder_ == null) {
          workSpecBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinary.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.VertexOrBinaryOrBuilder>(
                  getWorkSpec(),
                  getParentForChildren(),
                  isClean());
          workSpec_ = null;
        }
        return workSpecBuilder_;
      }

      private com.google.protobuf.ByteString workSpecSignature_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes work_spec_signature = 2;</code>
       * @return Whether the workSpecSignature field is set.
       */
      @java.lang.Override
      public boolean hasWorkSpecSignature() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional bytes work_spec_signature = 2;</code>
       * @return The workSpecSignature.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getWorkSpecSignature() {
        return workSpecSignature_;
      }
      /**
       * <code>optional bytes work_spec_signature = 2;</code>
       * @param value The workSpecSignature to set.
       * @return This builder for chaining.
       */
      public Builder setWorkSpecSignature(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        workSpecSignature_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes work_spec_signature = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearWorkSpecSignature() {
        bitField0_ = (bitField0_ & ~0x00000002);
        workSpecSignature_ = getDefaultInstance().getWorkSpecSignature();
        onChanged();
        return this;
      }

      private int fragmentNumber_ ;
      /**
       * <code>optional int32 fragment_number = 3;</code>
       * @return Whether the fragmentNumber field is set.
       */
      @java.lang.Override
      public boolean hasFragmentNumber() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional int32 fragment_number = 3;</code>
       * @return The fragmentNumber.
       */
      @java.lang.Override
      public int getFragmentNumber() {
        return fragmentNumber_;
      }
      /**
       * <code>optional int32 fragment_number = 3;</code>
       * @param value The fragmentNumber to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentNumber(int value) {
        bitField0_ |= 0x00000004;
        fragmentNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 fragment_number = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearFragmentNumber() {
        bitField0_ = (bitField0_ & ~0x00000004);
        fragmentNumber_ = 0;
        onChanged();
        return this;
      }

      private int attemptNumber_ ;
      /**
       * <code>optional int32 attempt_number = 4;</code>
       * @return Whether the attemptNumber field is set.
       */
      @java.lang.Override
      public boolean hasAttemptNumber() {
        return ((bitField0_ & 0x00000008) != 0);
      }
      /**
       * <code>optional int32 attempt_number = 4;</code>
       * @return The attemptNumber.
       */
      @java.lang.Override
      public int getAttemptNumber() {
        return attemptNumber_;
      }
      /**
       * <code>optional int32 attempt_number = 4;</code>
       * @param value The attemptNumber to set.
       * @return This builder for chaining.
       */
      public Builder setAttemptNumber(int value) {
        bitField0_ |= 0x00000008;
        attemptNumber_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 attempt_number = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearAttemptNumber() {
        bitField0_ = (bitField0_ & ~0x00000008);
        attemptNumber_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object containerIdString_ = "";
      /**
       * <code>optional string container_id_string = 5;</code>
       * @return Whether the containerIdString field is set.
       */
      public boolean hasContainerIdString() {
        return ((bitField0_ & 0x00000010) != 0);
      }
      /**
       * <code>optional string container_id_string = 5;</code>
       * @return The containerIdString.
       */
      public java.lang.String getContainerIdString() {
        java.lang.Object ref = containerIdString_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            containerIdString_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string container_id_string = 5;</code>
       * @return The bytes for containerIdString.
       */
      public com.google.protobuf.ByteString
          getContainerIdStringBytes() {
        java.lang.Object ref = containerIdString_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          containerIdString_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string container_id_string = 5;</code>
       * @param value The containerIdString to set.
       * @return This builder for chaining.
       */
      public Builder setContainerIdString(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        containerIdString_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string container_id_string = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearContainerIdString() {
        bitField0_ = (bitField0_ & ~0x00000010);
        containerIdString_ = getDefaultInstance().getContainerIdString();
        onChanged();
        return this;
      }
      /**
       * <code>optional string container_id_string = 5;</code>
       * @param value The bytes for containerIdString to set.
       * @return This builder for chaining.
       */
      public Builder setContainerIdStringBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        containerIdString_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object amHost_ = "";
      /**
       * <code>optional string am_host = 6;</code>
       * @return Whether the amHost field is set.
       */
      public boolean hasAmHost() {
        return ((bitField0_ & 0x00000020) != 0);
      }
      /**
       * <code>optional string am_host = 6;</code>
       * @return The amHost.
       */
      public java.lang.String getAmHost() {
        java.lang.Object ref = amHost_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            amHost_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string am_host = 6;</code>
       * @return The bytes for amHost.
       */
      public com.google.protobuf.ByteString
          getAmHostBytes() {
        java.lang.Object ref = amHost_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          amHost_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string am_host = 6;</code>
       * @param value The amHost to set.
       * @return This builder for chaining.
       */
      public Builder setAmHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        amHost_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string am_host = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearAmHost() {
        bitField0_ = (bitField0_ & ~0x00000020);
        amHost_ = getDefaultInstance().getAmHost();
        onChanged();
        return this;
      }
      /**
       * <code>optional string am_host = 6;</code>
       * @param value The bytes for amHost to set.
       * @return This builder for chaining.
       */
      public Builder setAmHostBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        amHost_ = value;
        onChanged();
        return this;
      }

      private int amPort_ ;
      /**
       * <code>optional int32 am_port = 7;</code>
       * @return Whether the amPort field is set.
       */
      @java.lang.Override
      public boolean hasAmPort() {
        return ((bitField0_ & 0x00000040) != 0);
      }
      /**
       * <code>optional int32 am_port = 7;</code>
       * @return The amPort.
       */
      @java.lang.Override
      public int getAmPort() {
        return amPort_;
      }
      /**
       * <code>optional int32 am_port = 7;</code>
       * @param value The amPort to set.
       * @return This builder for chaining.
       */
      public Builder setAmPort(int value) {
        bitField0_ |= 0x00000040;
        amPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 am_port = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearAmPort() {
        bitField0_ = (bitField0_ & ~0x00000040);
        amPort_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * Credentials are not signed - the client can add e.g. his own HDFS tokens.
       * </pre>
       *
       * <code>optional bytes credentials_binary = 8;</code>
       * @return Whether the credentialsBinary field is set.
       */
      @java.lang.Override
      public boolean hasCredentialsBinary() {
        return ((bitField0_ & 0x00000080) != 0);
      }
      /**
       * <pre>
       * Credentials are not signed - the client can add e.g. his own HDFS tokens.
       * </pre>
       *
       * <code>optional bytes credentials_binary = 8;</code>
       * @return The credentialsBinary.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getCredentialsBinary() {
        return credentialsBinary_;
      }
      /**
       * <pre>
       * Credentials are not signed - the client can add e.g. his own HDFS tokens.
       * </pre>
       *
       * <code>optional bytes credentials_binary = 8;</code>
       * @param value The credentialsBinary to set.
       * @return This builder for chaining.
       */
      public Builder setCredentialsBinary(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        credentialsBinary_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Credentials are not signed - the client can add e.g. his own HDFS tokens.
       * </pre>
       *
       * <code>optional bytes credentials_binary = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearCredentialsBinary() {
        bitField0_ = (bitField0_ & ~0x00000080);
        credentialsBinary_ = getDefaultInstance().getCredentialsBinary();
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo fragmentRuntimeInfo_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder> fragmentRuntimeInfoBuilder_;
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       * @return Whether the fragmentRuntimeInfo field is set.
       */
      public boolean hasFragmentRuntimeInfo() {
        return ((bitField0_ & 0x00000100) != 0);
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       * @return The fragmentRuntimeInfo.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo getFragmentRuntimeInfo() {
        if (fragmentRuntimeInfoBuilder_ == null) {
          return fragmentRuntimeInfo_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
        } else {
          return fragmentRuntimeInfoBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public Builder setFragmentRuntimeInfo(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo value) {
        if (fragmentRuntimeInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          fragmentRuntimeInfo_ = value;
          onChanged();
        } else {
          fragmentRuntimeInfoBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public Builder setFragmentRuntimeInfo(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder builderForValue) {
        if (fragmentRuntimeInfoBuilder_ == null) {
          fragmentRuntimeInfo_ = builderForValue.build();
          onChanged();
        } else {
          fragmentRuntimeInfoBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public Builder mergeFragmentRuntimeInfo(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo value) {
        if (fragmentRuntimeInfoBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0) &&
              fragmentRuntimeInfo_ != null &&
              fragmentRuntimeInfo_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance()) {
            fragmentRuntimeInfo_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.newBuilder(fragmentRuntimeInfo_).mergeFrom(value).buildPartial();
          } else {
            fragmentRuntimeInfo_ = value;
          }
          onChanged();
        } else {
          fragmentRuntimeInfoBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public Builder clearFragmentRuntimeInfo() {
        if (fragmentRuntimeInfoBuilder_ == null) {
          fragmentRuntimeInfo_ = null;
          onChanged();
        } else {
          fragmentRuntimeInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder getFragmentRuntimeInfoBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getFragmentRuntimeInfoFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder getFragmentRuntimeInfoOrBuilder() {
        if (fragmentRuntimeInfoBuilder_ != null) {
          return fragmentRuntimeInfoBuilder_.getMessageOrBuilder();
        } else {
          return fragmentRuntimeInfo_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.getDefaultInstance() : fragmentRuntimeInfo_;
        }
      }
      /**
       * <pre>
       * Not supported/honored for external clients right now.
       * </pre>
       *
       * <code>optional .FragmentRuntimeInfo fragment_runtime_info = 9;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder> 
          getFragmentRuntimeInfoFieldBuilder() {
        if (fragmentRuntimeInfoBuilder_ == null) {
          fragmentRuntimeInfoBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfoOrBuilder>(
                  getFragmentRuntimeInfo(),
                  getParentForChildren(),
                  isClean());
          fragmentRuntimeInfo_ = null;
        }
        return fragmentRuntimeInfoBuilder_;
      }

      private com.google.protobuf.ByteString initialEventBytes_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * Serialized (and signed) NotTezEvent; used only for external clients for now.
       * </pre>
       *
       * <code>optional bytes initial_event_bytes = 10;</code>
       * @return Whether the initialEventBytes field is set.
       */
      @java.lang.Override
      public boolean hasInitialEventBytes() {
        return ((bitField0_ & 0x00000200) != 0);
      }
      /**
       * <pre>
       * Serialized (and signed) NotTezEvent; used only for external clients for now.
       * </pre>
       *
       * <code>optional bytes initial_event_bytes = 10;</code>
       * @return The initialEventBytes.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getInitialEventBytes() {
        return initialEventBytes_;
      }
      /**
       * <pre>
       * Serialized (and signed) NotTezEvent; used only for external clients for now.
       * </pre>
       *
       * <code>optional bytes initial_event_bytes = 10;</code>
       * @param value The initialEventBytes to set.
       * @return This builder for chaining.
       */
      public Builder setInitialEventBytes(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000200;
        initialEventBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Serialized (and signed) NotTezEvent; used only for external clients for now.
       * </pre>
       *
       * <code>optional bytes initial_event_bytes = 10;</code>
       * @return This builder for chaining.
       */
      public Builder clearInitialEventBytes() {
        bitField0_ = (bitField0_ & ~0x00000200);
        initialEventBytes_ = getDefaultInstance().getInitialEventBytes();
        onChanged();
        return this;
      }

      private com.google.protobuf.ByteString initialEventSignature_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes initial_event_signature = 11;</code>
       * @return Whether the initialEventSignature field is set.
       */
      @java.lang.Override
      public boolean hasInitialEventSignature() {
        return ((bitField0_ & 0x00000400) != 0);
      }
      /**
       * <code>optional bytes initial_event_signature = 11;</code>
       * @return The initialEventSignature.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getInitialEventSignature() {
        return initialEventSignature_;
      }
      /**
       * <code>optional bytes initial_event_signature = 11;</code>
       * @param value The initialEventSignature to set.
       * @return This builder for chaining.
       */
      public Builder setInitialEventSignature(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000400;
        initialEventSignature_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes initial_event_signature = 11;</code>
       * @return This builder for chaining.
       */
      public Builder clearInitialEventSignature() {
        bitField0_ = (bitField0_ & ~0x00000400);
        initialEventSignature_ = getDefaultInstance().getInitialEventSignature();
        onChanged();
        return this;
      }

      private boolean isGuaranteed_ ;
      /**
       * <code>optional bool is_guaranteed = 12 [default = false];</code>
       * @return Whether the isGuaranteed field is set.
       */
      @java.lang.Override
      public boolean hasIsGuaranteed() {
        return ((bitField0_ & 0x00000800) != 0);
      }
      /**
       * <code>optional bool is_guaranteed = 12 [default = false];</code>
       * @return The isGuaranteed.
       */
      @java.lang.Override
      public boolean getIsGuaranteed() {
        return isGuaranteed_;
      }
      /**
       * <code>optional bool is_guaranteed = 12 [default = false];</code>
       * @param value The isGuaranteed to set.
       * @return This builder for chaining.
       */
      public Builder setIsGuaranteed(boolean value) {
        bitField0_ |= 0x00000800;
        isGuaranteed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_guaranteed = 12 [default = false];</code>
       * @return This builder for chaining.
       */
      public Builder clearIsGuaranteed() {
        bitField0_ = (bitField0_ & ~0x00000800);
        isGuaranteed_ = false;
        onChanged();
        return this;
      }

      private java.lang.Object jwt_ = "";
      /**
       * <code>optional string jwt = 13;</code>
       * @return Whether the jwt field is set.
       */
      public boolean hasJwt() {
        return ((bitField0_ & 0x00001000) != 0);
      }
      /**
       * <code>optional string jwt = 13;</code>
       * @return The jwt.
       */
      public java.lang.String getJwt() {
        java.lang.Object ref = jwt_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            jwt_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string jwt = 13;</code>
       * @return The bytes for jwt.
       */
      public com.google.protobuf.ByteString
          getJwtBytes() {
        java.lang.Object ref = jwt_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          jwt_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string jwt = 13;</code>
       * @param value The jwt to set.
       * @return This builder for chaining.
       */
      public Builder setJwt(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        jwt_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string jwt = 13;</code>
       * @return This builder for chaining.
       */
      public Builder clearJwt() {
        bitField0_ = (bitField0_ & ~0x00001000);
        jwt_ = getDefaultInstance().getJwt();
        onChanged();
        return this;
      }
      /**
       * <code>optional string jwt = 13;</code>
       * @param value The bytes for jwt to set.
       * @return This builder for chaining.
       */
      public Builder setJwtBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00001000;
        jwt_ = value;
        onChanged();
        return this;
      }

      private boolean isExternalClientRequest_ ;
      /**
       * <code>optional bool is_external_client_request = 14 [default = false];</code>
       * @return Whether the isExternalClientRequest field is set.
       */
      @java.lang.Override
      public boolean hasIsExternalClientRequest() {
        return ((bitField0_ & 0x00002000) != 0);
      }
      /**
       * <code>optional bool is_external_client_request = 14 [default = false];</code>
       * @return The isExternalClientRequest.
       */
      @java.lang.Override
      public boolean getIsExternalClientRequest() {
        return isExternalClientRequest_;
      }
      /**
       * <code>optional bool is_external_client_request = 14 [default = false];</code>
       * @param value The isExternalClientRequest to set.
       * @return This builder for chaining.
       */
      public Builder setIsExternalClientRequest(boolean value) {
        bitField0_ |= 0x00002000;
        isExternalClientRequest_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_external_client_request = 14 [default = false];</code>
       * @return This builder for chaining.
       */
      public Builder clearIsExternalClientRequest() {
        bitField0_ = (bitField0_ & ~0x00002000);
        isExternalClientRequest_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SubmitWorkRequestProto)
    }

    // @@protoc_insertion_point(class_scope:SubmitWorkRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SubmitWorkRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<SubmitWorkRequestProto>() {
      @java.lang.Override
      public SubmitWorkRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitWorkRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SubmitWorkRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitWorkRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterDagRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:RegisterDagRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string user = 1;</code>
     * @return Whether the user field is set.
     */
    boolean hasUser();
    /**
     * <code>optional string user = 1;</code>
     * @return The user.
     */
    java.lang.String getUser();
    /**
     * <code>optional string user = 1;</code>
     * @return The bytes for user.
     */
    com.google.protobuf.ByteString
        getUserBytes();

    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional bytes credentials_binary = 3;</code>
     * @return Whether the credentialsBinary field is set.
     */
    boolean hasCredentialsBinary();
    /**
     * <code>optional bytes credentials_binary = 3;</code>
     * @return The credentialsBinary.
     */
    com.google.protobuf.ByteString getCredentialsBinary();
  }
  /**
   * Protobuf type {@code RegisterDagRequestProto}
   */
  public static final class RegisterDagRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:RegisterDagRequestProto)
      RegisterDagRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterDagRequestProto.newBuilder() to construct.
    private RegisterDagRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterDagRequestProto() {
      user_ = "";
      credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RegisterDagRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterDagRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              user_ = bs;
              break;
            }
            case 18: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              credentialsBinary_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int USER_FIELD_NUMBER = 1;
    private volatile java.lang.Object user_;
    /**
     * <code>optional string user = 1;</code>
     * @return Whether the user field is set.
     */
    @java.lang.Override
    public boolean hasUser() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string user = 1;</code>
     * @return The user.
     */
    @java.lang.Override
    public java.lang.String getUser() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          user_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string user = 1;</code>
     * @return The bytes for user.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getUserBytes() {
      java.lang.Object ref = user_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        user_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 2;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>required .QueryIdentifierProto query_identifier = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int CREDENTIALS_BINARY_FIELD_NUMBER = 3;
    private com.google.protobuf.ByteString credentialsBinary_;
    /**
     * <code>optional bytes credentials_binary = 3;</code>
     * @return Whether the credentialsBinary field is set.
     */
    @java.lang.Override
    public boolean hasCredentialsBinary() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional bytes credentials_binary = 3;</code>
     * @return The credentialsBinary.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getCredentialsBinary() {
      return credentialsBinary_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasQueryIdentifier()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, user_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeMessage(2, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeBytes(3, credentialsBinary_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, user_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, credentialsBinary_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) obj;

      if (hasUser() != other.hasUser()) return false;
      if (hasUser()) {
        if (!getUser()
            .equals(other.getUser())) return false;
      }
      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasCredentialsBinary() != other.hasCredentialsBinary()) return false;
      if (hasCredentialsBinary()) {
        if (!getCredentialsBinary()
            .equals(other.getCredentialsBinary())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasUser()) {
        hash = (37 * hash) + USER_FIELD_NUMBER;
        hash = (53 * hash) + getUser().hashCode();
      }
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasCredentialsBinary()) {
        hash = (37 * hash) + CREDENTIALS_BINARY_FIELD_NUMBER;
        hash = (53 * hash) + getCredentialsBinary().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegisterDagRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:RegisterDagRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        user_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.user_ = user_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000002;
        }
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.credentialsBinary_ = credentialsBinary_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance()) return this;
        if (other.hasUser()) {
          bitField0_ |= 0x00000001;
          user_ = other.user_;
          onChanged();
        }
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasCredentialsBinary()) {
          setCredentialsBinary(other.getCredentialsBinary());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasQueryIdentifier()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object user_ = "";
      /**
       * <code>optional string user = 1;</code>
       * @return Whether the user field is set.
       */
      public boolean hasUser() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string user = 1;</code>
       * @return The user.
       */
      public java.lang.String getUser() {
        java.lang.Object ref = user_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            user_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string user = 1;</code>
       * @return The bytes for user.
       */
      public com.google.protobuf.ByteString
          getUserBytes() {
        java.lang.Object ref = user_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          user_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string user = 1;</code>
       * @param value The user to set.
       * @return This builder for chaining.
       */
      public Builder setUser(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearUser() {
        bitField0_ = (bitField0_ & ~0x00000001);
        user_ = getDefaultInstance().getUser();
        onChanged();
        return this;
      }
      /**
       * <code>optional string user = 1;</code>
       * @param value The bytes for user to set.
       * @return This builder for chaining.
       */
      public Builder setUserBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        user_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>required .QueryIdentifierProto query_identifier = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private com.google.protobuf.ByteString credentialsBinary_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes credentials_binary = 3;</code>
       * @return Whether the credentialsBinary field is set.
       */
      @java.lang.Override
      public boolean hasCredentialsBinary() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional bytes credentials_binary = 3;</code>
       * @return The credentialsBinary.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getCredentialsBinary() {
        return credentialsBinary_;
      }
      /**
       * <code>optional bytes credentials_binary = 3;</code>
       * @param value The credentialsBinary to set.
       * @return This builder for chaining.
       */
      public Builder setCredentialsBinary(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        credentialsBinary_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes credentials_binary = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearCredentialsBinary() {
        bitField0_ = (bitField0_ & ~0x00000004);
        credentialsBinary_ = getDefaultInstance().getCredentialsBinary();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:RegisterDagRequestProto)
    }

    // @@protoc_insertion_point(class_scope:RegisterDagRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDagRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<RegisterDagRequestProto>() {
      @java.lang.Override
      public RegisterDagRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterDagRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RegisterDagRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterDagRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface RegisterDagResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:RegisterDagResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code RegisterDagResponseProto}
   */
  public static final class RegisterDagResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:RegisterDagResponseProto)
      RegisterDagResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use RegisterDagResponseProto.newBuilder() to construct.
    private RegisterDagResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private RegisterDagResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new RegisterDagResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterDagResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code RegisterDagResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:RegisterDagResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_RegisterDagResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:RegisterDagResponseProto)
    }

    // @@protoc_insertion_point(class_scope:RegisterDagResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<RegisterDagResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<RegisterDagResponseProto>() {
      @java.lang.Override
      public RegisterDagResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterDagResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<RegisterDagResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterDagResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SubmitWorkResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SubmitWorkResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .SubmissionStateProto submission_state = 1;</code>
     * @return Whether the submissionState field is set.
     */
    boolean hasSubmissionState();
    /**
     * <code>optional .SubmissionStateProto submission_state = 1;</code>
     * @return The submissionState.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState();

    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return Whether the uniqueNodeId field is set.
     */
    boolean hasUniqueNodeId();
    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return The uniqueNodeId.
     */
    java.lang.String getUniqueNodeId();
    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return The bytes for uniqueNodeId.
     */
    com.google.protobuf.ByteString
        getUniqueNodeIdBytes();
  }
  /**
   * Protobuf type {@code SubmitWorkResponseProto}
   */
  public static final class SubmitWorkResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SubmitWorkResponseProto)
      SubmitWorkResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SubmitWorkResponseProto.newBuilder() to construct.
    private SubmitWorkResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SubmitWorkResponseProto() {
      submissionState_ = 1;
      uniqueNodeId_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SubmitWorkResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitWorkResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto value = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                submissionState_ = rawValue;
              }
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              uniqueNodeId_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int SUBMISSION_STATE_FIELD_NUMBER = 1;
    private int submissionState_;
    /**
     * <code>optional .SubmissionStateProto submission_state = 1;</code>
     * @return Whether the submissionState field is set.
     */
    @java.lang.Override public boolean hasSubmissionState() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .SubmissionStateProto submission_state = 1;</code>
     * @return The submissionState.
     */
    @java.lang.Override public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.valueOf(submissionState_);
      return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED : result;
    }

    public static final int UNIQUE_NODE_ID_FIELD_NUMBER = 2;
    private volatile java.lang.Object uniqueNodeId_;
    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return Whether the uniqueNodeId field is set.
     */
    @java.lang.Override
    public boolean hasUniqueNodeId() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return The uniqueNodeId.
     */
    @java.lang.Override
    public java.lang.String getUniqueNodeId() {
      java.lang.Object ref = uniqueNodeId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          uniqueNodeId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string unique_node_id = 2;</code>
     * @return The bytes for uniqueNodeId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getUniqueNodeIdBytes() {
      java.lang.Object ref = uniqueNodeId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        uniqueNodeId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeEnum(1, submissionState_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, uniqueNodeId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, submissionState_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, uniqueNodeId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) obj;

      if (hasSubmissionState() != other.hasSubmissionState()) return false;
      if (hasSubmissionState()) {
        if (submissionState_ != other.submissionState_) return false;
      }
      if (hasUniqueNodeId() != other.hasUniqueNodeId()) return false;
      if (hasUniqueNodeId()) {
        if (!getUniqueNodeId()
            .equals(other.getUniqueNodeId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasSubmissionState()) {
        hash = (37 * hash) + SUBMISSION_STATE_FIELD_NUMBER;
        hash = (53 * hash) + submissionState_;
      }
      if (hasUniqueNodeId()) {
        hash = (37 * hash) + UNIQUE_NODE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getUniqueNodeId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SubmitWorkResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SubmitWorkResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        submissionState_ = 1;
        bitField0_ = (bitField0_ & ~0x00000001);
        uniqueNodeId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SubmitWorkResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.submissionState_ = submissionState_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.uniqueNodeId_ = uniqueNodeId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance()) return this;
        if (other.hasSubmissionState()) {
          setSubmissionState(other.getSubmissionState());
        }
        if (other.hasUniqueNodeId()) {
          bitField0_ |= 0x00000002;
          uniqueNodeId_ = other.uniqueNodeId_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int submissionState_ = 1;
      /**
       * <code>optional .SubmissionStateProto submission_state = 1;</code>
       * @return Whether the submissionState field is set.
       */
      @java.lang.Override public boolean hasSubmissionState() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .SubmissionStateProto submission_state = 1;</code>
       * @return The submissionState.
       */
      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto getSubmissionState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.valueOf(submissionState_);
        return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto.ACCEPTED : result;
      }
      /**
       * <code>optional .SubmissionStateProto submission_state = 1;</code>
       * @param value The submissionState to set.
       * @return This builder for chaining.
       */
      public Builder setSubmissionState(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmissionStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        submissionState_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .SubmissionStateProto submission_state = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearSubmissionState() {
        bitField0_ = (bitField0_ & ~0x00000001);
        submissionState_ = 1;
        onChanged();
        return this;
      }

      private java.lang.Object uniqueNodeId_ = "";
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @return Whether the uniqueNodeId field is set.
       */
      public boolean hasUniqueNodeId() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @return The uniqueNodeId.
       */
      public java.lang.String getUniqueNodeId() {
        java.lang.Object ref = uniqueNodeId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            uniqueNodeId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @return The bytes for uniqueNodeId.
       */
      public com.google.protobuf.ByteString
          getUniqueNodeIdBytes() {
        java.lang.Object ref = uniqueNodeId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          uniqueNodeId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @param value The uniqueNodeId to set.
       * @return This builder for chaining.
       */
      public Builder setUniqueNodeId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        uniqueNodeId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearUniqueNodeId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        uniqueNodeId_ = getDefaultInstance().getUniqueNodeId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string unique_node_id = 2;</code>
       * @param value The bytes for uniqueNodeId to set.
       * @return This builder for chaining.
       */
      public Builder setUniqueNodeIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        uniqueNodeId_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SubmitWorkResponseProto)
    }

    // @@protoc_insertion_point(class_scope:SubmitWorkResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SubmitWorkResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<SubmitWorkResponseProto>() {
      @java.lang.Override
      public SubmitWorkResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitWorkResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SubmitWorkResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitWorkResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SourceStateUpdatedRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SourceStateUpdatedRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional string src_name = 2;</code>
     * @return Whether the srcName field is set.
     */
    boolean hasSrcName();
    /**
     * <code>optional string src_name = 2;</code>
     * @return The srcName.
     */
    java.lang.String getSrcName();
    /**
     * <code>optional string src_name = 2;</code>
     * @return The bytes for srcName.
     */
    com.google.protobuf.ByteString
        getSrcNameBytes();

    /**
     * <code>optional .SourceStateProto state = 3;</code>
     * @return Whether the state field is set.
     */
    boolean hasState();
    /**
     * <code>optional .SourceStateProto state = 3;</code>
     * @return The state.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState();
  }
  /**
   * Protobuf type {@code SourceStateUpdatedRequestProto}
   */
  public static final class SourceStateUpdatedRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SourceStateUpdatedRequestProto)
      SourceStateUpdatedRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SourceStateUpdatedRequestProto.newBuilder() to construct.
    private SourceStateUpdatedRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SourceStateUpdatedRequestProto() {
      srcName_ = "";
      state_ = 1;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SourceStateUpdatedRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SourceStateUpdatedRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              srcName_ = bs;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
                @SuppressWarnings("deprecation")
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto value = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000004;
                state_ = rawValue;
              }
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int SRC_NAME_FIELD_NUMBER = 2;
    private volatile java.lang.Object srcName_;
    /**
     * <code>optional string src_name = 2;</code>
     * @return Whether the srcName field is set.
     */
    @java.lang.Override
    public boolean hasSrcName() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string src_name = 2;</code>
     * @return The srcName.
     */
    @java.lang.Override
    public java.lang.String getSrcName() {
      java.lang.Object ref = srcName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          srcName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string src_name = 2;</code>
     * @return The bytes for srcName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getSrcNameBytes() {
      java.lang.Object ref = srcName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        srcName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int STATE_FIELD_NUMBER = 3;
    private int state_;
    /**
     * <code>optional .SourceStateProto state = 3;</code>
     * @return Whether the state field is set.
     */
    @java.lang.Override public boolean hasState() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .SourceStateProto state = 3;</code>
     * @return The state.
     */
    @java.lang.Override public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState() {
      @SuppressWarnings("deprecation")
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.valueOf(state_);
      return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED : result;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, srcName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeEnum(3, state_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, srcName_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, state_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) obj;

      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasSrcName() != other.hasSrcName()) return false;
      if (hasSrcName()) {
        if (!getSrcName()
            .equals(other.getSrcName())) return false;
      }
      if (hasState() != other.hasState()) return false;
      if (hasState()) {
        if (state_ != other.state_) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasSrcName()) {
        hash = (37 * hash) + SRC_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getSrcName().hashCode();
      }
      if (hasState()) {
        hash = (37 * hash) + STATE_FIELD_NUMBER;
        hash = (53 * hash) + state_;
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SourceStateUpdatedRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SourceStateUpdatedRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        srcName_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        state_ = 1;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.srcName_ = srcName_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          to_bitField0_ |= 0x00000004;
        }
        result.state_ = state_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.getDefaultInstance()) return this;
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasSrcName()) {
          bitField0_ |= 0x00000002;
          srcName_ = other.srcName_;
          onChanged();
        }
        if (other.hasState()) {
          setState(other.getState());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private java.lang.Object srcName_ = "";
      /**
       * <code>optional string src_name = 2;</code>
       * @return Whether the srcName field is set.
       */
      public boolean hasSrcName() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string src_name = 2;</code>
       * @return The srcName.
       */
      public java.lang.String getSrcName() {
        java.lang.Object ref = srcName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            srcName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string src_name = 2;</code>
       * @return The bytes for srcName.
       */
      public com.google.protobuf.ByteString
          getSrcNameBytes() {
        java.lang.Object ref = srcName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          srcName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string src_name = 2;</code>
       * @param value The srcName to set.
       * @return This builder for chaining.
       */
      public Builder setSrcName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        srcName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string src_name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearSrcName() {
        bitField0_ = (bitField0_ & ~0x00000002);
        srcName_ = getDefaultInstance().getSrcName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string src_name = 2;</code>
       * @param value The bytes for srcName to set.
       * @return This builder for chaining.
       */
      public Builder setSrcNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        srcName_ = value;
        onChanged();
        return this;
      }

      private int state_ = 1;
      /**
       * <code>optional .SourceStateProto state = 3;</code>
       * @return Whether the state field is set.
       */
      @java.lang.Override public boolean hasState() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .SourceStateProto state = 3;</code>
       * @return The state.
       */
      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto getState() {
        @SuppressWarnings("deprecation")
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto result = org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.valueOf(state_);
        return result == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto.S_SUCCEEDED : result;
      }
      /**
       * <code>optional .SourceStateProto state = 3;</code>
       * @param value The state to set.
       * @return This builder for chaining.
       */
      public Builder setState(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        state_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>optional .SourceStateProto state = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000004);
        state_ = 1;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SourceStateUpdatedRequestProto)
    }

    // @@protoc_insertion_point(class_scope:SourceStateUpdatedRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SourceStateUpdatedRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<SourceStateUpdatedRequestProto>() {
      @java.lang.Override
      public SourceStateUpdatedRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SourceStateUpdatedRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SourceStateUpdatedRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SourceStateUpdatedRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SourceStateUpdatedResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SourceStateUpdatedResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code SourceStateUpdatedResponseProto}
   */
  public static final class SourceStateUpdatedResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SourceStateUpdatedResponseProto)
      SourceStateUpdatedResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SourceStateUpdatedResponseProto.newBuilder() to construct.
    private SourceStateUpdatedResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SourceStateUpdatedResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SourceStateUpdatedResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SourceStateUpdatedResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SourceStateUpdatedResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SourceStateUpdatedResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SourceStateUpdatedResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SourceStateUpdatedResponseProto)
    }

    // @@protoc_insertion_point(class_scope:SourceStateUpdatedResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SourceStateUpdatedResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<SourceStateUpdatedResponseProto>() {
      @java.lang.Override
      public SourceStateUpdatedResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SourceStateUpdatedResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SourceStateUpdatedResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SourceStateUpdatedResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface QueryCompleteRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:QueryCompleteRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional int64 delete_delay = 2 [default = 0];</code>
     * @return Whether the deleteDelay field is set.
     */
    boolean hasDeleteDelay();
    /**
     * <code>optional int64 delete_delay = 2 [default = 0];</code>
     * @return The deleteDelay.
     */
    long getDeleteDelay();
  }
  /**
   * Protobuf type {@code QueryCompleteRequestProto}
   */
  public static final class QueryCompleteRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:QueryCompleteRequestProto)
      QueryCompleteRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use QueryCompleteRequestProto.newBuilder() to construct.
    private QueryCompleteRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private QueryCompleteRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new QueryCompleteRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private QueryCompleteRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              deleteDelay_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int DELETE_DELAY_FIELD_NUMBER = 2;
    private long deleteDelay_;
    /**
     * <code>optional int64 delete_delay = 2 [default = 0];</code>
     * @return Whether the deleteDelay field is set.
     */
    @java.lang.Override
    public boolean hasDeleteDelay() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 delete_delay = 2 [default = 0];</code>
     * @return The deleteDelay.
     */
    @java.lang.Override
    public long getDeleteDelay() {
      return deleteDelay_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, deleteDelay_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, deleteDelay_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) obj;

      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasDeleteDelay() != other.hasDeleteDelay()) return false;
      if (hasDeleteDelay()) {
        if (getDeleteDelay()
            != other.getDeleteDelay()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasDeleteDelay()) {
        hash = (37 * hash) + DELETE_DELAY_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getDeleteDelay());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code QueryCompleteRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:QueryCompleteRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        deleteDelay_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.deleteDelay_ = deleteDelay_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.getDefaultInstance()) return this;
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasDeleteDelay()) {
          setDeleteDelay(other.getDeleteDelay());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private long deleteDelay_ ;
      /**
       * <code>optional int64 delete_delay = 2 [default = 0];</code>
       * @return Whether the deleteDelay field is set.
       */
      @java.lang.Override
      public boolean hasDeleteDelay() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 delete_delay = 2 [default = 0];</code>
       * @return The deleteDelay.
       */
      @java.lang.Override
      public long getDeleteDelay() {
        return deleteDelay_;
      }
      /**
       * <code>optional int64 delete_delay = 2 [default = 0];</code>
       * @param value The deleteDelay to set.
       * @return This builder for chaining.
       */
      public Builder setDeleteDelay(long value) {
        bitField0_ |= 0x00000002;
        deleteDelay_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 delete_delay = 2 [default = 0];</code>
       * @return This builder for chaining.
       */
      public Builder clearDeleteDelay() {
        bitField0_ = (bitField0_ & ~0x00000002);
        deleteDelay_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:QueryCompleteRequestProto)
    }

    // @@protoc_insertion_point(class_scope:QueryCompleteRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryCompleteRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<QueryCompleteRequestProto>() {
      @java.lang.Override
      public QueryCompleteRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new QueryCompleteRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<QueryCompleteRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<QueryCompleteRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface QueryCompleteResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:QueryCompleteResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code QueryCompleteResponseProto}
   */
  public static final class QueryCompleteResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:QueryCompleteResponseProto)
      QueryCompleteResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use QueryCompleteResponseProto.newBuilder() to construct.
    private QueryCompleteResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private QueryCompleteResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new QueryCompleteResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private QueryCompleteResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code QueryCompleteResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:QueryCompleteResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_QueryCompleteResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:QueryCompleteResponseProto)
    }

    // @@protoc_insertion_point(class_scope:QueryCompleteResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<QueryCompleteResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<QueryCompleteResponseProto>() {
      @java.lang.Override
      public QueryCompleteResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new QueryCompleteResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<QueryCompleteResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<QueryCompleteResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TerminateFragmentRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:TerminateFragmentRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return Whether the fragmentIdentifierString field is set.
     */
    boolean hasFragmentIdentifierString();
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The fragmentIdentifierString.
     */
    java.lang.String getFragmentIdentifierString();
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The bytes for fragmentIdentifierString.
     */
    com.google.protobuf.ByteString
        getFragmentIdentifierStringBytes();
  }
  /**
   * Protobuf type {@code TerminateFragmentRequestProto}
   */
  public static final class TerminateFragmentRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:TerminateFragmentRequestProto)
      TerminateFragmentRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TerminateFragmentRequestProto.newBuilder() to construct.
    private TerminateFragmentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TerminateFragmentRequestProto() {
      fragmentIdentifierString_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TerminateFragmentRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TerminateFragmentRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              fragmentIdentifierString_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER = 2;
    private volatile java.lang.Object fragmentIdentifierString_;
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return Whether the fragmentIdentifierString field is set.
     */
    @java.lang.Override
    public boolean hasFragmentIdentifierString() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The fragmentIdentifierString.
     */
    @java.lang.Override
    public java.lang.String getFragmentIdentifierString() {
      java.lang.Object ref = fragmentIdentifierString_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          fragmentIdentifierString_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The bytes for fragmentIdentifierString.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getFragmentIdentifierStringBytes() {
      java.lang.Object ref = fragmentIdentifierString_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        fragmentIdentifierString_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, fragmentIdentifierString_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, fragmentIdentifierString_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) obj;

      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasFragmentIdentifierString() != other.hasFragmentIdentifierString()) return false;
      if (hasFragmentIdentifierString()) {
        if (!getFragmentIdentifierString()
            .equals(other.getFragmentIdentifierString())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasFragmentIdentifierString()) {
        hash = (37 * hash) + FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getFragmentIdentifierString().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code TerminateFragmentRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:TerminateFragmentRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        fragmentIdentifierString_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.fragmentIdentifierString_ = fragmentIdentifierString_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.getDefaultInstance()) return this;
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasFragmentIdentifierString()) {
          bitField0_ |= 0x00000002;
          fragmentIdentifierString_ = other.fragmentIdentifierString_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private java.lang.Object fragmentIdentifierString_ = "";
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return Whether the fragmentIdentifierString field is set.
       */
      public boolean hasFragmentIdentifierString() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return The fragmentIdentifierString.
       */
      public java.lang.String getFragmentIdentifierString() {
        java.lang.Object ref = fragmentIdentifierString_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            fragmentIdentifierString_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return The bytes for fragmentIdentifierString.
       */
      public com.google.protobuf.ByteString
          getFragmentIdentifierStringBytes() {
        java.lang.Object ref = fragmentIdentifierString_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          fragmentIdentifierString_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @param value The fragmentIdentifierString to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentIdentifierString(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        fragmentIdentifierString_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearFragmentIdentifierString() {
        bitField0_ = (bitField0_ & ~0x00000002);
        fragmentIdentifierString_ = getDefaultInstance().getFragmentIdentifierString();
        onChanged();
        return this;
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @param value The bytes for fragmentIdentifierString to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentIdentifierStringBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        fragmentIdentifierString_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:TerminateFragmentRequestProto)
    }

    // @@protoc_insertion_point(class_scope:TerminateFragmentRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<TerminateFragmentRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<TerminateFragmentRequestProto>() {
      @java.lang.Override
      public TerminateFragmentRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TerminateFragmentRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TerminateFragmentRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TerminateFragmentRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TerminateFragmentResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:TerminateFragmentResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code TerminateFragmentResponseProto}
   */
  public static final class TerminateFragmentResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:TerminateFragmentResponseProto)
      TerminateFragmentResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TerminateFragmentResponseProto.newBuilder() to construct.
    private TerminateFragmentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TerminateFragmentResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TerminateFragmentResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TerminateFragmentResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code TerminateFragmentResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:TerminateFragmentResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TerminateFragmentResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:TerminateFragmentResponseProto)
    }

    // @@protoc_insertion_point(class_scope:TerminateFragmentResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<TerminateFragmentResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<TerminateFragmentResponseProto>() {
      @java.lang.Override
      public TerminateFragmentResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TerminateFragmentResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TerminateFragmentResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TerminateFragmentResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface UpdateFragmentRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:UpdateFragmentRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    boolean hasQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier();
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder();

    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return Whether the fragmentIdentifierString field is set.
     */
    boolean hasFragmentIdentifierString();
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The fragmentIdentifierString.
     */
    java.lang.String getFragmentIdentifierString();
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The bytes for fragmentIdentifierString.
     */
    com.google.protobuf.ByteString
        getFragmentIdentifierStringBytes();

    /**
     * <code>optional bool is_guaranteed = 3;</code>
     * @return Whether the isGuaranteed field is set.
     */
    boolean hasIsGuaranteed();
    /**
     * <code>optional bool is_guaranteed = 3;</code>
     * @return The isGuaranteed.
     */
    boolean getIsGuaranteed();
  }
  /**
   * Protobuf type {@code UpdateFragmentRequestProto}
   */
  public static final class UpdateFragmentRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:UpdateFragmentRequestProto)
      UpdateFragmentRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use UpdateFragmentRequestProto.newBuilder() to construct.
    private UpdateFragmentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private UpdateFragmentRequestProto() {
      fragmentIdentifierString_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new UpdateFragmentRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateFragmentRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = queryIdentifier_.toBuilder();
              }
              queryIdentifier_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queryIdentifier_);
                queryIdentifier_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              fragmentIdentifierString_ = bs;
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              isGuaranteed_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int QUERY_IDENTIFIER_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return Whether the queryIdentifier field is set.
     */
    @java.lang.Override
    public boolean hasQueryIdentifier() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     * @return The queryIdentifier.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }
    /**
     * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
      return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
    }

    public static final int FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER = 2;
    private volatile java.lang.Object fragmentIdentifierString_;
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return Whether the fragmentIdentifierString field is set.
     */
    @java.lang.Override
    public boolean hasFragmentIdentifierString() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The fragmentIdentifierString.
     */
    @java.lang.Override
    public java.lang.String getFragmentIdentifierString() {
      java.lang.Object ref = fragmentIdentifierString_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          fragmentIdentifierString_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string fragment_identifier_string = 2;</code>
     * @return The bytes for fragmentIdentifierString.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getFragmentIdentifierStringBytes() {
      java.lang.Object ref = fragmentIdentifierString_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        fragmentIdentifierString_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_GUARANTEED_FIELD_NUMBER = 3;
    private boolean isGuaranteed_;
    /**
     * <code>optional bool is_guaranteed = 3;</code>
     * @return Whether the isGuaranteed field is set.
     */
    @java.lang.Override
    public boolean hasIsGuaranteed() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional bool is_guaranteed = 3;</code>
     * @return The isGuaranteed.
     */
    @java.lang.Override
    public boolean getIsGuaranteed() {
      return isGuaranteed_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, fragmentIdentifierString_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeBool(3, isGuaranteed_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getQueryIdentifier());
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, fragmentIdentifierString_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, isGuaranteed_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) obj;

      if (hasQueryIdentifier() != other.hasQueryIdentifier()) return false;
      if (hasQueryIdentifier()) {
        if (!getQueryIdentifier()
            .equals(other.getQueryIdentifier())) return false;
      }
      if (hasFragmentIdentifierString() != other.hasFragmentIdentifierString()) return false;
      if (hasFragmentIdentifierString()) {
        if (!getFragmentIdentifierString()
            .equals(other.getFragmentIdentifierString())) return false;
      }
      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
      if (hasIsGuaranteed()) {
        if (getIsGuaranteed()
            != other.getIsGuaranteed()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasQueryIdentifier()) {
        hash = (37 * hash) + QUERY_IDENTIFIER_FIELD_NUMBER;
        hash = (53 * hash) + getQueryIdentifier().hashCode();
      }
      if (hasFragmentIdentifierString()) {
        hash = (37 * hash) + FRAGMENT_IDENTIFIER_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getFragmentIdentifierString().hashCode();
      }
      if (hasIsGuaranteed()) {
        hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getIsGuaranteed());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code UpdateFragmentRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:UpdateFragmentRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getQueryIdentifierFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        fragmentIdentifierString_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        isGuaranteed_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (queryIdentifierBuilder_ == null) {
            result.queryIdentifier_ = queryIdentifier_;
          } else {
            result.queryIdentifier_ = queryIdentifierBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.fragmentIdentifierString_ = fragmentIdentifierString_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.isGuaranteed_ = isGuaranteed_;
          to_bitField0_ |= 0x00000004;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.getDefaultInstance()) return this;
        if (other.hasQueryIdentifier()) {
          mergeQueryIdentifier(other.getQueryIdentifier());
        }
        if (other.hasFragmentIdentifierString()) {
          bitField0_ |= 0x00000002;
          fragmentIdentifierString_ = other.fragmentIdentifierString_;
          onChanged();
        }
        if (other.hasIsGuaranteed()) {
          setIsGuaranteed(other.getIsGuaranteed());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto queryIdentifier_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> queryIdentifierBuilder_;
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return Whether the queryIdentifier field is set.
       */
      public boolean hasQueryIdentifier() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       * @return The queryIdentifier.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto getQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          return queryIdentifier_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        } else {
          return queryIdentifierBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queryIdentifier_ = value;
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder setQueryIdentifier(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder builderForValue) {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = builderForValue.build();
          onChanged();
        } else {
          queryIdentifierBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder mergeQueryIdentifier(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto value) {
        if (queryIdentifierBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              queryIdentifier_ != null &&
              queryIdentifier_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance()) {
            queryIdentifier_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.newBuilder(queryIdentifier_).mergeFrom(value).buildPartial();
          } else {
            queryIdentifier_ = value;
          }
          onChanged();
        } else {
          queryIdentifierBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public Builder clearQueryIdentifier() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifier_ = null;
          onChanged();
        } else {
          queryIdentifierBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder getQueryIdentifierBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueryIdentifierFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder getQueryIdentifierOrBuilder() {
        if (queryIdentifierBuilder_ != null) {
          return queryIdentifierBuilder_.getMessageOrBuilder();
        } else {
          return queryIdentifier_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.getDefaultInstance() : queryIdentifier_;
        }
      }
      /**
       * <code>optional .QueryIdentifierProto query_identifier = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder> 
          getQueryIdentifierFieldBuilder() {
        if (queryIdentifierBuilder_ == null) {
          queryIdentifierBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProtoOrBuilder>(
                  getQueryIdentifier(),
                  getParentForChildren(),
                  isClean());
          queryIdentifier_ = null;
        }
        return queryIdentifierBuilder_;
      }

      private java.lang.Object fragmentIdentifierString_ = "";
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return Whether the fragmentIdentifierString field is set.
       */
      public boolean hasFragmentIdentifierString() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return The fragmentIdentifierString.
       */
      public java.lang.String getFragmentIdentifierString() {
        java.lang.Object ref = fragmentIdentifierString_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            fragmentIdentifierString_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return The bytes for fragmentIdentifierString.
       */
      public com.google.protobuf.ByteString
          getFragmentIdentifierStringBytes() {
        java.lang.Object ref = fragmentIdentifierString_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          fragmentIdentifierString_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @param value The fragmentIdentifierString to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentIdentifierString(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        fragmentIdentifierString_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearFragmentIdentifierString() {
        bitField0_ = (bitField0_ & ~0x00000002);
        fragmentIdentifierString_ = getDefaultInstance().getFragmentIdentifierString();
        onChanged();
        return this;
      }
      /**
       * <code>optional string fragment_identifier_string = 2;</code>
       * @param value The bytes for fragmentIdentifierString to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentIdentifierStringBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        fragmentIdentifierString_ = value;
        onChanged();
        return this;
      }

      private boolean isGuaranteed_ ;
      /**
       * <code>optional bool is_guaranteed = 3;</code>
       * @return Whether the isGuaranteed field is set.
       */
      @java.lang.Override
      public boolean hasIsGuaranteed() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional bool is_guaranteed = 3;</code>
       * @return The isGuaranteed.
       */
      @java.lang.Override
      public boolean getIsGuaranteed() {
        return isGuaranteed_;
      }
      /**
       * <code>optional bool is_guaranteed = 3;</code>
       * @param value The isGuaranteed to set.
       * @return This builder for chaining.
       */
      public Builder setIsGuaranteed(boolean value) {
        bitField0_ |= 0x00000004;
        isGuaranteed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_guaranteed = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsGuaranteed() {
        bitField0_ = (bitField0_ & ~0x00000004);
        isGuaranteed_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:UpdateFragmentRequestProto)
    }

    // @@protoc_insertion_point(class_scope:UpdateFragmentRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateFragmentRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<UpdateFragmentRequestProto>() {
      @java.lang.Override
      public UpdateFragmentRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateFragmentRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<UpdateFragmentRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateFragmentRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface UpdateFragmentResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:UpdateFragmentResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional bool result = 1;</code>
     * @return Whether the result field is set.
     */
    boolean hasResult();
    /**
     * <code>optional bool result = 1;</code>
     * @return The result.
     */
    boolean getResult();

    /**
     * <code>optional bool is_guaranteed = 2;</code>
     * @return Whether the isGuaranteed field is set.
     */
    boolean hasIsGuaranteed();
    /**
     * <code>optional bool is_guaranteed = 2;</code>
     * @return The isGuaranteed.
     */
    boolean getIsGuaranteed();
  }
  /**
   * Protobuf type {@code UpdateFragmentResponseProto}
   */
  public static final class UpdateFragmentResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:UpdateFragmentResponseProto)
      UpdateFragmentResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use UpdateFragmentResponseProto.newBuilder() to construct.
    private UpdateFragmentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private UpdateFragmentResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new UpdateFragmentResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateFragmentResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              result_ = input.readBool();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              isGuaranteed_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int RESULT_FIELD_NUMBER = 1;
    private boolean result_;
    /**
     * <code>optional bool result = 1;</code>
     * @return Whether the result field is set.
     */
    @java.lang.Override
    public boolean hasResult() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bool result = 1;</code>
     * @return The result.
     */
    @java.lang.Override
    public boolean getResult() {
      return result_;
    }

    public static final int IS_GUARANTEED_FIELD_NUMBER = 2;
    private boolean isGuaranteed_;
    /**
     * <code>optional bool is_guaranteed = 2;</code>
     * @return Whether the isGuaranteed field is set.
     */
    @java.lang.Override
    public boolean hasIsGuaranteed() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional bool is_guaranteed = 2;</code>
     * @return The isGuaranteed.
     */
    @java.lang.Override
    public boolean getIsGuaranteed() {
      return isGuaranteed_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBool(1, result_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBool(2, isGuaranteed_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, result_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, isGuaranteed_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) obj;

      if (hasResult() != other.hasResult()) return false;
      if (hasResult()) {
        if (getResult()
            != other.getResult()) return false;
      }
      if (hasIsGuaranteed() != other.hasIsGuaranteed()) return false;
      if (hasIsGuaranteed()) {
        if (getIsGuaranteed()
            != other.getIsGuaranteed()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResult()) {
        hash = (37 * hash) + RESULT_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getResult());
      }
      if (hasIsGuaranteed()) {
        hash = (37 * hash) + IS_GUARANTEED_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getIsGuaranteed());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code UpdateFragmentResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:UpdateFragmentResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        result_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        isGuaranteed_ = false;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_UpdateFragmentResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.result_ = result_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.isGuaranteed_ = isGuaranteed_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance()) return this;
        if (other.hasResult()) {
          setResult(other.getResult());
        }
        if (other.hasIsGuaranteed()) {
          setIsGuaranteed(other.getIsGuaranteed());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private boolean result_ ;
      /**
       * <code>optional bool result = 1;</code>
       * @return Whether the result field is set.
       */
      @java.lang.Override
      public boolean hasResult() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bool result = 1;</code>
       * @return The result.
       */
      @java.lang.Override
      public boolean getResult() {
        return result_;
      }
      /**
       * <code>optional bool result = 1;</code>
       * @param value The result to set.
       * @return This builder for chaining.
       */
      public Builder setResult(boolean value) {
        bitField0_ |= 0x00000001;
        result_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool result = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearResult() {
        bitField0_ = (bitField0_ & ~0x00000001);
        result_ = false;
        onChanged();
        return this;
      }

      private boolean isGuaranteed_ ;
      /**
       * <code>optional bool is_guaranteed = 2;</code>
       * @return Whether the isGuaranteed field is set.
       */
      @java.lang.Override
      public boolean hasIsGuaranteed() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional bool is_guaranteed = 2;</code>
       * @return The isGuaranteed.
       */
      @java.lang.Override
      public boolean getIsGuaranteed() {
        return isGuaranteed_;
      }
      /**
       * <code>optional bool is_guaranteed = 2;</code>
       * @param value The isGuaranteed to set.
       * @return This builder for chaining.
       */
      public Builder setIsGuaranteed(boolean value) {
        bitField0_ |= 0x00000002;
        isGuaranteed_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_guaranteed = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsGuaranteed() {
        bitField0_ = (bitField0_ & ~0x00000002);
        isGuaranteed_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:UpdateFragmentResponseProto)
    }

    // @@protoc_insertion_point(class_scope:UpdateFragmentResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<UpdateFragmentResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<UpdateFragmentResponseProto>() {
      @java.lang.Override
      public UpdateFragmentResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateFragmentResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<UpdateFragmentResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateFragmentResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetTokenRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetTokenRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string app_id = 1;</code>
     * @return Whether the appId field is set.
     */
    boolean hasAppId();
    /**
     * <code>optional string app_id = 1;</code>
     * @return The appId.
     */
    java.lang.String getAppId();
    /**
     * <code>optional string app_id = 1;</code>
     * @return The bytes for appId.
     */
    com.google.protobuf.ByteString
        getAppIdBytes();
  }
  /**
   * Protobuf type {@code GetTokenRequestProto}
   */
  public static final class GetTokenRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetTokenRequestProto)
      GetTokenRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetTokenRequestProto.newBuilder() to construct.
    private GetTokenRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetTokenRequestProto() {
      appId_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetTokenRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetTokenRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              appId_ = bs;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int APP_ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object appId_;
    /**
     * <code>optional string app_id = 1;</code>
     * @return Whether the appId field is set.
     */
    @java.lang.Override
    public boolean hasAppId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string app_id = 1;</code>
     * @return The appId.
     */
    @java.lang.Override
    public java.lang.String getAppId() {
      java.lang.Object ref = appId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          appId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string app_id = 1;</code>
     * @return The bytes for appId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getAppIdBytes() {
      java.lang.Object ref = appId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        appId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, appId_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, appId_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) obj;

      if (hasAppId() != other.hasAppId()) return false;
      if (hasAppId()) {
        if (!getAppId()
            .equals(other.getAppId())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasAppId()) {
        hash = (37 * hash) + APP_ID_FIELD_NUMBER;
        hash = (53 * hash) + getAppId().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetTokenRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetTokenRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        appId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.appId_ = appId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance()) return this;
        if (other.hasAppId()) {
          bitField0_ |= 0x00000001;
          appId_ = other.appId_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object appId_ = "";
      /**
       * <code>optional string app_id = 1;</code>
       * @return Whether the appId field is set.
       */
      public boolean hasAppId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string app_id = 1;</code>
       * @return The appId.
       */
      public java.lang.String getAppId() {
        java.lang.Object ref = appId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            appId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string app_id = 1;</code>
       * @return The bytes for appId.
       */
      public com.google.protobuf.ByteString
          getAppIdBytes() {
        java.lang.Object ref = appId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          appId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string app_id = 1;</code>
       * @param value The appId to set.
       * @return This builder for chaining.
       */
      public Builder setAppId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        appId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string app_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearAppId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        appId_ = getDefaultInstance().getAppId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string app_id = 1;</code>
       * @param value The bytes for appId to set.
       * @return This builder for chaining.
       */
      public Builder setAppIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        appId_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetTokenRequestProto)
    }

    // @@protoc_insertion_point(class_scope:GetTokenRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetTokenRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<GetTokenRequestProto>() {
      @java.lang.Override
      public GetTokenRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetTokenRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetTokenRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetTokenRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetTokenResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetTokenResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional bytes token = 1;</code>
     * @return Whether the token field is set.
     */
    boolean hasToken();
    /**
     * <code>optional bytes token = 1;</code>
     * @return The token.
     */
    com.google.protobuf.ByteString getToken();
  }
  /**
   * Protobuf type {@code GetTokenResponseProto}
   */
  public static final class GetTokenResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetTokenResponseProto)
      GetTokenResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetTokenResponseProto.newBuilder() to construct.
    private GetTokenResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetTokenResponseProto() {
      token_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetTokenResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetTokenResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              bitField0_ |= 0x00000001;
              token_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int TOKEN_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString token_;
    /**
     * <code>optional bytes token = 1;</code>
     * @return Whether the token field is set.
     */
    @java.lang.Override
    public boolean hasToken() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bytes token = 1;</code>
     * @return The token.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getToken() {
      return token_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(1, token_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, token_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) obj;

      if (hasToken() != other.hasToken()) return false;
      if (hasToken()) {
        if (!getToken()
            .equals(other.getToken())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasToken()) {
        hash = (37 * hash) + TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getToken().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetTokenResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetTokenResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        token_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetTokenResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.token_ = token_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance()) return this;
        if (other.hasToken()) {
          setToken(other.getToken());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes token = 1;</code>
       * @return Whether the token field is set.
       */
      @java.lang.Override
      public boolean hasToken() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bytes token = 1;</code>
       * @return The token.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getToken() {
        return token_;
      }
      /**
       * <code>optional bytes token = 1;</code>
       * @param value The token to set.
       * @return This builder for chaining.
       */
      public Builder setToken(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        token_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes token = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearToken() {
        bitField0_ = (bitField0_ & ~0x00000001);
        token_ = getDefaultInstance().getToken();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetTokenResponseProto)
    }

    // @@protoc_insertion_point(class_scope:GetTokenResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetTokenResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<GetTokenResponseProto>() {
      @java.lang.Override
      public GetTokenResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetTokenResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetTokenResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetTokenResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface LlapOutputSocketInitMessageOrBuilder extends
      // @@protoc_insertion_point(interface_extends:LlapOutputSocketInitMessage)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required string fragment_id = 1;</code>
     * @return Whether the fragmentId field is set.
     */
    boolean hasFragmentId();
    /**
     * <code>required string fragment_id = 1;</code>
     * @return The fragmentId.
     */
    java.lang.String getFragmentId();
    /**
     * <code>required string fragment_id = 1;</code>
     * @return The bytes for fragmentId.
     */
    com.google.protobuf.ByteString
        getFragmentIdBytes();

    /**
     * <code>optional bytes token = 2;</code>
     * @return Whether the token field is set.
     */
    boolean hasToken();
    /**
     * <code>optional bytes token = 2;</code>
     * @return The token.
     */
    com.google.protobuf.ByteString getToken();
  }
  /**
   * <pre>
   * The message sent by external client to claim the output from the output socket.
   * </pre>
   *
   * Protobuf type {@code LlapOutputSocketInitMessage}
   */
  public static final class LlapOutputSocketInitMessage extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:LlapOutputSocketInitMessage)
      LlapOutputSocketInitMessageOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use LlapOutputSocketInitMessage.newBuilder() to construct.
    private LlapOutputSocketInitMessage(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private LlapOutputSocketInitMessage() {
      fragmentId_ = "";
      token_ = com.google.protobuf.ByteString.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new LlapOutputSocketInitMessage();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private LlapOutputSocketInitMessage(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              fragmentId_ = bs;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              token_ = input.readBytes();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.Builder.class);
    }

    private int bitField0_;
    public static final int FRAGMENT_ID_FIELD_NUMBER = 1;
    private volatile java.lang.Object fragmentId_;
    /**
     * <code>required string fragment_id = 1;</code>
     * @return Whether the fragmentId field is set.
     */
    @java.lang.Override
    public boolean hasFragmentId() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required string fragment_id = 1;</code>
     * @return The fragmentId.
     */
    @java.lang.Override
    public java.lang.String getFragmentId() {
      java.lang.Object ref = fragmentId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          fragmentId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string fragment_id = 1;</code>
     * @return The bytes for fragmentId.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getFragmentIdBytes() {
      java.lang.Object ref = fragmentId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        fragmentId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TOKEN_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString token_;
    /**
     * <code>optional bytes token = 2;</code>
     * @return Whether the token field is set.
     */
    @java.lang.Override
    public boolean hasToken() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional bytes token = 2;</code>
     * @return The token.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getToken() {
      return token_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasFragmentId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, fragmentId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeBytes(2, token_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, fragmentId_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, token_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) obj;

      if (hasFragmentId() != other.hasFragmentId()) return false;
      if (hasFragmentId()) {
        if (!getFragmentId()
            .equals(other.getFragmentId())) return false;
      }
      if (hasToken() != other.hasToken()) return false;
      if (hasToken()) {
        if (!getToken()
            .equals(other.getToken())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasFragmentId()) {
        hash = (37 * hash) + FRAGMENT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getFragmentId().hashCode();
      }
      if (hasToken()) {
        hash = (37 * hash) + TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getToken().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * The message sent by external client to claim the output from the output socket.
     * </pre>
     *
     * Protobuf type {@code LlapOutputSocketInitMessage}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:LlapOutputSocketInitMessage)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessageOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        fragmentId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        token_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_LlapOutputSocketInitMessage_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.fragmentId_ = fragmentId_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.token_ = token_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage.getDefaultInstance()) return this;
        if (other.hasFragmentId()) {
          bitField0_ |= 0x00000001;
          fragmentId_ = other.fragmentId_;
          onChanged();
        }
        if (other.hasToken()) {
          setToken(other.getToken());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasFragmentId()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object fragmentId_ = "";
      /**
       * <code>required string fragment_id = 1;</code>
       * @return Whether the fragmentId field is set.
       */
      public boolean hasFragmentId() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required string fragment_id = 1;</code>
       * @return The fragmentId.
       */
      public java.lang.String getFragmentId() {
        java.lang.Object ref = fragmentId_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            fragmentId_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string fragment_id = 1;</code>
       * @return The bytes for fragmentId.
       */
      public com.google.protobuf.ByteString
          getFragmentIdBytes() {
        java.lang.Object ref = fragmentId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          fragmentId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string fragment_id = 1;</code>
       * @param value The fragmentId to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        fragmentId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string fragment_id = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFragmentId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        fragmentId_ = getDefaultInstance().getFragmentId();
        onChanged();
        return this;
      }
      /**
       * <code>required string fragment_id = 1;</code>
       * @param value The bytes for fragmentId to set.
       * @return This builder for chaining.
       */
      public Builder setFragmentIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        fragmentId_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.ByteString token_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes token = 2;</code>
       * @return Whether the token field is set.
       */
      @java.lang.Override
      public boolean hasToken() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional bytes token = 2;</code>
       * @return The token.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getToken() {
        return token_;
      }
      /**
       * <code>optional bytes token = 2;</code>
       * @param value The token to set.
       * @return This builder for chaining.
       */
      public Builder setToken(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        token_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes token = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearToken() {
        bitField0_ = (bitField0_ & ~0x00000002);
        token_ = getDefaultInstance().getToken();
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:LlapOutputSocketInitMessage)
    }

    // @@protoc_insertion_point(class_scope:LlapOutputSocketInitMessage)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<LlapOutputSocketInitMessage>
        PARSER = new com.google.protobuf.AbstractParser<LlapOutputSocketInitMessage>() {
      @java.lang.Override
      public LlapOutputSocketInitMessage parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new LlapOutputSocketInitMessage(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<LlapOutputSocketInitMessage> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<LlapOutputSocketInitMessage> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapOutputSocketInitMessage getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PurgeCacheRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:PurgeCacheRequestProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code PurgeCacheRequestProto}
   */
  public static final class PurgeCacheRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:PurgeCacheRequestProto)
      PurgeCacheRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use PurgeCacheRequestProto.newBuilder() to construct.
    private PurgeCacheRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private PurgeCacheRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new PurgeCacheRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private PurgeCacheRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code PurgeCacheRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:PurgeCacheRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:PurgeCacheRequestProto)
    }

    // @@protoc_insertion_point(class_scope:PurgeCacheRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<PurgeCacheRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<PurgeCacheRequestProto>() {
      @java.lang.Override
      public PurgeCacheRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new PurgeCacheRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<PurgeCacheRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<PurgeCacheRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PurgeCacheResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:PurgeCacheResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional int64 purged_memory_bytes = 1;</code>
     * @return Whether the purgedMemoryBytes field is set.
     */
    boolean hasPurgedMemoryBytes();
    /**
     * <code>optional int64 purged_memory_bytes = 1;</code>
     * @return The purgedMemoryBytes.
     */
    long getPurgedMemoryBytes();
  }
  /**
   * Protobuf type {@code PurgeCacheResponseProto}
   */
  public static final class PurgeCacheResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:PurgeCacheResponseProto)
      PurgeCacheResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use PurgeCacheResponseProto.newBuilder() to construct.
    private PurgeCacheResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private PurgeCacheResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new PurgeCacheResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private PurgeCacheResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              purgedMemoryBytes_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int PURGED_MEMORY_BYTES_FIELD_NUMBER = 1;
    private long purgedMemoryBytes_;
    /**
     * <code>optional int64 purged_memory_bytes = 1;</code>
     * @return Whether the purgedMemoryBytes field is set.
     */
    @java.lang.Override
    public boolean hasPurgedMemoryBytes() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int64 purged_memory_bytes = 1;</code>
     * @return The purgedMemoryBytes.
     */
    @java.lang.Override
    public long getPurgedMemoryBytes() {
      return purgedMemoryBytes_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(1, purgedMemoryBytes_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, purgedMemoryBytes_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) obj;

      if (hasPurgedMemoryBytes() != other.hasPurgedMemoryBytes()) return false;
      if (hasPurgedMemoryBytes()) {
        if (getPurgedMemoryBytes()
            != other.getPurgedMemoryBytes()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPurgedMemoryBytes()) {
        hash = (37 * hash) + PURGED_MEMORY_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getPurgedMemoryBytes());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code PurgeCacheResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:PurgeCacheResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        purgedMemoryBytes_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_PurgeCacheResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.purgedMemoryBytes_ = purgedMemoryBytes_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance()) return this;
        if (other.hasPurgedMemoryBytes()) {
          setPurgedMemoryBytes(other.getPurgedMemoryBytes());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long purgedMemoryBytes_ ;
      /**
       * <code>optional int64 purged_memory_bytes = 1;</code>
       * @return Whether the purgedMemoryBytes field is set.
       */
      @java.lang.Override
      public boolean hasPurgedMemoryBytes() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int64 purged_memory_bytes = 1;</code>
       * @return The purgedMemoryBytes.
       */
      @java.lang.Override
      public long getPurgedMemoryBytes() {
        return purgedMemoryBytes_;
      }
      /**
       * <code>optional int64 purged_memory_bytes = 1;</code>
       * @param value The purgedMemoryBytes to set.
       * @return This builder for chaining.
       */
      public Builder setPurgedMemoryBytes(long value) {
        bitField0_ |= 0x00000001;
        purgedMemoryBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 purged_memory_bytes = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearPurgedMemoryBytes() {
        bitField0_ = (bitField0_ & ~0x00000001);
        purgedMemoryBytes_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:PurgeCacheResponseProto)
    }

    // @@protoc_insertion_point(class_scope:PurgeCacheResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<PurgeCacheResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<PurgeCacheResponseProto>() {
      @java.lang.Override
      public PurgeCacheResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new PurgeCacheResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<PurgeCacheResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<PurgeCacheResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface MapEntryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:MapEntry)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string key = 1;</code>
     * @return Whether the key field is set.
     */
    boolean hasKey();
    /**
     * <code>optional string key = 1;</code>
     * @return The key.
     */
    java.lang.String getKey();
    /**
     * <code>optional string key = 1;</code>
     * @return The bytes for key.
     */
    com.google.protobuf.ByteString
        getKeyBytes();

    /**
     * <code>optional int64 value = 2;</code>
     * @return Whether the value field is set.
     */
    boolean hasValue();
    /**
     * <code>optional int64 value = 2;</code>
     * @return The value.
     */
    long getValue();
  }
  /**
   * Protobuf type {@code MapEntry}
   */
  public static final class MapEntry extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:MapEntry)
      MapEntryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use MapEntry.newBuilder() to construct.
    private MapEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private MapEntry() {
      key_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new MapEntry();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private MapEntry(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              key_ = bs;
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              value_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder.class);
    }

    private int bitField0_;
    public static final int KEY_FIELD_NUMBER = 1;
    private volatile java.lang.Object key_;
    /**
     * <code>optional string key = 1;</code>
     * @return Whether the key field is set.
     */
    @java.lang.Override
    public boolean hasKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string key = 1;</code>
     * @return The key.
     */
    @java.lang.Override
    public java.lang.String getKey() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          key_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string key = 1;</code>
     * @return The bytes for key.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getKeyBytes() {
      java.lang.Object ref = key_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        key_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUE_FIELD_NUMBER = 2;
    private long value_;
    /**
     * <code>optional int64 value = 2;</code>
     * @return Whether the value field is set.
     */
    @java.lang.Override
    public boolean hasValue() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 value = 2;</code>
     * @return The value.
     */
    @java.lang.Override
    public long getValue() {
      return value_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, value_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, key_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, value_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) obj;

      if (hasKey() != other.hasKey()) return false;
      if (hasKey()) {
        if (!getKey()
            .equals(other.getKey())) return false;
      }
      if (hasValue() != other.hasValue()) return false;
      if (hasValue()) {
        if (getValue()
            != other.getValue()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasKey()) {
        hash = (37 * hash) + KEY_FIELD_NUMBER;
        hash = (53 * hash) + getKey().hashCode();
      }
      if (hasValue()) {
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getValue());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code MapEntry}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:MapEntry)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        key_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        value_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_MapEntry_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.key_ = key_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.value_ = value_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance()) return this;
        if (other.hasKey()) {
          bitField0_ |= 0x00000001;
          key_ = other.key_;
          onChanged();
        }
        if (other.hasValue()) {
          setValue(other.getValue());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object key_ = "";
      /**
       * <code>optional string key = 1;</code>
       * @return Whether the key field is set.
       */
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string key = 1;</code>
       * @return The key.
       */
      public java.lang.String getKey() {
        java.lang.Object ref = key_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            key_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       * @return The bytes for key.
       */
      public com.google.protobuf.ByteString
          getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string key = 1;</code>
       * @param value The key to set.
       * @return This builder for chaining.
       */
      public Builder setKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        key_ = getDefaultInstance().getKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string key = 1;</code>
       * @param value The bytes for key to set.
       * @return This builder for chaining.
       */
      public Builder setKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        key_ = value;
        onChanged();
        return this;
      }

      private long value_ ;
      /**
       * <code>optional int64 value = 2;</code>
       * @return Whether the value field is set.
       */
      @java.lang.Override
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 value = 2;</code>
       * @return The value.
       */
      @java.lang.Override
      public long getValue() {
        return value_;
      }
      /**
       * <code>optional int64 value = 2;</code>
       * @param value The value to set.
       * @return This builder for chaining.
       */
      public Builder setValue(long value) {
        bitField0_ |= 0x00000002;
        value_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 value = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearValue() {
        bitField0_ = (bitField0_ & ~0x00000002);
        value_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:MapEntry)
    }

    // @@protoc_insertion_point(class_scope:MapEntry)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<MapEntry>
        PARSER = new com.google.protobuf.AbstractParser<MapEntry>() {
      @java.lang.Override
      public MapEntry parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MapEntry(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<MapEntry> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<MapEntry> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetDaemonMetricsRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetDaemonMetricsRequestProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code GetDaemonMetricsRequestProto}
   */
  public static final class GetDaemonMetricsRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetDaemonMetricsRequestProto)
      GetDaemonMetricsRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetDaemonMetricsRequestProto.newBuilder() to construct.
    private GetDaemonMetricsRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetDaemonMetricsRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetDaemonMetricsRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetDaemonMetricsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetDaemonMetricsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetDaemonMetricsRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetDaemonMetricsRequestProto)
    }

    // @@protoc_insertion_point(class_scope:GetDaemonMetricsRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetDaemonMetricsRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<GetDaemonMetricsRequestProto>() {
      @java.lang.Override
      public GetDaemonMetricsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetDaemonMetricsRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetDaemonMetricsRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetDaemonMetricsRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetDaemonMetricsResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetDaemonMetricsResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> 
        getMetricsList();
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getMetrics(int index);
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    int getMetricsCount();
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
        getMetricsOrBuilderList();
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder getMetricsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code GetDaemonMetricsResponseProto}
   */
  public static final class GetDaemonMetricsResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetDaemonMetricsResponseProto)
      GetDaemonMetricsResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetDaemonMetricsResponseProto.newBuilder() to construct.
    private GetDaemonMetricsResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetDaemonMetricsResponseProto() {
      metrics_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetDaemonMetricsResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetDaemonMetricsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                metrics_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry>();
                mutable_bitField0_ |= 0x00000001;
              }
              metrics_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          metrics_ = java.util.Collections.unmodifiableList(metrics_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.Builder.class);
    }

    public static final int METRICS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> metrics_;
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> getMetricsList() {
      return metrics_;
    }
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
        getMetricsOrBuilderList() {
      return metrics_;
    }
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    @java.lang.Override
    public int getMetricsCount() {
      return metrics_.size();
    }
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getMetrics(int index) {
      return metrics_.get(index);
    }
    /**
     * <code>repeated .MapEntry metrics = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder getMetricsOrBuilder(
        int index) {
      return metrics_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < metrics_.size(); i++) {
        output.writeMessage(1, metrics_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < metrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, metrics_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) obj;

      if (!getMetricsList()
          .equals(other.getMetricsList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getMetricsCount() > 0) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetDaemonMetricsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetDaemonMetricsResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getMetricsFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetDaemonMetricsResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (metricsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            metrics_ = java.util.Collections.unmodifiableList(metrics_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.metrics_ = metrics_;
        } else {
          result.metrics_ = metricsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance()) return this;
        if (metricsBuilder_ == null) {
          if (!other.metrics_.isEmpty()) {
            if (metrics_.isEmpty()) {
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureMetricsIsMutable();
              metrics_.addAll(other.metrics_);
            }
            onChanged();
          }
        } else {
          if (!other.metrics_.isEmpty()) {
            if (metricsBuilder_.isEmpty()) {
              metricsBuilder_.dispose();
              metricsBuilder_ = null;
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000001);
              metricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsFieldBuilder() : null;
            } else {
              metricsBuilder_.addAllMessages(other.metrics_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> metrics_ =
        java.util.Collections.emptyList();
      private void ensureMetricsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          metrics_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry>(metrics_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> metricsBuilder_;

      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> getMetricsList() {
        if (metricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metrics_);
        } else {
          return metricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public int getMetricsCount() {
        if (metricsBuilder_ == null) {
          return metrics_.size();
        } else {
          return metricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry getMetrics(int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);
        } else {
          return metricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder setMetrics(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.set(index, value);
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder setMetrics(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder addMetrics(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder addMetrics(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(index, value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder addMetrics(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder addMetrics(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder addAllMetrics(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry> values) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metrics_);
          onChanged();
        } else {
          metricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder clearMetrics() {
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public Builder removeMetrics(int index) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.remove(index);
          onChanged();
        } else {
          metricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder getMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder getMetricsOrBuilder(
          int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);  } else {
          return metricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
           getMetricsOrBuilderList() {
        if (metricsBuilder_ != null) {
          return metricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metrics_);
        }
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder addMetricsBuilder() {
        return getMetricsFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance());
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder addMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.getDefaultInstance());
      }
      /**
       * <code>repeated .MapEntry metrics = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder> 
           getMetricsBuilderList() {
        return getMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder> 
          getMetricsFieldBuilder() {
        if (metricsBuilder_ == null) {
          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.MapEntryOrBuilder>(
                  metrics_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          metrics_ = null;
        }
        return metricsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetDaemonMetricsResponseProto)
    }

    // @@protoc_insertion_point(class_scope:GetDaemonMetricsResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetDaemonMetricsResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<GetDaemonMetricsResponseProto>() {
      @java.lang.Override
      public GetDaemonMetricsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetDaemonMetricsResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetDaemonMetricsResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetDaemonMetricsResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SetCapacityRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SetCapacityRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional int32 executorNum = 1;</code>
     * @return Whether the executorNum field is set.
     */
    boolean hasExecutorNum();
    /**
     * <code>optional int32 executorNum = 1;</code>
     * @return The executorNum.
     */
    int getExecutorNum();

    /**
     * <code>optional int32 queueSize = 2;</code>
     * @return Whether the queueSize field is set.
     */
    boolean hasQueueSize();
    /**
     * <code>optional int32 queueSize = 2;</code>
     * @return The queueSize.
     */
    int getQueueSize();
  }
  /**
   * Protobuf type {@code SetCapacityRequestProto}
   */
  public static final class SetCapacityRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SetCapacityRequestProto)
      SetCapacityRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SetCapacityRequestProto.newBuilder() to construct.
    private SetCapacityRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SetCapacityRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SetCapacityRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SetCapacityRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              executorNum_ = input.readInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              queueSize_ = input.readInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int EXECUTORNUM_FIELD_NUMBER = 1;
    private int executorNum_;
    /**
     * <code>optional int32 executorNum = 1;</code>
     * @return Whether the executorNum field is set.
     */
    @java.lang.Override
    public boolean hasExecutorNum() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int32 executorNum = 1;</code>
     * @return The executorNum.
     */
    @java.lang.Override
    public int getExecutorNum() {
      return executorNum_;
    }

    public static final int QUEUESIZE_FIELD_NUMBER = 2;
    private int queueSize_;
    /**
     * <code>optional int32 queueSize = 2;</code>
     * @return Whether the queueSize field is set.
     */
    @java.lang.Override
    public boolean hasQueueSize() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int32 queueSize = 2;</code>
     * @return The queueSize.
     */
    @java.lang.Override
    public int getQueueSize() {
      return queueSize_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(1, executorNum_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt32(2, queueSize_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, executorNum_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, queueSize_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) obj;

      if (hasExecutorNum() != other.hasExecutorNum()) return false;
      if (hasExecutorNum()) {
        if (getExecutorNum()
            != other.getExecutorNum()) return false;
      }
      if (hasQueueSize() != other.hasQueueSize()) return false;
      if (hasQueueSize()) {
        if (getQueueSize()
            != other.getQueueSize()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasExecutorNum()) {
        hash = (37 * hash) + EXECUTORNUM_FIELD_NUMBER;
        hash = (53 * hash) + getExecutorNum();
      }
      if (hasQueueSize()) {
        hash = (37 * hash) + QUEUESIZE_FIELD_NUMBER;
        hash = (53 * hash) + getQueueSize();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SetCapacityRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SetCapacityRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        executorNum_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        queueSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.executorNum_ = executorNum_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.queueSize_ = queueSize_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.getDefaultInstance()) return this;
        if (other.hasExecutorNum()) {
          setExecutorNum(other.getExecutorNum());
        }
        if (other.hasQueueSize()) {
          setQueueSize(other.getQueueSize());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int executorNum_ ;
      /**
       * <code>optional int32 executorNum = 1;</code>
       * @return Whether the executorNum field is set.
       */
      @java.lang.Override
      public boolean hasExecutorNum() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int32 executorNum = 1;</code>
       * @return The executorNum.
       */
      @java.lang.Override
      public int getExecutorNum() {
        return executorNum_;
      }
      /**
       * <code>optional int32 executorNum = 1;</code>
       * @param value The executorNum to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorNum(int value) {
        bitField0_ |= 0x00000001;
        executorNum_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 executorNum = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorNum() {
        bitField0_ = (bitField0_ & ~0x00000001);
        executorNum_ = 0;
        onChanged();
        return this;
      }

      private int queueSize_ ;
      /**
       * <code>optional int32 queueSize = 2;</code>
       * @return Whether the queueSize field is set.
       */
      @java.lang.Override
      public boolean hasQueueSize() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int32 queueSize = 2;</code>
       * @return The queueSize.
       */
      @java.lang.Override
      public int getQueueSize() {
        return queueSize_;
      }
      /**
       * <code>optional int32 queueSize = 2;</code>
       * @param value The queueSize to set.
       * @return This builder for chaining.
       */
      public Builder setQueueSize(int value) {
        bitField0_ |= 0x00000002;
        queueSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 queueSize = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearQueueSize() {
        bitField0_ = (bitField0_ & ~0x00000002);
        queueSize_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SetCapacityRequestProto)
    }

    // @@protoc_insertion_point(class_scope:SetCapacityRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SetCapacityRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<SetCapacityRequestProto>() {
      @java.lang.Override
      public SetCapacityRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SetCapacityRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SetCapacityRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SetCapacityRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SetCapacityResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:SetCapacityResponseProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code SetCapacityResponseProto}
   */
  public static final class SetCapacityResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:SetCapacityResponseProto)
      SetCapacityResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SetCapacityResponseProto.newBuilder() to construct.
    private SetCapacityResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SetCapacityResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SetCapacityResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SetCapacityResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code SetCapacityResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:SetCapacityResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_SetCapacityResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:SetCapacityResponseProto)
    }

    // @@protoc_insertion_point(class_scope:SetCapacityResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<SetCapacityResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<SetCapacityResponseProto>() {
      @java.lang.Override
      public SetCapacityResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SetCapacityResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SetCapacityResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SetCapacityResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface EvictEntityRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:EvictEntityRequestProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required string db_name = 1;</code>
     * @return Whether the dbName field is set.
     */
    boolean hasDbName();
    /**
     * <code>required string db_name = 1;</code>
     * @return The dbName.
     */
    java.lang.String getDbName();
    /**
     * <code>required string db_name = 1;</code>
     * @return The bytes for dbName.
     */
    com.google.protobuf.ByteString
        getDbNameBytes();

    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> 
        getTableList();
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getTable(int index);
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    int getTableCount();
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
        getTableOrBuilderList();
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder getTableOrBuilder(
        int index);
  }
  /**
   * <pre>
   * Used for proactive eviction request. Must contain one DB name, and optionally table information.
   * </pre>
   *
   * Protobuf type {@code EvictEntityRequestProto}
   */
  public static final class EvictEntityRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:EvictEntityRequestProto)
      EvictEntityRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use EvictEntityRequestProto.newBuilder() to construct.
    private EvictEntityRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private EvictEntityRequestProto() {
      dbName_ = "";
      table_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new EvictEntityRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private EvictEntityRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              dbName_ = bs;
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                table_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              table_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          table_ = java.util.Collections.unmodifiableList(table_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.Builder.class);
    }

    private int bitField0_;
    public static final int DB_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object dbName_;
    /**
     * <code>required string db_name = 1;</code>
     * @return Whether the dbName field is set.
     */
    @java.lang.Override
    public boolean hasDbName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required string db_name = 1;</code>
     * @return The dbName.
     */
    @java.lang.Override
    public java.lang.String getDbName() {
      java.lang.Object ref = dbName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          dbName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string db_name = 1;</code>
     * @return The bytes for dbName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDbNameBytes() {
      java.lang.Object ref = dbName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        dbName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TABLE_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> table_;
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> getTableList() {
      return table_;
    }
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
        getTableOrBuilderList() {
      return table_;
    }
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    @java.lang.Override
    public int getTableCount() {
      return table_.size();
    }
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getTable(int index) {
      return table_.get(index);
    }
    /**
     * <code>repeated .TableProto table = 2;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder getTableOrBuilder(
        int index) {
      return table_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasDbName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getTableCount(); i++) {
        if (!getTable(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, dbName_);
      }
      for (int i = 0; i < table_.size(); i++) {
        output.writeMessage(2, table_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, dbName_);
      }
      for (int i = 0; i < table_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, table_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) obj;

      if (hasDbName() != other.hasDbName()) return false;
      if (hasDbName()) {
        if (!getDbName()
            .equals(other.getDbName())) return false;
      }
      if (!getTableList()
          .equals(other.getTableList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasDbName()) {
        hash = (37 * hash) + DB_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getDbName().hashCode();
      }
      if (getTableCount() > 0) {
        hash = (37 * hash) + TABLE_FIELD_NUMBER;
        hash = (53 * hash) + getTableList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Used for proactive eviction request. Must contain one DB name, and optionally table information.
     * </pre>
     *
     * Protobuf type {@code EvictEntityRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:EvictEntityRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getTableFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        dbName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (tableBuilder_ == null) {
          table_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          tableBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.dbName_ = dbName_;
        if (tableBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            table_ = java.util.Collections.unmodifiableList(table_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.table_ = table_;
        } else {
          result.table_ = tableBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance()) return this;
        if (other.hasDbName()) {
          bitField0_ |= 0x00000001;
          dbName_ = other.dbName_;
          onChanged();
        }
        if (tableBuilder_ == null) {
          if (!other.table_.isEmpty()) {
            if (table_.isEmpty()) {
              table_ = other.table_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureTableIsMutable();
              table_.addAll(other.table_);
            }
            onChanged();
          }
        } else {
          if (!other.table_.isEmpty()) {
            if (tableBuilder_.isEmpty()) {
              tableBuilder_.dispose();
              tableBuilder_ = null;
              table_ = other.table_;
              bitField0_ = (bitField0_ & ~0x00000002);
              tableBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getTableFieldBuilder() : null;
            } else {
              tableBuilder_.addAllMessages(other.table_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasDbName()) {
          return false;
        }
        for (int i = 0; i < getTableCount(); i++) {
          if (!getTable(i).isInitialized()) {
            return false;
          }
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object dbName_ = "";
      /**
       * <code>required string db_name = 1;</code>
       * @return Whether the dbName field is set.
       */
      public boolean hasDbName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required string db_name = 1;</code>
       * @return The dbName.
       */
      public java.lang.String getDbName() {
        java.lang.Object ref = dbName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            dbName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string db_name = 1;</code>
       * @return The bytes for dbName.
       */
      public com.google.protobuf.ByteString
          getDbNameBytes() {
        java.lang.Object ref = dbName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          dbName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string db_name = 1;</code>
       * @param value The dbName to set.
       * @return This builder for chaining.
       */
      public Builder setDbName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        dbName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string db_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDbName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        dbName_ = getDefaultInstance().getDbName();
        onChanged();
        return this;
      }
      /**
       * <code>required string db_name = 1;</code>
       * @param value The bytes for dbName to set.
       * @return This builder for chaining.
       */
      public Builder setDbNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        dbName_ = value;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> table_ =
        java.util.Collections.emptyList();
      private void ensureTableIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          table_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto>(table_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> tableBuilder_;

      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> getTableList() {
        if (tableBuilder_ == null) {
          return java.util.Collections.unmodifiableList(table_);
        } else {
          return tableBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public int getTableCount() {
        if (tableBuilder_ == null) {
          return table_.size();
        } else {
          return tableBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getTable(int index) {
        if (tableBuilder_ == null) {
          return table_.get(index);
        } else {
          return tableBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder setTable(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto value) {
        if (tableBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTableIsMutable();
          table_.set(index, value);
          onChanged();
        } else {
          tableBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder setTable(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder builderForValue) {
        if (tableBuilder_ == null) {
          ensureTableIsMutable();
          table_.set(index, builderForValue.build());
          onChanged();
        } else {
          tableBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder addTable(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto value) {
        if (tableBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTableIsMutable();
          table_.add(value);
          onChanged();
        } else {
          tableBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder addTable(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto value) {
        if (tableBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureTableIsMutable();
          table_.add(index, value);
          onChanged();
        } else {
          tableBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder addTable(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder builderForValue) {
        if (tableBuilder_ == null) {
          ensureTableIsMutable();
          table_.add(builderForValue.build());
          onChanged();
        } else {
          tableBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder addTable(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder builderForValue) {
        if (tableBuilder_ == null) {
          ensureTableIsMutable();
          table_.add(index, builderForValue.build());
          onChanged();
        } else {
          tableBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder addAllTable(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto> values) {
        if (tableBuilder_ == null) {
          ensureTableIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, table_);
          onChanged();
        } else {
          tableBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder clearTable() {
        if (tableBuilder_ == null) {
          table_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          tableBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public Builder removeTable(int index) {
        if (tableBuilder_ == null) {
          ensureTableIsMutable();
          table_.remove(index);
          onChanged();
        } else {
          tableBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder getTableBuilder(
          int index) {
        return getTableFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder getTableOrBuilder(
          int index) {
        if (tableBuilder_ == null) {
          return table_.get(index);  } else {
          return tableBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
           getTableOrBuilderList() {
        if (tableBuilder_ != null) {
          return tableBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(table_);
        }
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder addTableBuilder() {
        return getTableFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance());
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder addTableBuilder(
          int index) {
        return getTableFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance());
      }
      /**
       * <code>repeated .TableProto table = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder> 
           getTableBuilderList() {
        return getTableFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder> 
          getTableFieldBuilder() {
        if (tableBuilder_ == null) {
          tableBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder>(
                  table_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          table_ = null;
        }
        return tableBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:EvictEntityRequestProto)
    }

    // @@protoc_insertion_point(class_scope:EvictEntityRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<EvictEntityRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<EvictEntityRequestProto>() {
      @java.lang.Override
      public EvictEntityRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EvictEntityRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<EvictEntityRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<EvictEntityRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TableProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:TableProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required string table_name = 1;</code>
     * @return Whether the tableName field is set.
     */
    boolean hasTableName();
    /**
     * <code>required string table_name = 1;</code>
     * @return The tableName.
     */
    java.lang.String getTableName();
    /**
     * <code>required string table_name = 1;</code>
     * @return The bytes for tableName.
     */
    com.google.protobuf.ByteString
        getTableNameBytes();

    /**
     * <code>repeated string part_key = 2;</code>
     * @return A list containing the partKey.
     */
    java.util.List<java.lang.String>
        getPartKeyList();
    /**
     * <code>repeated string part_key = 2;</code>
     * @return The count of partKey.
     */
    int getPartKeyCount();
    /**
     * <code>repeated string part_key = 2;</code>
     * @param index The index of the element to return.
     * @return The partKey at the given index.
     */
    java.lang.String getPartKey(int index);
    /**
     * <code>repeated string part_key = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partKey at the given index.
     */
    com.google.protobuf.ByteString
        getPartKeyBytes(int index);

    /**
     * <code>repeated string part_val = 3;</code>
     * @return A list containing the partVal.
     */
    java.util.List<java.lang.String>
        getPartValList();
    /**
     * <code>repeated string part_val = 3;</code>
     * @return The count of partVal.
     */
    int getPartValCount();
    /**
     * <code>repeated string part_val = 3;</code>
     * @param index The index of the element to return.
     * @return The partVal at the given index.
     */
    java.lang.String getPartVal(int index);
    /**
     * <code>repeated string part_val = 3;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partVal at the given index.
     */
    com.google.protobuf.ByteString
        getPartValBytes(int index);
  }
  /**
   * <pre>
   * Used in EvictEntityRequestProto, can be used for non-partitioned and partitioned tables too.
   * For the latter part_key contains only the keys, part_val has the values for all partitions on all keys:
   * e.g.: for partitions pk0=p00/pk1=p01/pk2=p02 and pk0=p10/pk1=p11/pk2=p12
   * part_key: [pk0, pk1, pk2], part_val: [p00, p01, p02, p10, p11, p12]
   * </pre>
   *
   * Protobuf type {@code TableProto}
   */
  public static final class TableProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:TableProto)
      TableProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TableProto.newBuilder() to construct.
    private TableProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TableProto() {
      tableName_ = "";
      partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new TableProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private TableProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              tableName_ = bs;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                partKey_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              partKey_.add(bs);
              break;
            }
            case 26: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                partVal_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000004;
              }
              partVal_.add(bs);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          partKey_ = partKey_.getUnmodifiableView();
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          partVal_ = partVal_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder.class);
    }

    private int bitField0_;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object tableName_;
    /**
     * <code>required string table_name = 1;</code>
     * @return Whether the tableName field is set.
     */
    @java.lang.Override
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required string table_name = 1;</code>
     * @return The tableName.
     */
    @java.lang.Override
    public java.lang.String getTableName() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          tableName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string table_name = 1;</code>
     * @return The bytes for tableName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTableNameBytes() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        tableName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PART_KEY_FIELD_NUMBER = 2;
    private com.google.protobuf.LazyStringList partKey_;
    /**
     * <code>repeated string part_key = 2;</code>
     * @return A list containing the partKey.
     */
    public com.google.protobuf.ProtocolStringList
        getPartKeyList() {
      return partKey_;
    }
    /**
     * <code>repeated string part_key = 2;</code>
     * @return The count of partKey.
     */
    public int getPartKeyCount() {
      return partKey_.size();
    }
    /**
     * <code>repeated string part_key = 2;</code>
     * @param index The index of the element to return.
     * @return The partKey at the given index.
     */
    public java.lang.String getPartKey(int index) {
      return partKey_.get(index);
    }
    /**
     * <code>repeated string part_key = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partKey at the given index.
     */
    public com.google.protobuf.ByteString
        getPartKeyBytes(int index) {
      return partKey_.getByteString(index);
    }

    public static final int PART_VAL_FIELD_NUMBER = 3;
    private com.google.protobuf.LazyStringList partVal_;
    /**
     * <code>repeated string part_val = 3;</code>
     * @return A list containing the partVal.
     */
    public com.google.protobuf.ProtocolStringList
        getPartValList() {
      return partVal_;
    }
    /**
     * <code>repeated string part_val = 3;</code>
     * @return The count of partVal.
     */
    public int getPartValCount() {
      return partVal_.size();
    }
    /**
     * <code>repeated string part_val = 3;</code>
     * @param index The index of the element to return.
     * @return The partVal at the given index.
     */
    public java.lang.String getPartVal(int index) {
      return partVal_.get(index);
    }
    /**
     * <code>repeated string part_val = 3;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partVal at the given index.
     */
    public com.google.protobuf.ByteString
        getPartValBytes(int index) {
      return partVal_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasTableName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, tableName_);
      }
      for (int i = 0; i < partKey_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, partKey_.getRaw(i));
      }
      for (int i = 0; i < partVal_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, partVal_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, tableName_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < partKey_.size(); i++) {
          dataSize += computeStringSizeNoTag(partKey_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getPartKeyList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < partVal_.size(); i++) {
          dataSize += computeStringSizeNoTag(partVal_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getPartValList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) obj;

      if (hasTableName() != other.hasTableName()) return false;
      if (hasTableName()) {
        if (!getTableName()
            .equals(other.getTableName())) return false;
      }
      if (!getPartKeyList()
          .equals(other.getPartKeyList())) return false;
      if (!getPartValList()
          .equals(other.getPartValList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (getPartKeyCount() > 0) {
        hash = (37 * hash) + PART_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getPartKeyList().hashCode();
      }
      if (getPartValCount() > 0) {
        hash = (37 * hash) + PART_VAL_FIELD_NUMBER;
        hash = (53 * hash) + getPartValList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Used in EvictEntityRequestProto, can be used for non-partitioned and partitioned tables too.
     * For the latter part_key contains only the keys, part_val has the values for all partitions on all keys:
     * e.g.: for partitions pk0=p00/pk1=p01/pk2=p02 and pk0=p10/pk1=p11/pk2=p12
     * part_key: [pk0, pk1, pk2], part_val: [p00, p01, p02, p10, p11, p12]
     * </pre>
     *
     * Protobuf type {@code TableProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:TableProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        tableName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_TableProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.tableName_ = tableName_;
        if (((bitField0_ & 0x00000002) != 0)) {
          partKey_ = partKey_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.partKey_ = partKey_;
        if (((bitField0_ & 0x00000004) != 0)) {
          partVal_ = partVal_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.partVal_ = partVal_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          bitField0_ |= 0x00000001;
          tableName_ = other.tableName_;
          onChanged();
        }
        if (!other.partKey_.isEmpty()) {
          if (partKey_.isEmpty()) {
            partKey_ = other.partKey_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensurePartKeyIsMutable();
            partKey_.addAll(other.partKey_);
          }
          onChanged();
        }
        if (!other.partVal_.isEmpty()) {
          if (partVal_.isEmpty()) {
            partVal_ = other.partVal_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensurePartValIsMutable();
            partVal_.addAll(other.partVal_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasTableName()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object tableName_ = "";
      /**
       * <code>required string table_name = 1;</code>
       * @return Whether the tableName field is set.
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required string table_name = 1;</code>
       * @return The tableName.
       */
      public java.lang.String getTableName() {
        java.lang.Object ref = tableName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            tableName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string table_name = 1;</code>
       * @return The bytes for tableName.
       */
      public com.google.protobuf.ByteString
          getTableNameBytes() {
        java.lang.Object ref = tableName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          tableName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string table_name = 1;</code>
       * @param value The tableName to set.
       * @return This builder for chaining.
       */
      public Builder setTableName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string table_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }
      /**
       * <code>required string table_name = 1;</code>
       * @param value The bytes for tableName to set.
       * @return This builder for chaining.
       */
      public Builder setTableNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensurePartKeyIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          partKey_ = new com.google.protobuf.LazyStringArrayList(partKey_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @return A list containing the partKey.
       */
      public com.google.protobuf.ProtocolStringList
          getPartKeyList() {
        return partKey_.getUnmodifiableView();
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @return The count of partKey.
       */
      public int getPartKeyCount() {
        return partKey_.size();
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param index The index of the element to return.
       * @return The partKey at the given index.
       */
      public java.lang.String getPartKey(int index) {
        return partKey_.get(index);
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the partKey at the given index.
       */
      public com.google.protobuf.ByteString
          getPartKeyBytes(int index) {
        return partKey_.getByteString(index);
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param index The index to set the value at.
       * @param value The partKey to set.
       * @return This builder for chaining.
       */
      public Builder setPartKey(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartKeyIsMutable();
        partKey_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param value The partKey to add.
       * @return This builder for chaining.
       */
      public Builder addPartKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartKeyIsMutable();
        partKey_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param values The partKey to add.
       * @return This builder for chaining.
       */
      public Builder addAllPartKey(
          java.lang.Iterable<java.lang.String> values) {
        ensurePartKeyIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, partKey_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPartKey() {
        partKey_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_key = 2;</code>
       * @param value The bytes of the partKey to add.
       * @return This builder for chaining.
       */
      public Builder addPartKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartKeyIsMutable();
        partKey_.add(value);
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensurePartValIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          partVal_ = new com.google.protobuf.LazyStringArrayList(partVal_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @return A list containing the partVal.
       */
      public com.google.protobuf.ProtocolStringList
          getPartValList() {
        return partVal_.getUnmodifiableView();
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @return The count of partVal.
       */
      public int getPartValCount() {
        return partVal_.size();
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param index The index of the element to return.
       * @return The partVal at the given index.
       */
      public java.lang.String getPartVal(int index) {
        return partVal_.get(index);
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param index The index of the value to return.
       * @return The bytes of the partVal at the given index.
       */
      public com.google.protobuf.ByteString
          getPartValBytes(int index) {
        return partVal_.getByteString(index);
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param index The index to set the value at.
       * @param value The partVal to set.
       * @return This builder for chaining.
       */
      public Builder setPartVal(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartValIsMutable();
        partVal_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param value The partVal to add.
       * @return This builder for chaining.
       */
      public Builder addPartVal(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartValIsMutable();
        partVal_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param values The partVal to add.
       * @return This builder for chaining.
       */
      public Builder addAllPartVal(
          java.lang.Iterable<java.lang.String> values) {
        ensurePartValIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, partVal_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearPartVal() {
        partVal_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string part_val = 3;</code>
       * @param value The bytes of the partVal to add.
       * @return This builder for chaining.
       */
      public Builder addPartValBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartValIsMutable();
        partVal_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:TableProto)
    }

    // @@protoc_insertion_point(class_scope:TableProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<TableProto>
        PARSER = new com.google.protobuf.AbstractParser<TableProto>() {
      @java.lang.Override
      public TableProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TableProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<TableProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<TableProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TableProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface EvictEntityResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:EvictEntityResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>required int64 evicted_bytes = 1;</code>
     * @return Whether the evictedBytes field is set.
     */
    boolean hasEvictedBytes();
    /**
     * <code>required int64 evicted_bytes = 1;</code>
     * @return The evictedBytes.
     */
    long getEvictedBytes();
  }
  /**
   * Protobuf type {@code EvictEntityResponseProto}
   */
  public static final class EvictEntityResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:EvictEntityResponseProto)
      EvictEntityResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use EvictEntityResponseProto.newBuilder() to construct.
    private EvictEntityResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private EvictEntityResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new EvictEntityResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private EvictEntityResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              evictedBytes_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int EVICTED_BYTES_FIELD_NUMBER = 1;
    private long evictedBytes_;
    /**
     * <code>required int64 evicted_bytes = 1;</code>
     * @return Whether the evictedBytes field is set.
     */
    @java.lang.Override
    public boolean hasEvictedBytes() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>required int64 evicted_bytes = 1;</code>
     * @return The evictedBytes.
     */
    @java.lang.Override
    public long getEvictedBytes() {
      return evictedBytes_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      if (!hasEvictedBytes()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(1, evictedBytes_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, evictedBytes_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) obj;

      if (hasEvictedBytes() != other.hasEvictedBytes()) return false;
      if (hasEvictedBytes()) {
        if (getEvictedBytes()
            != other.getEvictedBytes()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasEvictedBytes()) {
        hash = (37 * hash) + EVICTED_BYTES_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getEvictedBytes());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code EvictEntityResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:EvictEntityResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        evictedBytes_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_EvictEntityResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.evictedBytes_ = evictedBytes_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance()) return this;
        if (other.hasEvictedBytes()) {
          setEvictedBytes(other.getEvictedBytes());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        if (!hasEvictedBytes()) {
          return false;
        }
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long evictedBytes_ ;
      /**
       * <code>required int64 evicted_bytes = 1;</code>
       * @return Whether the evictedBytes field is set.
       */
      @java.lang.Override
      public boolean hasEvictedBytes() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>required int64 evicted_bytes = 1;</code>
       * @return The evictedBytes.
       */
      @java.lang.Override
      public long getEvictedBytes() {
        return evictedBytes_;
      }
      /**
       * <code>required int64 evicted_bytes = 1;</code>
       * @param value The evictedBytes to set.
       * @return This builder for chaining.
       */
      public Builder setEvictedBytes(long value) {
        bitField0_ |= 0x00000001;
        evictedBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 evicted_bytes = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearEvictedBytes() {
        bitField0_ = (bitField0_ & ~0x00000001);
        evictedBytes_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:EvictEntityResponseProto)
    }

    // @@protoc_insertion_point(class_scope:EvictEntityResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<EvictEntityResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<EvictEntityResponseProto>() {
      @java.lang.Override
      public EvictEntityResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EvictEntityResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<EvictEntityResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<EvictEntityResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetCacheContentRequestProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetCacheContentRequestProto)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code GetCacheContentRequestProto}
   */
  public static final class GetCacheContentRequestProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetCacheContentRequestProto)
      GetCacheContentRequestProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetCacheContentRequestProto.newBuilder() to construct.
    private GetCacheContentRequestProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetCacheContentRequestProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetCacheContentRequestProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetCacheContentRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetCacheContentRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetCacheContentRequestProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentRequestProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetCacheContentRequestProto)
    }

    // @@protoc_insertion_point(class_scope:GetCacheContentRequestProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetCacheContentRequestProto>
        PARSER = new com.google.protobuf.AbstractParser<GetCacheContentRequestProto>() {
      @java.lang.Override
      public GetCacheContentRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetCacheContentRequestProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetCacheContentRequestProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetCacheContentRequestProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetCacheContentResponseProtoOrBuilder extends
      // @@protoc_insertion_point(interface_extends:GetCacheContentResponseProto)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional .CacheEntryList result = 1;</code>
     * @return Whether the result field is set.
     */
    boolean hasResult();
    /**
     * <code>optional .CacheEntryList result = 1;</code>
     * @return The result.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult();
    /**
     * <code>optional .CacheEntryList result = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder getResultOrBuilder();
  }
  /**
   * Protobuf type {@code GetCacheContentResponseProto}
   */
  public static final class GetCacheContentResponseProto extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:GetCacheContentResponseProto)
      GetCacheContentResponseProtoOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetCacheContentResponseProto.newBuilder() to construct.
    private GetCacheContentResponseProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetCacheContentResponseProto() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetCacheContentResponseProto();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private GetCacheContentResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) != 0)) {
                subBuilder = result_.toBuilder();
              }
              result_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(result_);
                result_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.Builder.class);
    }

    private int bitField0_;
    public static final int RESULT_FIELD_NUMBER = 1;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result_;
    /**
     * <code>optional .CacheEntryList result = 1;</code>
     * @return Whether the result field is set.
     */
    @java.lang.Override
    public boolean hasResult() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional .CacheEntryList result = 1;</code>
     * @return The result.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult() {
      return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
    }
    /**
     * <code>optional .CacheEntryList result = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder getResultOrBuilder() {
      return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(1, getResult());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getResult());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) obj;

      if (hasResult() != other.hasResult()) return false;
      if (hasResult()) {
        if (!getResult()
            .equals(other.getResult())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasResult()) {
        hash = (37 * hash) + RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getResult().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code GetCacheContentResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:GetCacheContentResponseProto)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getResultFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (resultBuilder_ == null) {
          result_ = null;
        } else {
          resultBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_GetCacheContentResponseProto_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          if (resultBuilder_ == null) {
            result.result_ = result_;
          } else {
            result.result_ = resultBuilder_.build();
          }
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance()) return this;
        if (other.hasResult()) {
          mergeResult(other.getResult());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder> resultBuilder_;
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       * @return Whether the result field is set.
       */
      public boolean hasResult() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       * @return The result.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getResult() {
        if (resultBuilder_ == null) {
          return result_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
        } else {
          return resultBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public Builder setResult(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList value) {
        if (resultBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          result_ = value;
          onChanged();
        } else {
          resultBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public Builder setResult(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder builderForValue) {
        if (resultBuilder_ == null) {
          result_ = builderForValue.build();
          onChanged();
        } else {
          resultBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public Builder mergeResult(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList value) {
        if (resultBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
              result_ != null &&
              result_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance()) {
            result_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.newBuilder(result_).mergeFrom(value).buildPartial();
          } else {
            result_ = value;
          }
          onChanged();
        } else {
          resultBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public Builder clearResult() {
        if (resultBuilder_ == null) {
          result_ = null;
          onChanged();
        } else {
          resultBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder getResultBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getResultFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder getResultOrBuilder() {
        if (resultBuilder_ != null) {
          return resultBuilder_.getMessageOrBuilder();
        } else {
          return result_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance() : result_;
        }
      }
      /**
       * <code>optional .CacheEntryList result = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder> 
          getResultFieldBuilder() {
        if (resultBuilder_ == null) {
          resultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder>(
                  getResult(),
                  getParentForChildren(),
                  isClean());
          result_ = null;
        }
        return resultBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:GetCacheContentResponseProto)
    }

    // @@protoc_insertion_point(class_scope:GetCacheContentResponseProto)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<GetCacheContentResponseProto>
        PARSER = new com.google.protobuf.AbstractParser<GetCacheContentResponseProto>() {
      @java.lang.Override
      public GetCacheContentResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetCacheContentResponseProto(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<GetCacheContentResponseProto> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<GetCacheContentResponseProto> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheEntryListOrBuilder extends
      // @@protoc_insertion_point(interface_extends:CacheEntryList)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> 
        getEntriesList();
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getEntries(int index);
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    int getEntriesCount();
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
        getEntriesOrBuilderList();
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder getEntriesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code CacheEntryList}
   */
  public static final class CacheEntryList extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:CacheEntryList)
      CacheEntryListOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheEntryList.newBuilder() to construct.
    private CacheEntryList(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheEntryList() {
      entries_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheEntryList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheEntryList(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                entries_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry>();
                mutable_bitField0_ |= 0x00000001;
              }
              entries_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          entries_ = java.util.Collections.unmodifiableList(entries_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder.class);
    }

    public static final int ENTRIES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> entries_;
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> getEntriesList() {
      return entries_;
    }
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
        getEntriesOrBuilderList() {
      return entries_;
    }
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    @java.lang.Override
    public int getEntriesCount() {
      return entries_.size();
    }
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getEntries(int index) {
      return entries_.get(index);
    }
    /**
     * <code>repeated .CacheEntry entries = 1;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder getEntriesOrBuilder(
        int index) {
      return entries_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < entries_.size(); i++) {
        output.writeMessage(1, entries_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < entries_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, entries_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) obj;

      if (!getEntriesList()
          .equals(other.getEntriesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getEntriesCount() > 0) {
        hash = (37 * hash) + ENTRIES_FIELD_NUMBER;
        hash = (53 * hash) + getEntriesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CacheEntryList}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:CacheEntryList)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryListOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getEntriesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          entriesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryList_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList(this);
        int from_bitField0_ = bitField0_;
        if (entriesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            entries_ = java.util.Collections.unmodifiableList(entries_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.entries_ = entries_;
        } else {
          result.entries_ = entriesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList.getDefaultInstance()) return this;
        if (entriesBuilder_ == null) {
          if (!other.entries_.isEmpty()) {
            if (entries_.isEmpty()) {
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEntriesIsMutable();
              entries_.addAll(other.entries_);
            }
            onChanged();
          }
        } else {
          if (!other.entries_.isEmpty()) {
            if (entriesBuilder_.isEmpty()) {
              entriesBuilder_.dispose();
              entriesBuilder_ = null;
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
              entriesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getEntriesFieldBuilder() : null;
            } else {
              entriesBuilder_.addAllMessages(other.entries_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> entries_ =
        java.util.Collections.emptyList();
      private void ensureEntriesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          entries_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry>(entries_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> entriesBuilder_;

      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> getEntriesList() {
        if (entriesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(entries_);
        } else {
          return entriesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public int getEntriesCount() {
        if (entriesBuilder_ == null) {
          return entries_.size();
        } else {
          return entriesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getEntries(int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);
        } else {
          return entriesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder setEntries(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.set(index, value);
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder setEntries(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.set(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder addEntries(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder addEntries(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(index, value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder addEntries(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder addEntries(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder addAllEntries(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry> values) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, entries_);
          onChanged();
        } else {
          entriesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder clearEntries() {
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          entriesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public Builder removeEntries(int index) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.remove(index);
          onChanged();
        } else {
          entriesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder getEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder getEntriesOrBuilder(
          int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);  } else {
          return entriesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
           getEntriesOrBuilderList() {
        if (entriesBuilder_ != null) {
          return entriesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(entries_);
        }
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder addEntriesBuilder() {
        return getEntriesFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.getDefaultInstance());
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder addEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.getDefaultInstance());
      }
      /**
       * <code>repeated .CacheEntry entries = 1;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder> 
           getEntriesBuilderList() {
        return getEntriesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder> 
          getEntriesFieldBuilder() {
        if (entriesBuilder_ == null) {
          entriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder>(
                  entries_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          entries_ = null;
        }
        return entriesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:CacheEntryList)
    }

    // @@protoc_insertion_point(class_scope:CacheEntryList)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntryList>
        PARSER = new com.google.protobuf.AbstractParser<CacheEntryList>() {
      @java.lang.Override
      public CacheEntryList parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheEntryList(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheEntryList> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheEntryList> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryList getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheEntryOrBuilder extends
      // @@protoc_insertion_point(interface_extends:CacheEntry)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional bytes file_key = 1;</code>
     * @return Whether the fileKey field is set.
     */
    boolean hasFileKey();
    /**
     * <code>optional bytes file_key = 1;</code>
     * @return The fileKey.
     */
    com.google.protobuf.ByteString getFileKey();

    /**
     * <code>optional string file_path = 2;</code>
     * @return Whether the filePath field is set.
     */
    boolean hasFilePath();
    /**
     * <code>optional string file_path = 2;</code>
     * @return The filePath.
     */
    java.lang.String getFilePath();
    /**
     * <code>optional string file_path = 2;</code>
     * @return The bytes for filePath.
     */
    com.google.protobuf.ByteString
        getFilePathBytes();

    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     * @return Whether the cacheTag field is set.
     */
    boolean hasCacheTag();
    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     * @return The cacheTag.
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag();
    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder getCacheTagOrBuilder();

    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> 
        getRangesList();
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getRanges(int index);
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    int getRangesCount();
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
        getRangesOrBuilderList();
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder getRangesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code CacheEntry}
   */
  public static final class CacheEntry extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:CacheEntry)
      CacheEntryOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheEntry.newBuilder() to construct.
    private CacheEntry(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheEntry() {
      fileKey_ = com.google.protobuf.ByteString.EMPTY;
      filePath_ = "";
      ranges_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheEntry();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheEntry(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              bitField0_ |= 0x00000001;
              fileKey_ = input.readBytes();
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000002;
              filePath_ = bs;
              break;
            }
            case 26: {
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) != 0)) {
                subBuilder = cacheTag_.toBuilder();
              }
              cacheTag_ = input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cacheTag_);
                cacheTag_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                ranges_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange>();
                mutable_bitField0_ |= 0x00000008;
              }
              ranges_.add(
                  input.readMessage(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.PARSER, extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          ranges_ = java.util.Collections.unmodifiableList(ranges_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder.class);
    }

    private int bitField0_;
    public static final int FILE_KEY_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString fileKey_;
    /**
     * <code>optional bytes file_key = 1;</code>
     * @return Whether the fileKey field is set.
     */
    @java.lang.Override
    public boolean hasFileKey() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional bytes file_key = 1;</code>
     * @return The fileKey.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString getFileKey() {
      return fileKey_;
    }

    public static final int FILE_PATH_FIELD_NUMBER = 2;
    private volatile java.lang.Object filePath_;
    /**
     * <code>optional string file_path = 2;</code>
     * @return Whether the filePath field is set.
     */
    @java.lang.Override
    public boolean hasFilePath() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional string file_path = 2;</code>
     * @return The filePath.
     */
    @java.lang.Override
    public java.lang.String getFilePath() {
      java.lang.Object ref = filePath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          filePath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string file_path = 2;</code>
     * @return The bytes for filePath.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getFilePathBytes() {
      java.lang.Object ref = filePath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        filePath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CACHE_TAG_FIELD_NUMBER = 3;
    private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag cacheTag_;
    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     * @return Whether the cacheTag field is set.
     */
    @java.lang.Override
    public boolean hasCacheTag() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     * @return The cacheTag.
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag() {
      return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
    }
    /**
     * <code>optional .CacheTag cache_tag = 3;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder getCacheTagOrBuilder() {
      return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
    }

    public static final int RANGES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> ranges_;
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    @java.lang.Override
    public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> getRangesList() {
      return ranges_;
    }
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    @java.lang.Override
    public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
        getRangesOrBuilderList() {
      return ranges_;
    }
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    @java.lang.Override
    public int getRangesCount() {
      return ranges_.size();
    }
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getRanges(int index) {
      return ranges_.get(index);
    }
    /**
     * <code>repeated .CacheEntryRange ranges = 4;</code>
     */
    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder getRangesOrBuilder(
        int index) {
      return ranges_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBytes(1, fileKey_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, filePath_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        output.writeMessage(3, getCacheTag());
      }
      for (int i = 0; i < ranges_.size(); i++) {
        output.writeMessage(4, ranges_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, fileKey_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, filePath_);
      }
      if (((bitField0_ & 0x00000004) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getCacheTag());
      }
      for (int i = 0; i < ranges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, ranges_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) obj;

      if (hasFileKey() != other.hasFileKey()) return false;
      if (hasFileKey()) {
        if (!getFileKey()
            .equals(other.getFileKey())) return false;
      }
      if (hasFilePath() != other.hasFilePath()) return false;
      if (hasFilePath()) {
        if (!getFilePath()
            .equals(other.getFilePath())) return false;
      }
      if (hasCacheTag() != other.hasCacheTag()) return false;
      if (hasCacheTag()) {
        if (!getCacheTag()
            .equals(other.getCacheTag())) return false;
      }
      if (!getRangesList()
          .equals(other.getRangesList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasFileKey()) {
        hash = (37 * hash) + FILE_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getFileKey().hashCode();
      }
      if (hasFilePath()) {
        hash = (37 * hash) + FILE_PATH_FIELD_NUMBER;
        hash = (53 * hash) + getFilePath().hashCode();
      }
      if (hasCacheTag()) {
        hash = (37 * hash) + CACHE_TAG_FIELD_NUMBER;
        hash = (53 * hash) + getCacheTag().hashCode();
      }
      if (getRangesCount() > 0) {
        hash = (37 * hash) + RANGES_FIELD_NUMBER;
        hash = (53 * hash) + getRangesList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CacheEntry}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:CacheEntry)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getCacheTagFieldBuilder();
          getRangesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        fileKey_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        filePath_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        if (cacheTagBuilder_ == null) {
          cacheTag_ = null;
        } else {
          cacheTagBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (rangesBuilder_ == null) {
          ranges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          rangesBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntry_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.fileKey_ = fileKey_;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          to_bitField0_ |= 0x00000002;
        }
        result.filePath_ = filePath_;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          if (cacheTagBuilder_ == null) {
            result.cacheTag_ = cacheTag_;
          } else {
            result.cacheTag_ = cacheTagBuilder_.build();
          }
          to_bitField0_ |= 0x00000004;
        }
        if (rangesBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            ranges_ = java.util.Collections.unmodifiableList(ranges_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.ranges_ = ranges_;
        } else {
          result.ranges_ = rangesBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry.getDefaultInstance()) return this;
        if (other.hasFileKey()) {
          setFileKey(other.getFileKey());
        }
        if (other.hasFilePath()) {
          bitField0_ |= 0x00000002;
          filePath_ = other.filePath_;
          onChanged();
        }
        if (other.hasCacheTag()) {
          mergeCacheTag(other.getCacheTag());
        }
        if (rangesBuilder_ == null) {
          if (!other.ranges_.isEmpty()) {
            if (ranges_.isEmpty()) {
              ranges_ = other.ranges_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureRangesIsMutable();
              ranges_.addAll(other.ranges_);
            }
            onChanged();
          }
        } else {
          if (!other.ranges_.isEmpty()) {
            if (rangesBuilder_.isEmpty()) {
              rangesBuilder_.dispose();
              rangesBuilder_ = null;
              ranges_ = other.ranges_;
              bitField0_ = (bitField0_ & ~0x00000008);
              rangesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getRangesFieldBuilder() : null;
            } else {
              rangesBuilder_.addAllMessages(other.ranges_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.ByteString fileKey_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes file_key = 1;</code>
       * @return Whether the fileKey field is set.
       */
      @java.lang.Override
      public boolean hasFileKey() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional bytes file_key = 1;</code>
       * @return The fileKey.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString getFileKey() {
        return fileKey_;
      }
      /**
       * <code>optional bytes file_key = 1;</code>
       * @param value The fileKey to set.
       * @return This builder for chaining.
       */
      public Builder setFileKey(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        fileKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes file_key = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearFileKey() {
        bitField0_ = (bitField0_ & ~0x00000001);
        fileKey_ = getDefaultInstance().getFileKey();
        onChanged();
        return this;
      }

      private java.lang.Object filePath_ = "";
      /**
       * <code>optional string file_path = 2;</code>
       * @return Whether the filePath field is set.
       */
      public boolean hasFilePath() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional string file_path = 2;</code>
       * @return The filePath.
       */
      public java.lang.String getFilePath() {
        java.lang.Object ref = filePath_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            filePath_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string file_path = 2;</code>
       * @return The bytes for filePath.
       */
      public com.google.protobuf.ByteString
          getFilePathBytes() {
        java.lang.Object ref = filePath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          filePath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string file_path = 2;</code>
       * @param value The filePath to set.
       * @return This builder for chaining.
       */
      public Builder setFilePath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        filePath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string file_path = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearFilePath() {
        bitField0_ = (bitField0_ & ~0x00000002);
        filePath_ = getDefaultInstance().getFilePath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string file_path = 2;</code>
       * @param value The bytes for filePath to set.
       * @return This builder for chaining.
       */
      public Builder setFilePathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        filePath_ = value;
        onChanged();
        return this;
      }

      private org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag cacheTag_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder> cacheTagBuilder_;
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       * @return Whether the cacheTag field is set.
       */
      public boolean hasCacheTag() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       * @return The cacheTag.
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getCacheTag() {
        if (cacheTagBuilder_ == null) {
          return cacheTag_ == null ? org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
        } else {
          return cacheTagBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public Builder setCacheTag(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag value) {
        if (cacheTagBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cacheTag_ = value;
          onChanged();
        } else {
          cacheTagBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public Builder setCacheTag(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder builderForValue) {
        if (cacheTagBuilder_ == null) {
          cacheTag_ = builderForValue.build();
          onChanged();
        } else {
          cacheTagBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public Builder mergeCacheTag(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag value) {
        if (cacheTagBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0) &&
              cacheTag_ != null &&
              cacheTag_ != org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance()) {
            cacheTag_ =
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.newBuilder(cacheTag_).mergeFrom(value).buildPartial();
          } else {
            cacheTag_ = value;
          }
          onChanged();
        } else {
          cacheTagBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public Builder clearCacheTag() {
        if (cacheTagBuilder_ == null) {
          cacheTag_ = null;
          onChanged();
        } else {
          cacheTagBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder getCacheTagBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getCacheTagFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder getCacheTagOrBuilder() {
        if (cacheTagBuilder_ != null) {
          return cacheTagBuilder_.getMessageOrBuilder();
        } else {
          return cacheTag_ == null ?
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance() : cacheTag_;
        }
      }
      /**
       * <code>optional .CacheTag cache_tag = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder> 
          getCacheTagFieldBuilder() {
        if (cacheTagBuilder_ == null) {
          cacheTagBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder>(
                  getCacheTag(),
                  getParentForChildren(),
                  isClean());
          cacheTag_ = null;
        }
        return cacheTagBuilder_;
      }

      private java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> ranges_ =
        java.util.Collections.emptyList();
      private void ensureRangesIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          ranges_ = new java.util.ArrayList<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange>(ranges_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> rangesBuilder_;

      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> getRangesList() {
        if (rangesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(ranges_);
        } else {
          return rangesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public int getRangesCount() {
        if (rangesBuilder_ == null) {
          return ranges_.size();
        } else {
          return rangesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getRanges(int index) {
        if (rangesBuilder_ == null) {
          return ranges_.get(index);
        } else {
          return rangesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder setRanges(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.set(index, value);
          onChanged();
        } else {
          rangesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder setRanges(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.set(index, builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder addRanges(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.add(value);
          onChanged();
        } else {
          rangesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder addRanges(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.add(index, value);
          onChanged();
        } else {
          rangesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder addRanges(
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.add(builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder addRanges(
          int index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.add(index, builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder addAllRanges(
          java.lang.Iterable<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange> values) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, ranges_);
          onChanged();
        } else {
          rangesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder clearRanges() {
        if (rangesBuilder_ == null) {
          ranges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          rangesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public Builder removeRanges(int index) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.remove(index);
          onChanged();
        } else {
          rangesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder getRangesBuilder(
          int index) {
        return getRangesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder getRangesOrBuilder(
          int index) {
        if (rangesBuilder_ == null) {
          return ranges_.get(index);  } else {
          return rangesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
           getRangesOrBuilderList() {
        if (rangesBuilder_ != null) {
          return rangesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(ranges_);
        }
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder addRangesBuilder() {
        return getRangesFieldBuilder().addBuilder(
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.getDefaultInstance());
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder addRangesBuilder(
          int index) {
        return getRangesFieldBuilder().addBuilder(
            index, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.getDefaultInstance());
      }
      /**
       * <code>repeated .CacheEntryRange ranges = 4;</code>
       */
      public java.util.List<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder> 
           getRangesBuilderList() {
        return getRangesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder> 
          getRangesFieldBuilder() {
        if (rangesBuilder_ == null) {
          rangesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder>(
                  ranges_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          ranges_ = null;
        }
        return rangesBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:CacheEntry)
    }

    // @@protoc_insertion_point(class_scope:CacheEntry)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntry>
        PARSER = new com.google.protobuf.AbstractParser<CacheEntry>() {
      @java.lang.Override
      public CacheEntry parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheEntry(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheEntry> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheEntry> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntry getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheTagOrBuilder extends
      // @@protoc_insertion_point(interface_extends:CacheTag)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional string table_name = 1;</code>
     * @return Whether the tableName field is set.
     */
    boolean hasTableName();
    /**
     * <code>optional string table_name = 1;</code>
     * @return The tableName.
     */
    java.lang.String getTableName();
    /**
     * <code>optional string table_name = 1;</code>
     * @return The bytes for tableName.
     */
    com.google.protobuf.ByteString
        getTableNameBytes();

    /**
     * <code>repeated string partition_desc = 2;</code>
     * @return A list containing the partitionDesc.
     */
    java.util.List<java.lang.String>
        getPartitionDescList();
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @return The count of partitionDesc.
     */
    int getPartitionDescCount();
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @param index The index of the element to return.
     * @return The partitionDesc at the given index.
     */
    java.lang.String getPartitionDesc(int index);
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partitionDesc at the given index.
     */
    com.google.protobuf.ByteString
        getPartitionDescBytes(int index);
  }
  /**
   * Protobuf type {@code CacheTag}
   */
  public static final class CacheTag extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:CacheTag)
      CacheTagOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheTag.newBuilder() to construct.
    private CacheTag(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheTag() {
      tableName_ = "";
      partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheTag();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheTag(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              com.google.protobuf.ByteString bs = input.readBytes();
              bitField0_ |= 0x00000001;
              tableName_ = bs;
              break;
            }
            case 18: {
              com.google.protobuf.ByteString bs = input.readBytes();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                partitionDesc_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              partitionDesc_.add(bs);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          partitionDesc_ = partitionDesc_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder.class);
    }

    private int bitField0_;
    public static final int TABLE_NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object tableName_;
    /**
     * <code>optional string table_name = 1;</code>
     * @return Whether the tableName field is set.
     */
    @java.lang.Override
    public boolean hasTableName() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional string table_name = 1;</code>
     * @return The tableName.
     */
    @java.lang.Override
    public java.lang.String getTableName() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          tableName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string table_name = 1;</code>
     * @return The bytes for tableName.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getTableNameBytes() {
      java.lang.Object ref = tableName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        tableName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PARTITION_DESC_FIELD_NUMBER = 2;
    private com.google.protobuf.LazyStringList partitionDesc_;
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @return A list containing the partitionDesc.
     */
    public com.google.protobuf.ProtocolStringList
        getPartitionDescList() {
      return partitionDesc_;
    }
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @return The count of partitionDesc.
     */
    public int getPartitionDescCount() {
      return partitionDesc_.size();
    }
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @param index The index of the element to return.
     * @return The partitionDesc at the given index.
     */
    public java.lang.String getPartitionDesc(int index) {
      return partitionDesc_.get(index);
    }
    /**
     * <code>repeated string partition_desc = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the partitionDesc at the given index.
     */
    public com.google.protobuf.ByteString
        getPartitionDescBytes(int index) {
      return partitionDesc_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, tableName_);
      }
      for (int i = 0; i < partitionDesc_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, partitionDesc_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, tableName_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < partitionDesc_.size(); i++) {
          dataSize += computeStringSizeNoTag(partitionDesc_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getPartitionDescList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) obj;

      if (hasTableName() != other.hasTableName()) return false;
      if (hasTableName()) {
        if (!getTableName()
            .equals(other.getTableName())) return false;
      }
      if (!getPartitionDescList()
          .equals(other.getPartitionDescList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTableName()) {
        hash = (37 * hash) + TABLE_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTableName().hashCode();
      }
      if (getPartitionDescCount() > 0) {
        hash = (37 * hash) + PARTITION_DESC_FIELD_NUMBER;
        hash = (53 * hash) + getPartitionDescList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CacheTag}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:CacheTag)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTagOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        tableName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheTag_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          to_bitField0_ |= 0x00000001;
        }
        result.tableName_ = tableName_;
        if (((bitField0_ & 0x00000002) != 0)) {
          partitionDesc_ = partitionDesc_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.partitionDesc_ = partitionDesc_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag.getDefaultInstance()) return this;
        if (other.hasTableName()) {
          bitField0_ |= 0x00000001;
          tableName_ = other.tableName_;
          onChanged();
        }
        if (!other.partitionDesc_.isEmpty()) {
          if (partitionDesc_.isEmpty()) {
            partitionDesc_ = other.partitionDesc_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensurePartitionDescIsMutable();
            partitionDesc_.addAll(other.partitionDesc_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object tableName_ = "";
      /**
       * <code>optional string table_name = 1;</code>
       * @return Whether the tableName field is set.
       */
      public boolean hasTableName() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional string table_name = 1;</code>
       * @return The tableName.
       */
      public java.lang.String getTableName() {
        java.lang.Object ref = tableName_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (bs.isValidUtf8()) {
            tableName_ = s;
          }
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string table_name = 1;</code>
       * @return The bytes for tableName.
       */
      public com.google.protobuf.ByteString
          getTableNameBytes() {
        java.lang.Object ref = tableName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          tableName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string table_name = 1;</code>
       * @param value The tableName to set.
       * @return This builder for chaining.
       */
      public Builder setTableName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string table_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTableName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        tableName_ = getDefaultInstance().getTableName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string table_name = 1;</code>
       * @param value The bytes for tableName to set.
       * @return This builder for chaining.
       */
      public Builder setTableNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        tableName_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensurePartitionDescIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          partitionDesc_ = new com.google.protobuf.LazyStringArrayList(partitionDesc_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @return A list containing the partitionDesc.
       */
      public com.google.protobuf.ProtocolStringList
          getPartitionDescList() {
        return partitionDesc_.getUnmodifiableView();
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @return The count of partitionDesc.
       */
      public int getPartitionDescCount() {
        return partitionDesc_.size();
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param index The index of the element to return.
       * @return The partitionDesc at the given index.
       */
      public java.lang.String getPartitionDesc(int index) {
        return partitionDesc_.get(index);
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the partitionDesc at the given index.
       */
      public com.google.protobuf.ByteString
          getPartitionDescBytes(int index) {
        return partitionDesc_.getByteString(index);
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param index The index to set the value at.
       * @param value The partitionDesc to set.
       * @return This builder for chaining.
       */
      public Builder setPartitionDesc(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartitionDescIsMutable();
        partitionDesc_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param value The partitionDesc to add.
       * @return This builder for chaining.
       */
      public Builder addPartitionDesc(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartitionDescIsMutable();
        partitionDesc_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param values The partitionDesc to add.
       * @return This builder for chaining.
       */
      public Builder addAllPartitionDesc(
          java.lang.Iterable<java.lang.String> values) {
        ensurePartitionDescIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, partitionDesc_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPartitionDesc() {
        partitionDesc_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string partition_desc = 2;</code>
       * @param value The bytes of the partitionDesc to add.
       * @return This builder for chaining.
       */
      public Builder addPartitionDescBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensurePartitionDescIsMutable();
        partitionDesc_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:CacheTag)
    }

    // @@protoc_insertion_point(class_scope:CacheTag)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheTag>
        PARSER = new com.google.protobuf.AbstractParser<CacheTag>() {
      @java.lang.Override
      public CacheTag parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheTag(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheTag> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheTag> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheTag getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface CacheEntryRangeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:CacheEntryRange)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>optional int64 start = 1;</code>
     * @return Whether the start field is set.
     */
    boolean hasStart();
    /**
     * <code>optional int64 start = 1;</code>
     * @return The start.
     */
    long getStart();

    /**
     * <code>optional int64 end = 2;</code>
     * @return Whether the end field is set.
     */
    boolean hasEnd();
    /**
     * <code>optional int64 end = 2;</code>
     * @return The end.
     */
    long getEnd();
  }
  /**
   * Protobuf type {@code CacheEntryRange}
   */
  public static final class CacheEntryRange extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:CacheEntryRange)
      CacheEntryRangeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use CacheEntryRange.newBuilder() to construct.
    private CacheEntryRange(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private CacheEntryRange() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new CacheEntryRange();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private CacheEntryRange(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              bitField0_ |= 0x00000001;
              start_ = input.readInt64();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              end_ = input.readInt64();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder.class);
    }

    private int bitField0_;
    public static final int START_FIELD_NUMBER = 1;
    private long start_;
    /**
     * <code>optional int64 start = 1;</code>
     * @return Whether the start field is set.
     */
    @java.lang.Override
    public boolean hasStart() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <code>optional int64 start = 1;</code>
     * @return The start.
     */
    @java.lang.Override
    public long getStart() {
      return start_;
    }

    public static final int END_FIELD_NUMBER = 2;
    private long end_;
    /**
     * <code>optional int64 end = 2;</code>
     * @return Whether the end field is set.
     */
    @java.lang.Override
    public boolean hasEnd() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <code>optional int64 end = 2;</code>
     * @return The end.
     */
    @java.lang.Override
    public long getEnd() {
      return end_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(1, start_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        output.writeInt64(2, end_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, start_);
      }
      if (((bitField0_ & 0x00000002) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, end_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange other = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) obj;

      if (hasStart() != other.hasStart()) return false;
      if (hasStart()) {
        if (getStart()
            != other.getStart()) return false;
      }
      if (hasEnd() != other.hasEnd()) return false;
      if (hasEnd()) {
        if (getEnd()
            != other.getEnd()) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasStart()) {
        hash = (37 * hash) + START_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getStart());
      }
      if (hasEnd()) {
        hash = (37 * hash) + END_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getEnd());
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code CacheEntryRange}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:CacheEntryRange)
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRangeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.class, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.Builder.class);
      }

      // Construct using org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        start_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        end_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.internal_static_CacheEntryRange_descriptor;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstanceForType() {
        return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.getDefaultInstance();
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange build() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange buildPartial() {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange result = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.start_ = start_;
          to_bitField0_ |= 0x00000001;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.end_ = end_;
          to_bitField0_ |= 0x00000002;
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) {
          return mergeFrom((org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange other) {
        if (other == org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange.getDefaultInstance()) return this;
        if (other.hasStart()) {
          setStart(other.getStart());
        }
        if (other.hasEnd()) {
          setEnd(other.getEnd());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private long start_ ;
      /**
       * <code>optional int64 start = 1;</code>
       * @return Whether the start field is set.
       */
      @java.lang.Override
      public boolean hasStart() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>optional int64 start = 1;</code>
       * @return The start.
       */
      @java.lang.Override
      public long getStart() {
        return start_;
      }
      /**
       * <code>optional int64 start = 1;</code>
       * @param value The start to set.
       * @return This builder for chaining.
       */
      public Builder setStart(long value) {
        bitField0_ |= 0x00000001;
        start_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStart() {
        bitField0_ = (bitField0_ & ~0x00000001);
        start_ = 0L;
        onChanged();
        return this;
      }

      private long end_ ;
      /**
       * <code>optional int64 end = 2;</code>
       * @return Whether the end field is set.
       */
      @java.lang.Override
      public boolean hasEnd() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <code>optional int64 end = 2;</code>
       * @return The end.
       */
      @java.lang.Override
      public long getEnd() {
        return end_;
      }
      /**
       * <code>optional int64 end = 2;</code>
       * @param value The end to set.
       * @return This builder for chaining.
       */
      public Builder setEnd(long value) {
        bitField0_ |= 0x00000002;
        end_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 end = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearEnd() {
        bitField0_ = (bitField0_ & ~0x00000002);
        end_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:CacheEntryRange)
    }

    // @@protoc_insertion_point(class_scope:CacheEntryRange)
    private static final org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange();
    }

    public static org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    @java.lang.Deprecated public static final com.google.protobuf.Parser<CacheEntryRange>
        PARSER = new com.google.protobuf.AbstractParser<CacheEntryRange>() {
      @java.lang.Override
      public CacheEntryRange parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CacheEntryRange(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<CacheEntryRange> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<CacheEntryRange> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.CacheEntryRange getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  /**
   * Protobuf service {@code LlapDaemonProtocol}
   */
  public static abstract class LlapDaemonProtocol
      implements com.google.protobuf.Service {
    protected LlapDaemonProtocol() {}

    public interface Interface {
      /**
       * <code>rpc registerDag(.RegisterDagRequestProto) returns (.RegisterDagResponseProto);</code>
       */
      public abstract void registerDag(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto> done);

      /**
       * <code>rpc submitWork(.SubmitWorkRequestProto) returns (.SubmitWorkResponseProto);</code>
       */
      public abstract void submitWork(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto> done);

      /**
       * <code>rpc sourceStateUpdated(.SourceStateUpdatedRequestProto) returns (.SourceStateUpdatedResponseProto);</code>
       */
      public abstract void sourceStateUpdated(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto> done);

      /**
       * <code>rpc queryComplete(.QueryCompleteRequestProto) returns (.QueryCompleteResponseProto);</code>
       */
      public abstract void queryComplete(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto> done);

      /**
       * <code>rpc terminateFragment(.TerminateFragmentRequestProto) returns (.TerminateFragmentResponseProto);</code>
       */
      public abstract void terminateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto> done);

      /**
       * <code>rpc updateFragment(.UpdateFragmentRequestProto) returns (.UpdateFragmentResponseProto);</code>
       */
      public abstract void updateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto> done);

    }

    public static com.google.protobuf.Service newReflectiveService(
        final Interface impl) {
      return new LlapDaemonProtocol() {
        @java.lang.Override
        public  void registerDag(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto> done) {
          impl.registerDag(controller, request, done);
        }

        @java.lang.Override
        public  void submitWork(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto> done) {
          impl.submitWork(controller, request, done);
        }

        @java.lang.Override
        public  void sourceStateUpdated(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto> done) {
          impl.sourceStateUpdated(controller, request, done);
        }

        @java.lang.Override
        public  void queryComplete(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto> done) {
          impl.queryComplete(controller, request, done);
        }

        @java.lang.Override
        public  void terminateFragment(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto> done) {
          impl.terminateFragment(controller, request, done);
        }

        @java.lang.Override
        public  void updateFragment(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto> done) {
          impl.updateFragment(controller, request, done);
        }

      };
    }

    public static com.google.protobuf.BlockingService
        newReflectiveBlockingService(final BlockingInterface impl) {
      return new com.google.protobuf.BlockingService() {
        public final com.google.protobuf.Descriptors.ServiceDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }

        public final com.google.protobuf.Message callBlockingMethod(
            com.google.protobuf.Descriptors.MethodDescriptor method,
            com.google.protobuf.RpcController controller,
            com.google.protobuf.Message request)
            throws com.google.protobuf.ServiceException {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.callBlockingMethod() given method descriptor for " +
              "wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return impl.registerDag(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto)request);
            case 1:
              return impl.submitWork(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto)request);
            case 2:
              return impl.sourceStateUpdated(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto)request);
            case 3:
              return impl.queryComplete(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto)request);
            case 4:
              return impl.terminateFragment(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto)request);
            case 5:
              return impl.updateFragment(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto)request);
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getRequestPrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getRequestPrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getResponsePrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getResponsePrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

      };
    }

    /**
     * <code>rpc registerDag(.RegisterDagRequestProto) returns (.RegisterDagResponseProto);</code>
     */
    public abstract void registerDag(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto> done);

    /**
     * <code>rpc submitWork(.SubmitWorkRequestProto) returns (.SubmitWorkResponseProto);</code>
     */
    public abstract void submitWork(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto> done);

    /**
     * <code>rpc sourceStateUpdated(.SourceStateUpdatedRequestProto) returns (.SourceStateUpdatedResponseProto);</code>
     */
    public abstract void sourceStateUpdated(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto> done);

    /**
     * <code>rpc queryComplete(.QueryCompleteRequestProto) returns (.QueryCompleteResponseProto);</code>
     */
    public abstract void queryComplete(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto> done);

    /**
     * <code>rpc terminateFragment(.TerminateFragmentRequestProto) returns (.TerminateFragmentResponseProto);</code>
     */
    public abstract void terminateFragment(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto> done);

    /**
     * <code>rpc updateFragment(.UpdateFragmentRequestProto) returns (.UpdateFragmentResponseProto);</code>
     */
    public abstract void updateFragment(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto> done);

    public static final
        com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.getDescriptor().getServices().get(0);
    }
    public final com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }

    public final void callMethod(
        com.google.protobuf.Descriptors.MethodDescriptor method,
        com.google.protobuf.RpcController controller,
        com.google.protobuf.Message request,
        com.google.protobuf.RpcCallback<
          com.google.protobuf.Message> done) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.callMethod() given method descriptor for wrong " +
          "service type.");
      }
      switch(method.getIndex()) {
        case 0:
          this.registerDag(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto>specializeCallback(
              done));
          return;
        case 1:
          this.submitWork(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto>specializeCallback(
              done));
          return;
        case 2:
          this.sourceStateUpdated(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto>specializeCallback(
              done));
          return;
        case 3:
          this.queryComplete(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto>specializeCallback(
              done));
          return;
        case 4:
          this.terminateFragment(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto>specializeCallback(
              done));
          return;
        case 5:
          this.updateFragment(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto>specializeCallback(
              done));
          return;
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getRequestPrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getRequestPrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getResponsePrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getResponsePrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public static Stub newStub(
        com.google.protobuf.RpcChannel channel) {
      return new Stub(channel);
    }

    public static final class Stub extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapDaemonProtocol implements Interface {
      private Stub(com.google.protobuf.RpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.RpcChannel channel;

      public com.google.protobuf.RpcChannel getChannel() {
        return channel;
      }

      public  void registerDag(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance()));
      }

      public  void submitWork(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance()));
      }

      public  void sourceStateUpdated(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance()));
      }

      public  void queryComplete(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance()));
      }

      public  void terminateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance()));
      }

      public  void updateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance()));
      }
    }

    public static BlockingInterface newBlockingStub(
        com.google.protobuf.BlockingRpcChannel channel) {
      return new BlockingStub(channel);
    }

    public interface BlockingInterface {
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto registerDag(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto submitWork(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto sourceStateUpdated(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto queryComplete(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto terminateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto updateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request)
          throws com.google.protobuf.ServiceException;
    }

    private static final class BlockingStub implements BlockingInterface {
      private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.BlockingRpcChannel channel;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto registerDag(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.RegisterDagResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto submitWork(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SubmitWorkResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto sourceStateUpdated(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto queryComplete(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto terminateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.TerminateFragmentResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto updateFragment(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.UpdateFragmentResponseProto.getDefaultInstance());
      }

    }

    // @@protoc_insertion_point(class_scope:LlapDaemonProtocol)
  }

  /**
   * Protobuf service {@code LlapManagementProtocol}
   */
  public static abstract class LlapManagementProtocol
      implements com.google.protobuf.Service {
    protected LlapManagementProtocol() {}

    public interface Interface {
      /**
       * <code>rpc getDelegationToken(.GetTokenRequestProto) returns (.GetTokenResponseProto);</code>
       */
      public abstract void getDelegationToken(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto> done);

      /**
       * <code>rpc purgeCache(.PurgeCacheRequestProto) returns (.PurgeCacheResponseProto);</code>
       */
      public abstract void purgeCache(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto> done);

      /**
       * <code>rpc getDaemonMetrics(.GetDaemonMetricsRequestProto) returns (.GetDaemonMetricsResponseProto);</code>
       */
      public abstract void getDaemonMetrics(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto> done);

      /**
       * <code>rpc setCapacity(.SetCapacityRequestProto) returns (.SetCapacityResponseProto);</code>
       */
      public abstract void setCapacity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto> done);

      /**
       * <code>rpc evictEntity(.EvictEntityRequestProto) returns (.EvictEntityResponseProto);</code>
       */
      public abstract void evictEntity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto> done);

      /**
       * <code>rpc getCacheContent(.GetCacheContentRequestProto) returns (.GetCacheContentResponseProto);</code>
       */
      public abstract void getCacheContent(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto> done);

    }

    public static com.google.protobuf.Service newReflectiveService(
        final Interface impl) {
      return new LlapManagementProtocol() {
        @java.lang.Override
        public  void getDelegationToken(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto> done) {
          impl.getDelegationToken(controller, request, done);
        }

        @java.lang.Override
        public  void purgeCache(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto> done) {
          impl.purgeCache(controller, request, done);
        }

        @java.lang.Override
        public  void getDaemonMetrics(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto> done) {
          impl.getDaemonMetrics(controller, request, done);
        }

        @java.lang.Override
        public  void setCapacity(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto> done) {
          impl.setCapacity(controller, request, done);
        }

        @java.lang.Override
        public  void evictEntity(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto> done) {
          impl.evictEntity(controller, request, done);
        }

        @java.lang.Override
        public  void getCacheContent(
            com.google.protobuf.RpcController controller,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request,
            com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto> done) {
          impl.getCacheContent(controller, request, done);
        }

      };
    }

    public static com.google.protobuf.BlockingService
        newReflectiveBlockingService(final BlockingInterface impl) {
      return new com.google.protobuf.BlockingService() {
        public final com.google.protobuf.Descriptors.ServiceDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }

        public final com.google.protobuf.Message callBlockingMethod(
            com.google.protobuf.Descriptors.MethodDescriptor method,
            com.google.protobuf.RpcController controller,
            com.google.protobuf.Message request)
            throws com.google.protobuf.ServiceException {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.callBlockingMethod() given method descriptor for " +
              "wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return impl.getDelegationToken(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto)request);
            case 1:
              return impl.purgeCache(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto)request);
            case 2:
              return impl.getDaemonMetrics(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto)request);
            case 3:
              return impl.setCapacity(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto)request);
            case 4:
              return impl.evictEntity(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto)request);
            case 5:
              return impl.getCacheContent(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto)request);
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getRequestPrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getRequestPrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

        public final com.google.protobuf.Message
            getResponsePrototype(
            com.google.protobuf.Descriptors.MethodDescriptor method) {
          if (method.getService() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "Service.getResponsePrototype() given method " +
              "descriptor for wrong service type.");
          }
          switch(method.getIndex()) {
            case 0:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance();
            case 1:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance();
            case 2:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance();
            case 3:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance();
            case 4:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance();
            case 5:
              return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance();
            default:
              throw new java.lang.AssertionError("Can't get here.");
          }
        }

      };
    }

    /**
     * <code>rpc getDelegationToken(.GetTokenRequestProto) returns (.GetTokenResponseProto);</code>
     */
    public abstract void getDelegationToken(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto> done);

    /**
     * <code>rpc purgeCache(.PurgeCacheRequestProto) returns (.PurgeCacheResponseProto);</code>
     */
    public abstract void purgeCache(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto> done);

    /**
     * <code>rpc getDaemonMetrics(.GetDaemonMetricsRequestProto) returns (.GetDaemonMetricsResponseProto);</code>
     */
    public abstract void getDaemonMetrics(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto> done);

    /**
     * <code>rpc setCapacity(.SetCapacityRequestProto) returns (.SetCapacityResponseProto);</code>
     */
    public abstract void setCapacity(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto> done);

    /**
     * <code>rpc evictEntity(.EvictEntityRequestProto) returns (.EvictEntityResponseProto);</code>
     */
    public abstract void evictEntity(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto> done);

    /**
     * <code>rpc getCacheContent(.GetCacheContentRequestProto) returns (.GetCacheContentResponseProto);</code>
     */
    public abstract void getCacheContent(
        com.google.protobuf.RpcController controller,
        org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request,
        com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto> done);

    public static final
        com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptor() {
      return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.getDescriptor().getServices().get(1);
    }
    public final com.google.protobuf.Descriptors.ServiceDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }

    public final void callMethod(
        com.google.protobuf.Descriptors.MethodDescriptor method,
        com.google.protobuf.RpcController controller,
        com.google.protobuf.Message request,
        com.google.protobuf.RpcCallback<
          com.google.protobuf.Message> done) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.callMethod() given method descriptor for wrong " +
          "service type.");
      }
      switch(method.getIndex()) {
        case 0:
          this.getDelegationToken(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto>specializeCallback(
              done));
          return;
        case 1:
          this.purgeCache(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto>specializeCallback(
              done));
          return;
        case 2:
          this.getDaemonMetrics(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto>specializeCallback(
              done));
          return;
        case 3:
          this.setCapacity(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto>specializeCallback(
              done));
          return;
        case 4:
          this.evictEntity(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto>specializeCallback(
              done));
          return;
        case 5:
          this.getCacheContent(controller, (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto)request,
            com.google.protobuf.RpcUtil.<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto>specializeCallback(
              done));
          return;
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getRequestPrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getRequestPrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public final com.google.protobuf.Message
        getResponsePrototype(
        com.google.protobuf.Descriptors.MethodDescriptor method) {
      if (method.getService() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "Service.getResponsePrototype() given method " +
          "descriptor for wrong service type.");
      }
      switch(method.getIndex()) {
        case 0:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance();
        case 1:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance();
        case 2:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance();
        case 3:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance();
        case 4:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance();
        case 5:
          return org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance();
        default:
          throw new java.lang.AssertionError("Can't get here.");
      }
    }

    public static Stub newStub(
        com.google.protobuf.RpcChannel channel) {
      return new Stub(channel);
    }

    public static final class Stub extends org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.LlapManagementProtocol implements Interface {
      private Stub(com.google.protobuf.RpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.RpcChannel channel;

      public com.google.protobuf.RpcChannel getChannel() {
        return channel;
      }

      public  void getDelegationToken(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance()));
      }

      public  void purgeCache(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance()));
      }

      public  void getDaemonMetrics(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance()));
      }

      public  void setCapacity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance()));
      }

      public  void evictEntity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance()));
      }

      public  void getCacheContent(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request,
          com.google.protobuf.RpcCallback<org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto> done) {
        channel.callMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance(),
          com.google.protobuf.RpcUtil.generalizeCallback(
            done,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.class,
            org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance()));
      }
    }

    public static BlockingInterface newBlockingStub(
        com.google.protobuf.BlockingRpcChannel channel) {
      return new BlockingStub(channel);
    }

    public interface BlockingInterface {
      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDelegationToken(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto purgeCache(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDaemonMetrics(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto setCapacity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto evictEntity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request)
          throws com.google.protobuf.ServiceException;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getCacheContent(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request)
          throws com.google.protobuf.ServiceException;
    }

    private static final class BlockingStub implements BlockingInterface {
      private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) {
        this.channel = channel;
      }

      private final com.google.protobuf.BlockingRpcChannel channel;

      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto getDelegationToken(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(0),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetTokenResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto purgeCache(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(1),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.PurgeCacheResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto getDaemonMetrics(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(2),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetDaemonMetricsResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto setCapacity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(3),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SetCapacityResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto evictEntity(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(4),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.EvictEntityResponseProto.getDefaultInstance());
      }


      public org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto getCacheContent(
          com.google.protobuf.RpcController controller,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentRequestProto request)
          throws com.google.protobuf.ServiceException {
        return (org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto) channel.callBlockingMethod(
          getDescriptor().getMethods().get(5),
          controller,
          request,
          org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.GetCacheContentResponseProto.getDefaultInstance());
      }

    }

    // @@protoc_insertion_point(class_scope:LlapManagementProtocol)
  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_UserPayloadProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_UserPayloadProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_EntityDescriptorProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_EntityDescriptorProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_IOSpecProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_IOSpecProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GroupInputSpecProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GroupInputSpecProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SignableVertexSpec_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SignableVertexSpec_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_VertexOrBinary_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_VertexOrBinary_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_FragmentRuntimeInfo_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_FragmentRuntimeInfo_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_QueryIdentifierProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_QueryIdentifierProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_NotTezEvent_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_NotTezEvent_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SubmitWorkRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SubmitWorkRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_RegisterDagRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_RegisterDagRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_RegisterDagResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_RegisterDagResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SubmitWorkResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SubmitWorkResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SourceStateUpdatedRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SourceStateUpdatedResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_QueryCompleteRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_QueryCompleteRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_QueryCompleteResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_QueryCompleteResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_TerminateFragmentRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_TerminateFragmentRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_TerminateFragmentResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_TerminateFragmentResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_UpdateFragmentRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_UpdateFragmentRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_UpdateFragmentResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_UpdateFragmentResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetTokenRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetTokenRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetTokenResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetTokenResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_LlapOutputSocketInitMessage_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_LlapOutputSocketInitMessage_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_PurgeCacheRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_PurgeCacheRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_PurgeCacheResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_PurgeCacheResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_MapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_MapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetDaemonMetricsRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetDaemonMetricsResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SetCapacityRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SetCapacityRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_SetCapacityResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_SetCapacityResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_EvictEntityRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_EvictEntityRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_TableProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_TableProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_EvictEntityResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_EvictEntityResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetCacheContentRequestProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetCacheContentRequestProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_GetCacheContentResponseProto_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_GetCacheContentResponseProto_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_CacheEntryList_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_CacheEntryList_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_CacheEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_CacheEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_CacheTag_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_CacheTag_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_CacheEntryRange_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_CacheEntryRange_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\030LlapDaemonProtocol.proto\"9\n\020UserPayloa" +
      "dProto\022\024\n\014user_payload\030\001 \001(\014\022\017\n\007version\030" +
      "\002 \001(\005\"j\n\025EntityDescriptorProto\022\022\n\nclass_" +
      "name\030\001 \001(\t\022\'\n\014user_payload\030\002 \001(\0132\021.UserP" +
      "ayloadProto\022\024\n\014history_text\030\003 \001(\014\"x\n\013IOS" +
      "pecProto\022\035\n\025connected_vertex_name\030\001 \001(\t\022" +
      "-\n\rio_descriptor\030\002 \001(\0132\026.EntityDescripto" +
      "rProto\022\033\n\023physical_edge_count\030\003 \001(\005\"z\n\023G" +
      "roupInputSpecProto\022\022\n\ngroup_name\030\001 \001(\t\022\026" +
      "\n\016group_vertices\030\002 \003(\t\0227\n\027merged_input_d" +
      "escriptor\030\003 \001(\0132\026.EntityDescriptorProto\"" +
      "\314\003\n\022SignableVertexSpec\022\014\n\004user\030\001 \001(\t\022\026\n\016" +
      "signatureKeyId\030\002 \001(\003\022/\n\020query_identifier" +
      "\030\003 \001(\0132\025.QueryIdentifierProto\022\025\n\rhive_qu" +
      "ery_id\030\004 \001(\t\022\020\n\010dag_name\030\005 \001(\t\022\023\n\013vertex" +
      "_name\030\006 \001(\t\022\024\n\014vertex_index\030\007 \001(\005\022\030\n\020tok" +
      "en_identifier\030\010 \001(\t\0224\n\024processor_descrip" +
      "tor\030\t \001(\0132\026.EntityDescriptorProto\022!\n\013inp" +
      "ut_specs\030\n \003(\0132\014.IOSpecProto\022\"\n\014output_s" +
      "pecs\030\013 \003(\0132\014.IOSpecProto\0221\n\023grouped_inpu" +
      "t_specs\030\014 \003(\0132\024.GroupInputSpecProto\022\032\n\022v" +
      "ertex_parallelism\030\r \001(\005\022%\n\026is_external_s" +
      "ubmission\030\016 \001(\010:\005false\"K\n\016VertexOrBinary" +
      "\022#\n\006vertex\030\001 \001(\0132\023.SignableVertexSpec\022\024\n" +
      "\014vertexBinary\030\002 \001(\014\"\344\001\n\023FragmentRuntimeI" +
      "nfo\022#\n\033num_self_and_upstream_tasks\030\001 \001(\005" +
      "\022-\n%num_self_and_upstream_completed_task" +
      "s\030\002 \001(\005\022\033\n\023within_dag_priority\030\003 \001(\005\022\026\n\016" +
      "dag_start_time\030\004 \001(\003\022 \n\030first_attempt_st" +
      "art_time\030\005 \001(\003\022\"\n\032current_attempt_start_" +
      "time\030\006 \001(\003\"d\n\024QueryIdentifierProto\022\035\n\025ap" +
      "plication_id_string\030\001 \001(\t\022\021\n\tdag_index\030\002" +
      " \001(\005\022\032\n\022app_attempt_number\030\003 \001(\005\"l\n\013NotT" +
      "ezEvent\022\037\n\027input_event_proto_bytes\030\001 \002(\014" +
      "\022\023\n\013vertex_name\030\002 \002(\t\022\027\n\017dest_input_name" +
      "\030\003 \002(\t\022\016\n\006key_id\030\004 \001(\005\"\256\003\n\026SubmitWorkReq" +
      "uestProto\022\"\n\twork_spec\030\001 \001(\0132\017.VertexOrB" +
      "inary\022\033\n\023work_spec_signature\030\002 \001(\014\022\027\n\017fr" +
      "agment_number\030\003 \001(\005\022\026\n\016attempt_number\030\004 " +
      "\001(\005\022\033\n\023container_id_string\030\005 \001(\t\022\017\n\007am_h" +
      "ost\030\006 \001(\t\022\017\n\007am_port\030\007 \001(\005\022\032\n\022credential" +
      "s_binary\030\010 \001(\014\0223\n\025fragment_runtime_info\030" +
      "\t \001(\0132\024.FragmentRuntimeInfo\022\033\n\023initial_e" +
      "vent_bytes\030\n \001(\014\022\037\n\027initial_event_signat" +
      "ure\030\013 \001(\014\022\034\n\ris_guaranteed\030\014 \001(\010:\005false\022" +
      "\013\n\003jwt\030\r \001(\t\022)\n\032is_external_client_reque" +
      "st\030\016 \001(\010:\005false\"t\n\027RegisterDagRequestPro" +
      "to\022\014\n\004user\030\001 \001(\t\022/\n\020query_identifier\030\002 \002" +
      "(\0132\025.QueryIdentifierProto\022\032\n\022credentials" +
      "_binary\030\003 \001(\014\"\032\n\030RegisterDagResponseProt" +
      "o\"b\n\027SubmitWorkResponseProto\022/\n\020submissi" +
      "on_state\030\001 \001(\0162\025.SubmissionStateProto\022\026\n" +
      "\016unique_node_id\030\002 \001(\t\"\205\001\n\036SourceStateUpd" +
      "atedRequestProto\022/\n\020query_identifier\030\001 \001" +
      "(\0132\025.QueryIdentifierProto\022\020\n\010src_name\030\002 " +
      "\001(\t\022 \n\005state\030\003 \001(\0162\021.SourceStateProto\"!\n" +
      "\037SourceStateUpdatedResponseProto\"e\n\031Quer" +
      "yCompleteRequestProto\022/\n\020query_identifie" +
      "r\030\001 \001(\0132\025.QueryIdentifierProto\022\027\n\014delete" +
      "_delay\030\002 \001(\003:\0010\"\034\n\032QueryCompleteResponse" +
      "Proto\"t\n\035TerminateFragmentRequestProto\022/" +
      "\n\020query_identifier\030\001 \001(\0132\025.QueryIdentifi" +
      "erProto\022\"\n\032fragment_identifier_string\030\002 " +
      "\001(\t\" \n\036TerminateFragmentResponseProto\"\210\001" +
      "\n\032UpdateFragmentRequestProto\022/\n\020query_id" +
      "entifier\030\001 \001(\0132\025.QueryIdentifierProto\022\"\n" +
      "\032fragment_identifier_string\030\002 \001(\t\022\025\n\ris_" +
      "guaranteed\030\003 \001(\010\"D\n\033UpdateFragmentRespon" +
      "seProto\022\016\n\006result\030\001 \001(\010\022\025\n\ris_guaranteed" +
      "\030\002 \001(\010\"&\n\024GetTokenRequestProto\022\016\n\006app_id" +
      "\030\001 \001(\t\"&\n\025GetTokenResponseProto\022\r\n\005token" +
      "\030\001 \001(\014\"A\n\033LlapOutputSocketInitMessage\022\023\n" +
      "\013fragment_id\030\001 \002(\t\022\r\n\005token\030\002 \001(\014\"\030\n\026Pur" +
      "geCacheRequestProto\"6\n\027PurgeCacheRespons" +
      "eProto\022\033\n\023purged_memory_bytes\030\001 \001(\003\"&\n\010M" +
      "apEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\003\"\036\n\034G" +
      "etDaemonMetricsRequestProto\";\n\035GetDaemon" +
      "MetricsResponseProto\022\032\n\007metrics\030\001 \003(\0132\t." +
      "MapEntry\"A\n\027SetCapacityRequestProto\022\023\n\013e" +
      "xecutorNum\030\001 \001(\005\022\021\n\tqueueSize\030\002 \001(\005\"\032\n\030S" +
      "etCapacityResponseProto\"F\n\027EvictEntityRe" +
      "questProto\022\017\n\007db_name\030\001 \002(\t\022\032\n\005table\030\002 \003" +
      "(\0132\013.TableProto\"D\n\nTableProto\022\022\n\ntable_n" +
      "ame\030\001 \002(\t\022\020\n\010part_key\030\002 \003(\t\022\020\n\010part_val\030" +
      "\003 \003(\t\"1\n\030EvictEntityResponseProto\022\025\n\revi" +
      "cted_bytes\030\001 \002(\003\"\035\n\033GetCacheContentReque" +
      "stProto\"?\n\034GetCacheContentResponseProto\022" +
      "\037\n\006result\030\001 \001(\0132\017.CacheEntryList\".\n\016Cach" +
      "eEntryList\022\034\n\007entries\030\001 \003(\0132\013.CacheEntry" +
      "\"q\n\nCacheEntry\022\020\n\010file_key\030\001 \001(\014\022\021\n\tfile" +
      "_path\030\002 \001(\t\022\034\n\tcache_tag\030\003 \001(\0132\t.CacheTa" +
      "g\022 \n\006ranges\030\004 \003(\0132\020.CacheEntryRange\"6\n\010C" +
      "acheTag\022\022\n\ntable_name\030\001 \001(\t\022\026\n\016partition" +
      "_desc\030\002 \003(\t\"-\n\017CacheEntryRange\022\r\n\005start\030" +
      "\001 \001(\003\022\013\n\003end\030\002 \001(\003*2\n\020SourceStateProto\022\017" +
      "\n\013S_SUCCEEDED\020\001\022\r\n\tS_RUNNING\020\002*E\n\024Submis" +
      "sionStateProto\022\014\n\010ACCEPTED\020\001\022\014\n\010REJECTED" +
      "\020\002\022\021\n\rEVICTED_OTHER\020\0032\337\003\n\022LlapDaemonProt" +
      "ocol\022B\n\013registerDag\022\030.RegisterDagRequest" +
      "Proto\032\031.RegisterDagResponseProto\022?\n\nsubm" +
      "itWork\022\027.SubmitWorkRequestProto\032\030.Submit" +
      "WorkResponseProto\022W\n\022sourceStateUpdated\022" +
      "\037.SourceStateUpdatedRequestProto\032 .Sourc" +
      "eStateUpdatedResponseProto\022H\n\rqueryCompl" +
      "ete\022\032.QueryCompleteRequestProto\032\033.QueryC" +
      "ompleteResponseProto\022T\n\021terminateFragmen" +
      "t\022\036.TerminateFragmentRequestProto\032\037.Term" +
      "inateFragmentResponseProto\022K\n\016updateFrag" +
      "ment\022\033.UpdateFragmentRequestProto\032\034.Upda" +
      "teFragmentResponseProto2\311\003\n\026LlapManageme" +
      "ntProtocol\022C\n\022getDelegationToken\022\025.GetTo" +
      "kenRequestProto\032\026.GetTokenResponseProto\022" +
      "?\n\npurgeCache\022\027.PurgeCacheRequestProto\032\030" +
      ".PurgeCacheResponseProto\022Q\n\020getDaemonMet" +
      "rics\022\035.GetDaemonMetricsRequestProto\032\036.Ge" +
      "tDaemonMetricsResponseProto\022B\n\013setCapaci" +
      "ty\022\030.SetCapacityRequestProto\032\031.SetCapaci" +
      "tyResponseProto\022B\n\013evictEntity\022\030.EvictEn" +
      "tityRequestProto\032\031.EvictEntityResponsePr" +
      "oto\022N\n\017getCacheContent\022\034.GetCacheContent" +
      "RequestProto\032\035.GetCacheContentResponsePr" +
      "otoBH\n&org.apache.hadoop.hive.llap.daemo" +
      "n.rpcB\030LlapDaemonProtocolProtos\210\001\001\240\001\001"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_UserPayloadProto_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_UserPayloadProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_UserPayloadProto_descriptor,
        new java.lang.String[] { "UserPayload", "Version", });
    internal_static_EntityDescriptorProto_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_EntityDescriptorProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_EntityDescriptorProto_descriptor,
        new java.lang.String[] { "ClassName", "UserPayload", "HistoryText", });
    internal_static_IOSpecProto_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_IOSpecProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_IOSpecProto_descriptor,
        new java.lang.String[] { "ConnectedVertexName", "IoDescriptor", "PhysicalEdgeCount", });
    internal_static_GroupInputSpecProto_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_GroupInputSpecProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GroupInputSpecProto_descriptor,
        new java.lang.String[] { "GroupName", "GroupVertices", "MergedInputDescriptor", });
    internal_static_SignableVertexSpec_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_SignableVertexSpec_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SignableVertexSpec_descriptor,
        new java.lang.String[] { "User", "SignatureKeyId", "QueryIdentifier", "HiveQueryId", "DagName", "VertexName", "VertexIndex", "TokenIdentifier", "ProcessorDescriptor", "InputSpecs", "OutputSpecs", "GroupedInputSpecs", "VertexParallelism", "IsExternalSubmission", });
    internal_static_VertexOrBinary_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_VertexOrBinary_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_VertexOrBinary_descriptor,
        new java.lang.String[] { "Vertex", "VertexBinary", });
    internal_static_FragmentRuntimeInfo_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_FragmentRuntimeInfo_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_FragmentRuntimeInfo_descriptor,
        new java.lang.String[] { "NumSelfAndUpstreamTasks", "NumSelfAndUpstreamCompletedTasks", "WithinDagPriority", "DagStartTime", "FirstAttemptStartTime", "CurrentAttemptStartTime", });
    internal_static_QueryIdentifierProto_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_QueryIdentifierProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_QueryIdentifierProto_descriptor,
        new java.lang.String[] { "ApplicationIdString", "DagIndex", "AppAttemptNumber", });
    internal_static_NotTezEvent_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_NotTezEvent_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_NotTezEvent_descriptor,
        new java.lang.String[] { "InputEventProtoBytes", "VertexName", "DestInputName", "KeyId", });
    internal_static_SubmitWorkRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_SubmitWorkRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SubmitWorkRequestProto_descriptor,
        new java.lang.String[] { "WorkSpec", "WorkSpecSignature", "FragmentNumber", "AttemptNumber", "ContainerIdString", "AmHost", "AmPort", "CredentialsBinary", "FragmentRuntimeInfo", "InitialEventBytes", "InitialEventSignature", "IsGuaranteed", "Jwt", "IsExternalClientRequest", });
    internal_static_RegisterDagRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_RegisterDagRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_RegisterDagRequestProto_descriptor,
        new java.lang.String[] { "User", "QueryIdentifier", "CredentialsBinary", });
    internal_static_RegisterDagResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_RegisterDagResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_RegisterDagResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_SubmitWorkResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_SubmitWorkResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SubmitWorkResponseProto_descriptor,
        new java.lang.String[] { "SubmissionState", "UniqueNodeId", });
    internal_static_SourceStateUpdatedRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(13);
    internal_static_SourceStateUpdatedRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SourceStateUpdatedRequestProto_descriptor,
        new java.lang.String[] { "QueryIdentifier", "SrcName", "State", });
    internal_static_SourceStateUpdatedResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(14);
    internal_static_SourceStateUpdatedResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SourceStateUpdatedResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_QueryCompleteRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(15);
    internal_static_QueryCompleteRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_QueryCompleteRequestProto_descriptor,
        new java.lang.String[] { "QueryIdentifier", "DeleteDelay", });
    internal_static_QueryCompleteResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(16);
    internal_static_QueryCompleteResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_QueryCompleteResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_TerminateFragmentRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(17);
    internal_static_TerminateFragmentRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_TerminateFragmentRequestProto_descriptor,
        new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", });
    internal_static_TerminateFragmentResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(18);
    internal_static_TerminateFragmentResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_TerminateFragmentResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_UpdateFragmentRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(19);
    internal_static_UpdateFragmentRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_UpdateFragmentRequestProto_descriptor,
        new java.lang.String[] { "QueryIdentifier", "FragmentIdentifierString", "IsGuaranteed", });
    internal_static_UpdateFragmentResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(20);
    internal_static_UpdateFragmentResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_UpdateFragmentResponseProto_descriptor,
        new java.lang.String[] { "Result", "IsGuaranteed", });
    internal_static_GetTokenRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(21);
    internal_static_GetTokenRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetTokenRequestProto_descriptor,
        new java.lang.String[] { "AppId", });
    internal_static_GetTokenResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(22);
    internal_static_GetTokenResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetTokenResponseProto_descriptor,
        new java.lang.String[] { "Token", });
    internal_static_LlapOutputSocketInitMessage_descriptor =
      getDescriptor().getMessageTypes().get(23);
    internal_static_LlapOutputSocketInitMessage_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_LlapOutputSocketInitMessage_descriptor,
        new java.lang.String[] { "FragmentId", "Token", });
    internal_static_PurgeCacheRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(24);
    internal_static_PurgeCacheRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_PurgeCacheRequestProto_descriptor,
        new java.lang.String[] { });
    internal_static_PurgeCacheResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(25);
    internal_static_PurgeCacheResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_PurgeCacheResponseProto_descriptor,
        new java.lang.String[] { "PurgedMemoryBytes", });
    internal_static_MapEntry_descriptor =
      getDescriptor().getMessageTypes().get(26);
    internal_static_MapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_MapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_GetDaemonMetricsRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(27);
    internal_static_GetDaemonMetricsRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetDaemonMetricsRequestProto_descriptor,
        new java.lang.String[] { });
    internal_static_GetDaemonMetricsResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(28);
    internal_static_GetDaemonMetricsResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetDaemonMetricsResponseProto_descriptor,
        new java.lang.String[] { "Metrics", });
    internal_static_SetCapacityRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(29);
    internal_static_SetCapacityRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SetCapacityRequestProto_descriptor,
        new java.lang.String[] { "ExecutorNum", "QueueSize", });
    internal_static_SetCapacityResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(30);
    internal_static_SetCapacityResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_SetCapacityResponseProto_descriptor,
        new java.lang.String[] { });
    internal_static_EvictEntityRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(31);
    internal_static_EvictEntityRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_EvictEntityRequestProto_descriptor,
        new java.lang.String[] { "DbName", "Table", });
    internal_static_TableProto_descriptor =
      getDescriptor().getMessageTypes().get(32);
    internal_static_TableProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_TableProto_descriptor,
        new java.lang.String[] { "TableName", "PartKey", "PartVal", });
    internal_static_EvictEntityResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(33);
    internal_static_EvictEntityResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_EvictEntityResponseProto_descriptor,
        new java.lang.String[] { "EvictedBytes", });
    internal_static_GetCacheContentRequestProto_descriptor =
      getDescriptor().getMessageTypes().get(34);
    internal_static_GetCacheContentRequestProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetCacheContentRequestProto_descriptor,
        new java.lang.String[] { });
    internal_static_GetCacheContentResponseProto_descriptor =
      getDescriptor().getMessageTypes().get(35);
    internal_static_GetCacheContentResponseProto_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_GetCacheContentResponseProto_descriptor,
        new java.lang.String[] { "Result", });
    internal_static_CacheEntryList_descriptor =
      getDescriptor().getMessageTypes().get(36);
    internal_static_CacheEntryList_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_CacheEntryList_descriptor,
        new java.lang.String[] { "Entries", });
    internal_static_CacheEntry_descriptor =
      getDescriptor().getMessageTypes().get(37);
    internal_static_CacheEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_CacheEntry_descriptor,
        new java.lang.String[] { "FileKey", "FilePath", "CacheTag", "Ranges", });
    internal_static_CacheTag_descriptor =
      getDescriptor().getMessageTypes().get(38);
    internal_static_CacheTag_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_CacheTag_descriptor,
        new java.lang.String[] { "TableName", "PartitionDesc", });
    internal_static_CacheEntryRange_descriptor =
      getDescriptor().getMessageTypes().get(39);
    internal_static_CacheEntryRange_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_CacheEntryRange_descriptor,
        new java.lang.String[] { "Start", "End", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
