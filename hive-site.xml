<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<property>
  <name>hive.server2.thrift.max.worker.threads</name>
  <value>1</value>
</property>
<property>
  <name>hive.server2.thrift.min.worker.threads</name>
  <value>1</value>
</property>
<property>
  <name>datanucleus.deletionPolicy</name>
  <value>DataNucleus</value>
</property>

<!-- Intellij -->
<property>
  <name>hive.jar.path</name>
  <value>/Users/petervary/dev/upstream/hive/ql/target/hive-exec-4.0.0-SNAPSHOT.jar</value>
  <description>The location of hive_cli.jar that is used when submitting jobs in a separate jvm.</description>
</property>
<property>
  <name>hive.hadoop.classpath</name>
  <value>/Users/petervary/dev/upstream/hive/ql/target/hive-exec-4.0.0-SNAPSHOT.jar</value>
</property>
<property>
  <name>hive.metastore.local</name>
  <value>false</value>
</property>
<property>
  <name>metastore.thrift.uris</name>
  <value>thrift://localhost:9084</value>
</property>
<property>
  <name>metastore.thrift.port</name>
  <value>9084</value>
</property>
<property>
  <name>hive.server2.thrift.port</name>
  <value>10003</value>
</property>
<property>
  <name>hive.server2.webui.port</name>
  <value>10004</value>
</property>
<property>
  <name>metastore.warehouse.dir</name>
  <value>pfile:///Users/petervary/data/apache/hive/warehouse</value>
</property>
<property>
  <name>fs.pfile.impl</name>
  <value>org.apache.hadoop.fs.ProxyLocalFileSystem</value>
  <description>A proxy for local file system used for cross file system testing</description>
</property>
<property>
  <name>metastore.expression.proxy</name>
  <value>org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy</value>
</property>
<property>
  <name>metastore.task.threads.always</name>
  <value>org.apache.hadoop.hive.metastore.events.EventCleanerTask</value>
</property>
<!-- Intellij -->

<!--
<property>
  <name>hive.server2.use.SSL</name>
  <value>true</value>
</property>
<property>
  <name>hive.server2.keystore.path</name>
  <value>/home/sergio/Development/tools/hive-ssl-keystore.jks</value>
</property>
<property>
  <name>hive.server2.keystore.password</name>
  <value>hive123</value>
</property>
-->

<!--
<property>
  <name>hive.server2.authentication</name>
  <value>KERBEROS</value>
</property>
<property>
  <name>hive.server2.authentication.kerberos.principal</name>
  <value>sergio/localhost@VICTORY.COM</value>
</property>
<property>
  <name>hive.server2.authentication.kerberos.keytab</name>
  <value>/etc/hadoop/sergio.keytab</value>
</property>
<property>
  <name>hive.metastore.sasl.enabled</name>
  <value>true</value>
  <description>If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.</description>
</property>
<property>
  <name>hive.metastore.kerberos.keytab.file</name>
  <value>/etc/hadoop/sergio.keytab</value>
  <description>The path to the Kerberos Keytab file containing the metastore thrift server's service principal.</description>
</property>
<property>
  <name>hive.metastore.kerberos.principal</name>
  <value>sergio/localhost@VICTORY.COM</value>
  <description>The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.</description>
</property>
-->

<property>
 <name>hive.metastore.schema.verification</name>
 <value>false</value>
</property>

<property>
<name>datanucleus.autoCreateTables</name>
<value>true</value>
</property>

<property>
<name>datanucleus.schema.autoCreateAll</name>
<value>true</value>
</property>


  <property>
    <name>metastore.metastore.event.db.notification.api.auth</name>
    <value>false</value>
  </property>

<!--
<property>
  <name>hive.exec.pre.hooks</name>
  <value>org.apache.hadoop.hive.ql.hooks.PreExecutePrinter</value>
  <description>Pre Execute Hook for Tests</description>
</property>

<property>
  <name>hive.exec.post.hooks</name>
  <value>org.apache.hadoop.hive.ql.hooks.PostExecutePrinter</value>
  <description>Post Execute Hook for Tests</description>
</property>
-->

<!--
<property>
  <name>hive.server2.authentication</name>
  <value>LDAP</value>
</property>
<property>
  <name>hive.server2.authentication.ldap.url</name>
  <value>ldap://w2k8-1.ad.sec.cloudera.com</value>
</property>
<property>
  <name>hive.server2.authentication.ldap.Domain</name>
  <value>AD.SEC.CLOUDERA.COM</value>
</property>
-->

<property>
  <name>hive.exec.scratchdir</name>
  <value>/tmp/hive-${user.name}</value>
</property>

<!--
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://172.28.199.31:3306/sqoop</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>sqoop</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>Sqoop12345</value>
</property>
-->

<!--
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:oracle:thin:@//172.28.199.31:1521/sqoop</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>oracle.jdbc.OracleDriver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>
-->

<!--
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:sqlserver://172.28.199.31:1433;database=master</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.microsoft.sqlserver.jdbc.SQLServerDriver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>sa</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>Sqoop12345</value>
</property>
-->

<!--
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:postgresql://172.28.199.31:5432/postgres</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>org.postgresql.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>sqoop</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>Sqoop12345</value>
</property>
-->

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <!--value>jdbc:mysql://localhost/upstream_hive?useLegacyDatetimeCode=false&amp;serverTimezone=Europe/Budapest</value-->
  <value>jdbc:mysql://localhost/upstream_hive</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
  <name>hive.metastore.try.direct.sql</name>
  <value>true</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>upstream_hive</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>

<property>
  <name>hive.server2.enable.doAs</name>
  <value>false</value>
  <description></description>
</property>

<property>
  <name>hive.server2.enable.impersonation</name>
  <value>false</value>
  <description></description>
</property>

<property>
  <name>dfs.namenode.acls.enabled</name>
  <value>false</value>
</property>

<!-- FAIR SCHEDULER -->

<!--
<property>
  <name>mapred.jobtracker.taskScheduler</name>
  <value>org.apache.hadoop.mapred.FairScheduler</value>
</property>

<property>
  <name>mapred.fairscheduler.allocation.file</name>
  <value>/mnt/samsung/dev-tools/conf/allocations.xml</value>
</property>

<property>
  <name>mapred.fairscheduler.poolnameproperty</name>
  <value>pool.name</value>
</property>

<property>
  <name>pool.name</name>
  <value>${user.name}</value>
</property>
-->

<!-- These following lines are needed to use ACID features -->
<!-- BEGIN -->

<!--
<property>
  <name>hive.enforce.bucketing</name>
  <value>true</value>
</property>
<property>
  <name>hive.support.concurrency</name>
  <value>true</value>
</property>
<property>
  <name>hive.exec.dynamic.partition.mode</name>
  <value>nonstrict</value>
</property>
<property>
  <name>hive.txn.manager</name>
  <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
</property>
<property>
  <name>hive.lock.manager</name>
  <value>org.apache.hadoop.hive.ql.lockmgr.DbLockManager</value>
</property>
<property>
  <name>hive.compactor.initiator.on</name>
  <value>true</value>
</property>
<property>
  <name>hive.compactor.worker.threads</name>
  <value>2</value>
</property>
-->
<!-- END -->

</configuration>
